---
layout: page
title: About
---

I’m Vipul Maheshwari from Bharat, and I’m about to graduate with a degree in Computer Science in just one month. My journey into the world of Artificial Intelligence and Deep Learning began in my second year when a senior student of mine was working on a classified project for the Indian Army. I volunteered in that project and I was tasked with object localization and creating a manual test set using bounding boxes. The project was a huge success, and it immediately grabbed my attention. I was amazed at how a blend of linear algebra, computational resources, and a dash of Python could work wonders.

Following that project, I embarked on a research apprenticeship with one of my professors. Our research focused on creating unified model predictors for neurological diseases. We delved into neuroimaging data related to Impulse Control Disorder (ICD) and developed an LSTM synthesizer capable of predicting the deterioration levels of patients. This research grabs the attention of reviewers at ICICV and we begged the Best paper award.

Moving on, I had the opportunity to intern with an incredible AI company as a computer vision practitioner. During this internship, I utilized YOLO, image slicing, and segmentation algorithms to build impressive machine learning pipelines for a wide range of tasks.

Soon after this internship, I joined an AI company specializing in the creation of multilingual AI bots using deep learning algorithms as a NLP Deep Learning Intern. My role involved resolving numerous multilingual Natural Language Understanding (NLU) and Named Entity Recognition (NER) challenges for over 15 clients. I implemented NER entity extraction using rule-based regex and pattern dictionaries to enhance intent coverage. Additionally, I Improved and refactored the Python Scripts for NER data creation and the multilingual NLU code base for the production level NLP applications. Along with it, I conducted extensive test on knowledge distillation where I exhibit the use of Transfer learning to decrease the latency of the production models and the usage of Large Language Models (LLMs) in large-scale conversational AI.