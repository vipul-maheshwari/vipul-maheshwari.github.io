{
    "version": "https://jsonfeed.org/version/1",
    "title": "Deox Labs",
    "home_page_url": "http://localhost:4000/",
    "feed_url": "http://localhost:4000/feed.json",
    "description": "Everything related to AI",
    "icon": "http://localhost:4000/apple-touch-icon.png",
    "favicon": "http://localhost:4000/favicon.ico",
    "expired": false,
    
    "author": "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}",
    
"items": [
    
        {
            "id": "http://localhost:4000/2025/02/23/create-a-fintech-agent",
            "title": "Creating a Fintech AI agent without any framework",
            "summary": "This post gives a detailed introduction on how agents work and how we can create one using the simple Python",
            "content_text": "AI agents are everywhere, and they’re only going to get more common in the next few years. To put it simply, imagine a bunch of small digital assistants—agents—that analyze what’s going on, make decisions, and get things done. This makes me wonder: how are we going to handle all these agents in the future? How do we trust them to make decisions for us and let them operate on their own?Right now, there are tons of frameworks that let you quickly build AI agents. Honestly, it feels like every other day a new “low-code” tool pops up, claiming to make building agents super easy. These tools are everywhere—simple drag-and-drop boards or text boxes where you just connect a few prompts and data, and boom, you have your agent.At the heart of every AI agent, there are three key parts: the Model, the Tools, and the Reasoning Loop.      The Model: This is the Large Language Model (LLM) that powers the agent. It’s the core of the agent’s intelligence, trained on tons of text data to understand language and follow complex instructions. This model gives the agent its ability to think and respond like a human.        The Tools: These are the specific functions that the AI agent uses to interact with its environment and carry out tasks. Whether it’s fetching data, calling APIs, or interacting with other software, tools are what make the agent capable of performing its job.        The Reasoning Loop: This is how the agent makes decisions. It’s a process that allows the agent to solve problems, figure out the best course of action, and get things done.  In this post, I’m going to show you something a bit different. Instead of using a framework, I’ll walk you through building a fintech agent with Python. This agent will help with decision-making when it comes to loans and insurance.What’s the deal?So how this fintech agent works, Well, imagine you’re a FinTech company, now customers hit you up with questions like, “Can I borrow $10K for a new kitchen? I am 26 years old” or “I crashed my car—will you cover it?” Lol, I would literally close that insurance claim for sure.Ok the point is What we need is a system that’s fast, smart, and trustworthy in making the decisions on the loan queries as well the insurance claims. Now what I did is, instead of a single alone Agent with all the tools, we will have three agents, loan, insurance and the kernel ones.  Loan Agent: Takes a query, figures out intent (e.g., “home improvement”), and predicts if they’re loan-worthy based on age, income, and amount with the help of a ML model.  Insurance Side: Reads a claim, compares it to past ones, and decides if it’s payable—think semantic search over a claims database.  Kernel: The brains tying it together, routing queries to the right agent.Why raw Python? It’s flexible; I mean I am just presenting a POC and the companies can literally customize it to their data and needs, no black-box framework nonsense. The Credit Risk Dataset and synthetic claims are just stand-ins; real firms would use their own loan histories and claim records.Here’s the big picture: we’ve got a Kernel Agent directing traffic, a Loan Agent predicting eligibility with ML, and an Insurance Agent checking claims against a vector database.I am using the Credit Risk Dataset as a demo for loans, in real;companies can use the CIBIL score like metric to check if the given person is eligible for the loans or not. But for the sake of this POC, I have utilized the dataset, trained a random forest and use it for the predictive analysis.For the insurance-agent, I have used a synthetic pipeline to generate a dataset which consists of the fake insurance queries, and the target is based on that conversation, if the insurance claim is done, the target is one, insurance—placeholders to show the ropes. Companies can plug in their own data instead.Well I would say you can tune the dataset you want as you require the data. Let’s break it down step by step, with code and the why behind it all.Setting the Stage: Libraries and SetupWe start by grabbing our tools—libraries that handle data, models, and vectors:import pandas as pdimport joblibimport pyarrow as pafrom sentence_transformers import SentenceTransformerimport lancedbfrom mistralai import Mistralimport osfrom abc import ABC, abstractmethodfrom typing import Any# Abstract Tool Classclass Tool(ABC):    @abstractmethod    def name(self) -&gt; str:        pass    @abstractmethod    def description(self) -&gt; str:        pass    @abstractmethod    def use(self, *args, **kwargs) -&gt; Any:        pass# Initialize Mistral and Embedderapi_key = os.environ.get(\"MISTRAL_API_KEY\", \"xxxxxxxxxxxxx\")  if not api_key:    raise ValueError(\"Please set the MISTRAL_API_KEY environment variable.\")model = \"mistral-large-latest\"client = Mistral(api_key=api_key)embedder = SentenceTransformer('all-MiniLM-L6-v2')# Connect to LanceDBdb = lancedb.connect('./lancedb_data')For the quick demonstration, I am going to use mistral-large-latest model from the Mistral for LLM tasks and the intent classification in the Loan queries. The Tool class is our way to keep the agents modular - every tool gets a clear job.Loan Agent: Predicting with MLThe Loan Agent decides who gets a loan. We’re using the Credit Risk Dataset as a demo—it’s got age, income, loan amounts, and intents, perfect for a proof-of-concept.#",
            "content_html": "<p>AI agents are everywhere, and they’re only going to get more common in the next few years. To put it simply, imagine a bunch of small digital assistants—agents—that analyze what’s going on, make decisions, and get things done. This makes me wonder: how are we going to handle all these agents in the future? How do we trust them to make decisions for us and let them operate on their own?</p><p>Right now, there are tons of frameworks that let you quickly build AI agents. Honestly, it feels like every other day a new “low-code” tool pops up, claiming to make building agents super easy. These tools are everywhere—simple drag-and-drop boards or text boxes where you just connect a few prompts and data, and boom, you have your agent.</p><p>At the heart of every AI agent, there are three key parts: the Model, the Tools, and the Reasoning Loop.</p><ol>  <li>    <p>The Model: This is the Large Language Model (LLM) that powers the agent. It’s the core of the agent’s intelligence, trained on tons of text data to understand language and follow complex instructions. This model gives the agent its ability to think and respond like a human.</p>  </li>  <li>    <p>The Tools: These are the specific functions that the AI agent uses to interact with its environment and carry out tasks. Whether it’s fetching data, calling APIs, or interacting with other software, tools are what make the agent capable of performing its job.</p>  </li>  <li>    <p>The Reasoning Loop: This is how the agent makes decisions. It’s a process that allows the agent to solve problems, figure out the best course of action, and get things done.</p>  </li></ol><p>In this post, I’m going to show you something a bit different. Instead of using a framework, I’ll walk you through building a fintech agent with Python. This agent will help with decision-making when it comes to loans and insurance.</p><h3 id=\"whats-the-deal\">What’s the deal?</h3><p>So how this fintech agent works, Well, imagine you’re a FinTech company, now customers hit you up with questions like, “Can I borrow $10K for a new kitchen? I am 26 years old” or “I crashed my car—will you cover it?” Lol, I would literally close that insurance claim for sure.</p><p>Ok the point is What we need is a system that’s fast, smart, and trustworthy in making the decisions on the loan queries as well the insurance claims. Now what I did is, instead of a single alone Agent with all the tools, we will have three agents, loan, insurance and the kernel ones.</p><ol>  <li>Loan Agent: Takes a query, figures out intent (e.g., “home improvement”), and predicts if they’re loan-worthy based on age, income, and amount with the help of a ML model.</li>  <li>Insurance Side: Reads a claim, compares it to past ones, and decides if it’s payable—think semantic search over a claims database.</li>  <li>Kernel: The brains tying it together, routing queries to the right agent.</li></ol><p>Why raw Python? It’s flexible; I mean I am just presenting a POC and the companies can literally customize it to their data and needs, no black-box framework nonsense. The <a href=\"https://www.kaggle.com/datasets/laotse/credit-risk-dataset\">Credit Risk Dataset</a> and synthetic claims are just stand-ins; real firms would use their own loan histories and claim records.</p><p>Here’s the big picture: we’ve got a Kernel Agent directing traffic, a Loan Agent predicting eligibility with ML, and an Insurance Agent checking claims against a vector database.</p><p>I am using the Credit Risk Dataset as a demo for loans, in real;companies can use the CIBIL score like metric to check if the given person is eligible for the loans or not. But for the sake of this POC, I have utilized the dataset, trained a random forest and use it for the predictive analysis.</p><p>For the insurance-agent, I have used a synthetic pipeline to generate a dataset which consists of the fake insurance queries, and the target is based on that conversation, if the insurance claim is done, the target is one, insurance—placeholders to show the ropes. Companies can plug in their own data instead.</p><p>Well I would say you can tune the dataset you want as you require the data. Let’s break it down step by step, with code and the why behind it all.</p><h3 id=\"setting-the-stage-libraries-and-setup\">Setting the Stage: Libraries and Setup</h3><p>We start by grabbing our tools—libraries that handle data, models, and vectors:</p><pre><code class=\"language-python\">import pandas as pdimport joblibimport pyarrow as pafrom sentence_transformers import SentenceTransformerimport lancedbfrom mistralai import Mistralimport osfrom abc import ABC, abstractmethodfrom typing import Any# Abstract Tool Classclass Tool(ABC):    @abstractmethod    def name(self) -&gt; str:        pass    @abstractmethod    def description(self) -&gt; str:        pass    @abstractmethod    def use(self, *args, **kwargs) -&gt; Any:        pass# Initialize Mistral and Embedderapi_key = os.environ.get(\"MISTRAL_API_KEY\", \"xxxxxxxxxxxxx\")  if not api_key:    raise ValueError(\"Please set the MISTRAL_API_KEY environment variable.\")model = \"mistral-large-latest\"client = Mistral(api_key=api_key)embedder = SentenceTransformer('all-MiniLM-L6-v2')# Connect to LanceDBdb = lancedb.connect('./lancedb_data')</code></pre><p>For the quick demonstration, I am going to use <code>mistral-large-latest</code> model from the Mistral for LLM tasks and the intent classification in the Loan queries. The <code>Tool</code> class is our way to keep the agents modular - every tool gets a clear job.</p><h2 id=\"loan-agent-predicting-with-ml\">Loan Agent: Predicting with ML</h2><p>The Loan Agent decides who gets a loan. We’re using the Credit Risk Dataset as a demo—it’s got age, income, loan amounts, and intents, perfect for a proof-of-concept.</p><p>#</p>",
            "url": "http://localhost:4000/2025/02/23/create-a-fintech-agent",
            
            
            
            "tags": ["AI Agents","Python Agent from Scratch"],
            
            "date_published": "2025-02-23T00:00:00+05:30",
            "date_modified": "2025-02-23T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/12/28/creating-a-restaurant-bot",
            "title": "Creating a Restaurant Recommendation System",
            "summary": "This post gives an adequate understanding of how to use RAG to create a restaurant recommendation system.",
            "content_text": "In 2024, I did spend significant amount of time playing with RAG and Vector Databases. Typically, LanceDB was at the core and it was one amazing hell of a ride. As the year is ending, I tried to make a simple restaurant recommendation system but it’s going to be slightly different. The idea is to use this Restaurant Dataset and to answer the queries like “Is there any cafe nearby which serves Punjabi food?” or “What are the top 5 restaurants in the city which serve Chinese food?”. The twist is, we need some form of geospatial data to mould the data points into a form that can be used for queries which are related to the distance.So if you want to name this stuff, it can be called “Geospatial Recommendation System”. Sounds cool.. Ok let’s get started.How to start?Well, ok so the first thing is to sort out some relevant features that are required for the recommendation system. So generally, folks who are searching for any recommendation for a restaurant, they are generally looking out for things like restaurant’s name (obviously lol), location maybe, and what kind of food they serve, or other details like ratings or maybe how much time it takes for a restaurant to deliver the food or what’s the average order prices. So, we need to extract these features from the dataset.import pandas as pdrestaurant_data = pd.read_csv(\"data.csv\")restaurant_data = restaurant_data[restaurant_data.columns[1:]]restaurant_data.dropna(inplace=True)restaurant_data.drop_duplicates(inplace=True)restaurant_data.head()To keep things simple, we’re just going to focus on a few key details for now: the type of food a restaurant serves, how customers rate it on average, and where it’s located. By sticking to these basics, we can quickly give people great recommendations without making things too complicated.data_points_vectors = []for _, row in restaurant_data.iterrows():    filter_cols = ['Food type', 'Avg ratings', 'Address']    data_point = \"#\".join(f\"{col}/{row[col]}\" for col in filter_cols)    data_points_vectors.append(data_point)# Add the new column to the DataFramerestaurant_data[\"query_string\"] = data_points_vectorsYou can see that I’ve used # to separate different sections and / for splitting up the key-value pairs. Just a heads up, you can pick different separators and delimiters if you like, but since I’m using FTS (full text search) from LanceDB, a few are reserved for internal representations. If you need to, you can use a backslash as a prefix to support the reserved ones and still use them. This is how a single query string might look'Food type/Biryani,Chinese,North Indian,South Indian#Avg ratings/4.4#Address/5Th Block'Ok, this looks good! Next, we need to turn our query string into a vector. You can choose any embedding model that fits your needs, but I’ll be using the paraphrase-MiniLM-L6-v2 model for now. Basically, all we have to do is encode our query strings into vectors and then load up the payload with the relevant information.from sentence_transformers import SentenceTransformermodel = SentenceTransformer('paraphrase-MiniLM-L6-v2')list_of_payloads = []for index, row in restaurant_data.iterrows():    encoded_vector = model.encode(row['query_string'])    payload = {        'Area': row['Area'],        'City': row['City'],        'Restaurant': row['Restaurant'],        'Price': row['Price'],        'Avg_ratings': row['Avg ratings'],        'Total_ratings': row['Total ratings'],        'Food_type': row['Food type'],        'Address': row['Address'],        'Delivery_time': row['Delivery time'],        'query_string': row['query_string'],        'vector': encoded_vector    }    list_of_payloads.append(payload)Setting up the Vector DatabaseSo, we’ve got our list_of_payloads that includes all the relevant data we’re going to store in our vector database. Let’s get LanceDB set up here:import lancedb# Connect to the LanceDB instanceuri = \"data\"db = lancedb.connect(uri)lancedb_table = db.create_table(\"restaurant-geocoding-app\", data=list_of_payloads)lancedb_df = lancedb_table.to_pandas()lancedb_df.head()Setting up the Geospatial ReferenceNow that our vector database is ready, the next step is to convert user queries into our specific query format. Essentially, what we will do here is to carefully extract key details from each user query to form a structured dictionary. This structured data will then be reformatted to match the pattern of our query strings. To achieve this, I’ll just use a LLM to decipher the user queries and identify the crucial entities we need.import osfrom openai import OpenAIfrom dotenv import load_dotenvload_dotenv()api_key = os.getenv(\"OPENAI_API_KEY\")# Initialize the OpenAI clientclient = OpenAI(api_key=api_key)query_string = \"Hi, I am looking for a casual dining restaurant where Indian or Italian food is served near the HSR Bangalore\"# Helper prompt to extract structured data from ip_prompttotal_prompt = f\"\"\"Query String: {query_string}\\n\\n\\Now from the query string above extract these following entities pinpoints:1. Food type : Extract the food type 2. Avg ratings : Extract the average ratings3. Address : Extract the current exact location, don't consider the fillers like \"near\" or \"nearby\".NOTE : For the Current location, try to understand the pin point location in the query string. Do not give any extra information. If you make the mistakes, bad thingswill happen.Finally return a python dictionary using those points as keys and don't write the markdown of python. If value of a key is not mentioned, then set it as None.\"\"\"# Make a request to OpenAI's APIcompletion = client.chat.completions.create(    model=\"gpt-4o\",  # Use the appropriate model    store=True,    messages=[        {\"role\": \"user\", \"content\": total_prompt}    ])# Extract the generated textcontent = completion.choices[0].message.contentprint(content){  \"Food type\": \"Indian or Italian\",  \"Avg ratings\": None,  \"Address\": \"HSR Bangalore\"}Now, all we need to do is process this output.import ast# Convert the string content to a dictionarytry:    response_dict = ast.literal_eval(content)except (ValueError, SyntaxError) as e:    print(\"Error parsing the response:\", e)    response_dict = {}filter_cols = ['Food type', 'Avg ratings', 'Address']query_string_parts = [f\"{col}/{response_dict.get(col)}\" for col in filter_cols if response_dict.get(col)]query_string = \"#\".join(query_string_parts)print((query_string))Well, now the user query looks like this:Food type/Indian or Italian#Address/HSR BangaloreWell, this user query is now formatted exactly like our query_strings. We can go ahead and search through the vector database to find the top restaurants that best match this query.Searching like a pro.I’ll be using the Full Text Search (FTS) feature from LanceDB to run the search. You can read more about what’s happening behind the scenes here.# Create the FTS index and searchlancedb_table.create_fts_index(\"query_string\", replace=True)results = lancedb_table.search(query_string).to_pandas()results.head()Using the Geospatial dataSo basically when someone searches for nearby restaurants, maybe they’re craving a specific type of cuisine or looking for highly-rated places, we first search up the places that fills there requirements. Now after identifying potential options, we use the Geospatial API to pinpoint their exact locations. The Google Maps API is perfect for this—it grabs the latitude and longitude so we know precisely where each restaurant is. With these coordinates, we can then easily figure out which places are closest to the user’s location, by just calculating the distance.If you didn’t get that, Bear with me, this is going to be super cool.. So first thing we need to do is to setup our Geospatial function which takes a place and return the coordinates:import requestsimport mathdef get_google_geocoding(address, api_key):    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"    params = {\"address\": address, \"key\": api_key}    response = requests.get(base_url, params=params)        if response.status_code == 200:        result = response.json()        if result[\"status\"] == \"OK\":            latitude = result[\"results\"][0][\"geometry\"][\"location\"][\"lat\"]            longitude = result[\"results\"][0][\"geometry\"][\"location\"][\"lng\"]            return (latitude, longitude)        else:            print(f\"Google API: No results found for address: {address}\")            return None    else:        print(f\"Google API: Request failed for address: {address}\")        return NoneFor the distance calculation, there’s this thing called the Haversine formula. It uses the coordinates of two points and basically draws an imaginary straight line between them across the earth to measure how far they are from each other. There’s a bit of math involved in how this formula works, but we can skip that part for now. Here’s what the formula looks like:def haversine(coord1, coord2):    R = 6371.0  # Radius of the Earth in kilometers    lat1, lon1 = map(math.radians, coord1)    lat2, lon2 = map(math.radians, coord2)    dlat = lat2 - lat1    dlon = lon2 - lon1    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))    distance = R * c    return distanceWell, everything seems solid now, the only thing left is the current location. Here’s what we can do: if a user asks about restaurants near a specific area like “nearby HSR layout,” we can easily pull the current location from the preprocessing we did earlier. If not, for now, we can just input the current location manually. Then, we’ll check which restaurants from our vector database match our query and see how far they are based on the current location.Well let’s see what we get:def process_top_restaurants(data, current_location, api_key, top_n=5):    current_coords = get_google_geocoding(current_location, api_key)    if not current_coords:        return    for index, row in data.head(top_n).iterrows():        complete_address = f\"{row['Restaurant']}, {row['City']}\"        restaurant_coords = get_google_geocoding(complete_address, api_key)        if restaurant_coords:            distance = haversine(current_coords, restaurant_coords)            print(f\"Restaurant Name: {row['Restaurant']}\")            print(f\"Distance: {distance:.2f} km\")            print(f\"Area: {row['Area']}\")            print(f\"Price: {row['Price']}\")            print(f\"Coordinates: {restaurant_coords}\")            print(f\"Cuisines Type: {row['Food_type']}\")            print(\"-\" * 40)# Example usageapi_key = os.getenv('GOOGLE_MAPS_API')current_location = 'HSR Layout, Bangalore'process_top_restaurants(results, current_location, api_key, top_n=5)Restaurant Name: Brooks And Bonds BreweryDistance: 3.36 kmArea: KoramangalaPrice: 200.0Coordinates: (12.9341801, 77.62334249999999)Cuisines Type: Indian----------------------------------------Restaurant Name: Cafe AzzureDistance: 8.06 kmArea: Ashok NagarPrice: 1000.0Coordinates: (12.975012, 77.6076558)Cuisines Type: American,Italian----------------------------------------Restaurant Name: Tottos PizzaDistance: 7.92 kmArea: Central BangalorePrice: 500.0Coordinates: (12.9731935, 77.607012)Cuisines Type: Continental,Italian----------------------------------------Restaurant Name: Holy Doh!Distance: 4.15 kmArea: Central BangalorePrice: 600.0Coordinates: (12.9346188, 77.6139914)Cuisines Type: Pizzas,Italian----------------------------------------Restaurant Name: Bakery By FoodhallDistance: 7.31 kmArea: UlsoorPrice: 300.0Coordinates: (12.9734944, 77.62038629999999)Cuisines Type: Bakery,Italian,Desserts----------------------------------------Here we go.. This is cool :)",
            "content_html": "<p>In 2024, I did spend significant amount of time playing with RAG and Vector Databases. Typically, LanceDB was at the core and it was one amazing hell of a ride. As the year is ending, I tried to make a simple restaurant recommendation system but it’s going to be slightly different. The idea is to use this <a href=\"https://www.kaggle.com/datasets/abhijitdahatonde/swiggy-restuarant-dataset\">Restaurant Dataset</a> and to answer the queries like “Is there any cafe nearby which serves Punjabi food?” or “What are the top 5 restaurants in the city which serve Chinese food?”. The twist is, we need some form of geospatial data to mould the data points into a form that can be used for queries which are related to the distance.</p><p>So if you want to name this stuff, it can be called “Geospatial Recommendation System”. Sounds cool.. Ok let’s get started.</p><h3 id=\"how-to-start\">How to start?</h3><p>Well, ok so the first thing is to sort out some relevant features that are required for the recommendation system. So generally, folks who are searching for any recommendation for a restaurant, they are generally looking out for things like restaurant’s name (obviously lol), location maybe, and what kind of food they serve, or other details like ratings or maybe how much time it takes for a restaurant to deliver the food or what’s the average order prices. So, we need to extract these features from the dataset.</p><pre><code class=\"language-python\">import pandas as pdrestaurant_data = pd.read_csv(\"data.csv\")restaurant_data = restaurant_data[restaurant_data.columns[1:]]restaurant_data.dropna(inplace=True)restaurant_data.drop_duplicates(inplace=True)restaurant_data.head()</code></pre><p><img src=\"../images/creating-a-restaurant-bot/data-reference.png\" alt=\"image-showing-the-table-and-the-data\" /></p><p>To keep things simple, we’re just going to focus on a few key details for now: the type of food a restaurant serves, how customers rate it on average, and where it’s located. By sticking to these basics, we can quickly give people great recommendations without making things too complicated.</p><pre><code class=\"language-python\">data_points_vectors = []for _, row in restaurant_data.iterrows():    filter_cols = ['Food type', 'Avg ratings', 'Address']    data_point = \"#\".join(f\"{col}/{row[col]}\" for col in filter_cols)    data_points_vectors.append(data_point)# Add the new column to the DataFramerestaurant_data[\"query_string\"] = data_points_vectors</code></pre><p>You can see that I’ve used <code>#</code> to separate different sections and <code>/</code> for splitting up the key-value pairs. Just a heads up, you can pick different separators and delimiters if you like, but since I’m using FTS (full text search) from LanceDB, a few are reserved for internal representations. If you need to, you can use a backslash as a prefix to support the reserved ones and still use them. This is how a single query string might look</p><pre><code class=\"language-python\">'Food type/Biryani,Chinese,North Indian,South Indian#Avg ratings/4.4#Address/5Th Block'</code></pre><p>Ok, this looks good! Next, we need to turn our query string into a vector. You can choose any embedding model that fits your needs, but I’ll be using the paraphrase-MiniLM-L6-v2 model for now. Basically, all we have to do is encode our query strings into vectors and then load up the payload with the relevant information.</p><pre><code class=\"language-python\">from sentence_transformers import SentenceTransformermodel = SentenceTransformer('paraphrase-MiniLM-L6-v2')list_of_payloads = []for index, row in restaurant_data.iterrows():    encoded_vector = model.encode(row['query_string'])    payload = {        'Area': row['Area'],        'City': row['City'],        'Restaurant': row['Restaurant'],        'Price': row['Price'],        'Avg_ratings': row['Avg ratings'],        'Total_ratings': row['Total ratings'],        'Food_type': row['Food type'],        'Address': row['Address'],        'Delivery_time': row['Delivery time'],        'query_string': row['query_string'],        'vector': encoded_vector    }    list_of_payloads.append(payload)</code></pre><h3 id=\"setting-up-the-vector-database\">Setting up the Vector Database</h3><p>So, we’ve got our <code>list_of_payloads</code> that includes all the relevant data we’re going to store in our vector database. Let’s get LanceDB set up here:</p><pre><code class=\"language-python\">import lancedb# Connect to the LanceDB instanceuri = \"data\"db = lancedb.connect(uri)lancedb_table = db.create_table(\"restaurant-geocoding-app\", data=list_of_payloads)lancedb_df = lancedb_table.to_pandas()lancedb_df.head()</code></pre><p><img src=\"../images/creating-a-restaurant-bot/lancedb-table-reference.png\" alt=\"image-showing-the-lancedb-table-with-the-data\" /></p><h3 id=\"setting-up-the-geospatial-reference\">Setting up the Geospatial Reference</h3><p>Now that our vector database is ready, the next step is to convert user queries into our specific query format. Essentially, what we will do here is to carefully extract key details from each user query to form a structured dictionary. This structured data will then be reformatted to match the pattern of our query strings. To achieve this, I’ll just use a LLM to decipher the user queries and identify the crucial entities we need.</p><pre><code class=\"language-python\">import osfrom openai import OpenAIfrom dotenv import load_dotenvload_dotenv()api_key = os.getenv(\"OPENAI_API_KEY\")# Initialize the OpenAI clientclient = OpenAI(api_key=api_key)query_string = \"Hi, I am looking for a casual dining restaurant where Indian or Italian food is served near the HSR Bangalore\"# Helper prompt to extract structured data from ip_prompttotal_prompt = f\"\"\"Query String: {query_string}\\n\\n\\Now from the query string above extract these following entities pinpoints:1. Food type : Extract the food type 2. Avg ratings : Extract the average ratings3. Address : Extract the current exact location, don't consider the fillers like \"near\" or \"nearby\".NOTE : For the Current location, try to understand the pin point location in the query string. Do not give any extra information. If you make the mistakes, bad thingswill happen.Finally return a python dictionary using those points as keys and don't write the markdown of python. If value of a key is not mentioned, then set it as None.\"\"\"# Make a request to OpenAI's APIcompletion = client.chat.completions.create(    model=\"gpt-4o\",  # Use the appropriate model    store=True,    messages=[        {\"role\": \"user\", \"content\": total_prompt}    ])# Extract the generated textcontent = completion.choices[0].message.contentprint(content)</code></pre><pre><code class=\"language-python\">{  \"Food type\": \"Indian or Italian\",  \"Avg ratings\": None,  \"Address\": \"HSR Bangalore\"}</code></pre><p>Now, all we need to do is process this output.</p><pre><code class=\"language-python\">import ast# Convert the string content to a dictionarytry:    response_dict = ast.literal_eval(content)except (ValueError, SyntaxError) as e:    print(\"Error parsing the response:\", e)    response_dict = {}filter_cols = ['Food type', 'Avg ratings', 'Address']query_string_parts = [f\"{col}/{response_dict.get(col)}\" for col in filter_cols if response_dict.get(col)]query_string = \"#\".join(query_string_parts)print((query_string))</code></pre><p>Well, now the user query looks like this:</p><pre><code class=\"language-python\">Food type/Indian or Italian#Address/HSR Bangalore</code></pre><p>Well, this user query is now formatted exactly like our <code>query_strings</code>. We can go ahead and search through the vector database to find the top restaurants that best match this query.</p><h3 id=\"searching-like-a-pro\">Searching like a pro.</h3><p>I’ll be using the Full Text Search (FTS) feature from LanceDB to run the search. You can read more about what’s happening behind the scenes <a href=\"https://lancedb.github.io/lancedb/fts/#example\">here</a>.</p><pre><code class=\"language-python\"># Create the FTS index and searchlancedb_table.create_fts_index(\"query_string\", replace=True)results = lancedb_table.search(query_string).to_pandas()results.head()</code></pre><p><img src=\"../images/creating-a-restaurant-bot/search-results.png\" alt=\"image-showing-the-results\" /></p><h3 id=\"using-the-geospatial-data\">Using the Geospatial data</h3><p>So basically when someone searches for nearby restaurants, maybe they’re craving a specific type of cuisine or looking for highly-rated places, we first search up the places that fills there requirements. Now after identifying potential options, we use the <a href=\"https://developers.google.com/maps/documentation/geocoding/overview\">Geospatial API</a> to pinpoint their exact locations. The Google Maps API is perfect for this—it grabs the latitude and longitude so we know precisely where each restaurant is. With these coordinates, we can then easily figure out which places are closest to the user’s location, by just calculating the distance.</p><p>If you didn’t get that, Bear with me, this is going to be super cool.. So first thing we need to do is to setup our Geospatial function which takes a place and return the coordinates:</p><pre><code class=\"language-python\">import requestsimport mathdef get_google_geocoding(address, api_key):    base_url = \"https://maps.googleapis.com/maps/api/geocode/json\"    params = {\"address\": address, \"key\": api_key}    response = requests.get(base_url, params=params)        if response.status_code == 200:        result = response.json()        if result[\"status\"] == \"OK\":            latitude = result[\"results\"][0][\"geometry\"][\"location\"][\"lat\"]            longitude = result[\"results\"][0][\"geometry\"][\"location\"][\"lng\"]            return (latitude, longitude)        else:            print(f\"Google API: No results found for address: {address}\")            return None    else:        print(f\"Google API: Request failed for address: {address}\")        return None</code></pre><p>For the distance calculation, there’s this thing called the <code>Haversine formula</code>. It uses the coordinates of two points and basically draws an imaginary straight line between them across the earth to measure how far they are from each other. There’s a bit of math involved in how this formula works, but we can skip that part for now. Here’s what the formula looks like:</p><pre><code class=\"language-python\">def haversine(coord1, coord2):    R = 6371.0  # Radius of the Earth in kilometers    lat1, lon1 = map(math.radians, coord1)    lat2, lon2 = map(math.radians, coord2)    dlat = lat2 - lat1    dlon = lon2 - lon1    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))    distance = R * c    return distance</code></pre><p>Well, everything seems solid now, the only thing left is the current location. Here’s what we can do: if a user asks about restaurants near a specific area like “nearby HSR layout,” we can easily pull the current location from the preprocessing we did earlier. If not, for now, we can just input the current location manually. Then, we’ll check which restaurants from our vector database match our query and see how far they are based on the current location.</p><p>Well let’s see what we get:</p><pre><code class=\"language-python\">def process_top_restaurants(data, current_location, api_key, top_n=5):    current_coords = get_google_geocoding(current_location, api_key)    if not current_coords:        return    for index, row in data.head(top_n).iterrows():        complete_address = f\"{row['Restaurant']}, {row['City']}\"        restaurant_coords = get_google_geocoding(complete_address, api_key)        if restaurant_coords:            distance = haversine(current_coords, restaurant_coords)            print(f\"Restaurant Name: {row['Restaurant']}\")            print(f\"Distance: {distance:.2f} km\")            print(f\"Area: {row['Area']}\")            print(f\"Price: {row['Price']}\")            print(f\"Coordinates: {restaurant_coords}\")            print(f\"Cuisines Type: {row['Food_type']}\")            print(\"-\" * 40)# Example usageapi_key = os.getenv('GOOGLE_MAPS_API')current_location = 'HSR Layout, Bangalore'process_top_restaurants(results, current_location, api_key, top_n=5)</code></pre><pre><code class=\"language-python\">Restaurant Name: Brooks And Bonds BreweryDistance: 3.36 kmArea: KoramangalaPrice: 200.0Coordinates: (12.9341801, 77.62334249999999)Cuisines Type: Indian----------------------------------------Restaurant Name: Cafe AzzureDistance: 8.06 kmArea: Ashok NagarPrice: 1000.0Coordinates: (12.975012, 77.6076558)Cuisines Type: American,Italian----------------------------------------Restaurant Name: Tottos PizzaDistance: 7.92 kmArea: Central BangalorePrice: 500.0Coordinates: (12.9731935, 77.607012)Cuisines Type: Continental,Italian----------------------------------------Restaurant Name: Holy Doh!Distance: 4.15 kmArea: Central BangalorePrice: 600.0Coordinates: (12.9346188, 77.6139914)Cuisines Type: Pizzas,Italian----------------------------------------Restaurant Name: Bakery By FoodhallDistance: 7.31 kmArea: UlsoorPrice: 300.0Coordinates: (12.9734944, 77.62038629999999)Cuisines Type: Bakery,Italian,Desserts----------------------------------------</code></pre><p>Here we go.. This is cool :)</p>",
            "url": "http://localhost:4000/2024/12/28/creating-a-restaurant-bot",
            
            
            
            "tags": ["RAG","LanceDB","Recommendation System"],
            
            "date_published": "2024-12-28T00:00:00+05:30",
            "date_modified": "2024-12-28T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/12/25/rag-fusion-understanding",
            "title": "Understanding RAG fusion",
            "summary": "This post gives an adequate understanding of how the RAG fusion works.",
            "content_text": "If you don’t have any clue on what RAG is, please go through this one to get a brief on what this is all about.So when the RAG model ends, RAG Fusion picks up by adding more layers that improve the RAG retrieval phase, particularly by adding more sophisticated mechanisms for interpretation and integration of the retrieval output. RAG Fusion tries to combat some of the weaknesses inherent also to RAG, including better response to ambiguous queries and returning more relevant, accurate information by improving the retrieval-to-generation loop.What was missed in RAG?  Constraints with Current Search Technologies: RAG is limited by the same things limiting our retrieval-based lexical and vector search technologies.  Human Search Inefficiencies: Humans are not great at writing what they want into search systems, such as typos, vague queries, or limited vocabulary, which often lead to missing the vast reservoir of information that lies beyond the obvious top search results. While RAG assists, it hasn’t entirely solved this problem.  Over-Simplification of Search: Our prevalent search paradigm linearly maps queries to answers, lacking the depth to understand the multi-dimensional nature of human queries. This linear model often fails to capture the nuances and contexts of more complex user inquiries, resulting in less relevant results.How the improvement really happens?So basically when we talk about the traditional RAG, it works by ranking documents in the order of relevance to the query based on vector similarity distances, usually using cosine similarity.RAG Fusion on the other hands addresses the challenges of document retrieval using  Query Transformation: Generates multiple new queries from different angels based on the original query and  Reciprocal Rank Fusion(RRF): Reranking the document relevance based on Reciprocal Rank Fusion(RRF)That being said, when RAG Fusion receives the original query, it sends the original query to the large language model(LLM) to generate a number of new search queries based on the original query from different perspectives.So what really happens is  Query Duplication with a Twist: Translate a user’s query into similar, yet distinct queries via an LLM.  Vector Search Unleashed: Perform vector searches for the original and its newly generated query siblings.  Intelligent Reranking: Aggregate and refine all the results using reciprocal rank fusion.  Final Step:  Pair the cherry-picked results with the new queries, guiding the large language model to a crafted output that considers all the queries and the reranked list of results.Now For all the documents retrieved from the vector database for each query, like a list of lists.  Determine the rank of each document within its respective ranked list.  For each document, compute the reciprocal of its rank (e.g., rank 1 → 1/1 = 1; rank 3 → 1/3).  Sum the reciprocal ranks of each retrieved document across all generated queries.  Order the documents based on their total aggregated scores to determine their final ranking.And then now the top-ranked retrieved documents will be then sent to the LLM along with all the queries to generate a response.",
            "content_html": "<p>If you don’t have any clue on what RAG is, please go through this <a href=\"https://vipul-maheshwari.github.io/2024/02/14/rag-application-with-langchain\">one</a> to get a brief on what this is all about.</p><p>So when the RAG model ends, RAG Fusion picks up by adding more layers that improve the RAG retrieval phase, particularly by adding more sophisticated mechanisms for interpretation and integration of the retrieval output. RAG Fusion tries to combat some of the weaknesses inherent also to RAG, including better response to ambiguous queries and returning more relevant, accurate information by improving the retrieval-to-generation loop.</p><h2 id=\"what-was-missed-in-rag\">What was missed in RAG?</h2><ol>  <li>Constraints with Current Search Technologies: RAG is limited by the same things limiting our retrieval-based lexical and vector search technologies.</li>  <li>Human Search Inefficiencies: Humans are not great at writing what they want into search systems, such as typos, vague queries, or limited vocabulary, which often lead to missing the vast reservoir of information that lies beyond the obvious top search results. While RAG assists, it hasn’t entirely solved this problem.</li>  <li>Over-Simplification of Search: Our prevalent search paradigm linearly maps queries to answers, lacking the depth to understand the multi-dimensional nature of human queries. This linear model often fails to capture the nuances and contexts of more complex user inquiries, resulting in less relevant results.</li></ol><h2 id=\"how-the-improvement-really-happens\">How the improvement really happens?</h2><p>So basically when we talk about the traditional RAG, it works by ranking documents in the order of relevance to the query based on vector similarity distances, usually using cosine similarity.</p><p>RAG Fusion on the other hands addresses the challenges of document retrieval using</p><ol>  <li>Query Transformation: Generates multiple new queries from different angels based on the original query and</li>  <li>Reciprocal Rank Fusion(RRF): Reranking the document relevance based on Reciprocal Rank Fusion(RRF)</li></ol><p>That being said, when RAG Fusion receives the original query, it sends the original query to the large language model(LLM) to generate a number of new search queries based on the original query from different perspectives.</p><p>So what really happens is</p><ol>  <li>Query Duplication with a Twist: Translate a user’s query into similar, yet distinct queries via an LLM.</li>  <li>Vector Search Unleashed: Perform vector searches for the original and its newly generated query siblings.</li>  <li>Intelligent Reranking: Aggregate and refine all the results using reciprocal rank fusion.</li>  <li>Final Step:  Pair the cherry-picked results with the new queries, guiding the large language model to a crafted output that considers all the queries and the reranked list of results.</li></ol><p>Now For all the documents retrieved from the vector database for each query, like a list of lists.</p><ul>  <li>Determine the rank of each document within its respective ranked list.</li>  <li>For each document, compute the reciprocal of its rank (e.g., rank 1 → 1/1 = 1; rank 3 → 1/3).</li>  <li>Sum the reciprocal ranks of each retrieved document across all generated queries.</li>  <li>Order the documents based on their total aggregated scores to determine their final ranking.</li></ul><p>And then now the top-ranked retrieved documents will be then sent to the LLM along with all the queries to generate a response.</p>",
            "url": "http://localhost:4000/2024/12/25/rag-fusion-understanding",
            
            
            
            "tags": ["RAG","LanceDB","RAG Fusion","Improvement in RAG"],
            
            "date_published": "2024-12-25T00:00:00+05:30",
            "date_modified": "2024-12-25T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/12/04/lance-converter-package",
            "title": "Python Package to convert image datasets to lance type",
            "summary": "This post gives a detailed overview of how we can convert any image dataset to lance using a python package",
            "content_text": "A few months ago, I wrote two articles on how the Lance format can supercharge your machine learning workflows. In the first, I showed how Lance’s columnar storage can make handling large image datasets much more efficient for the ML workflows. Then, I followed up with a guide on converting datasets like cinic and mini-imagenet into Lance format using a custom Python script in Google Colab. While that worked well, it was a bit manual.Well, Some of my friends are lazy as fuxcks but excited enough to run the colab and use the Lance formatted datatype for some of their experiments. Being a good friend, I’m excited to share a much easier solution: the lancify Python package.I mean it’s literally just running a one command, and boom — your image datasets are in lance format, ready to go. And, just between us, it makes my life a lot easier too.Installing the PackageBefore diving into the conversion process, let’s start by installing the lancify package. You can easily install it via pip:pip install lancifyConverting Your Image Dataset to LanceOnce you’ve installed the package, converting any image dataset to the Lance format is as simple as running the following Python code. The lancify package abstracts away the complexity of running the colab notebooks manually;from lancify.converter import convert_dataset# Define the path to your image datasetimage_dataset_path = 'cards-classification'resize_dimensions = (256, 256)  # Example resize dimensionssplits = [\"train\", \"test\", \"valid\"]convert_dataset(    dataset_path=image_dataset_path,    batch_size=10,  # You can adjust the batch size as needed    resize=resize_dimensions,  # Pass resize dimensions if you want to resize images    splits=splits)For this demonstration, I have used this dataset, which provides flexibility in terms of image resizing and dataset splits. The image resizing is optional; by default, the images are processed with their original dimensions. However, if needed, you can specify a target size, such as 256x256, by passing the desired dimensions. If you prefer to keep the original size, simply pass None for the resize parameter. Regarding dataset splits, if the dataset includes predefined divisions like training, testing, and validation sets, you can pass a list specifying the relevant splits.For datasets that do not have predefined splits, the images are organized by classification labels. In such cases, you only need to provide the dataset path, and a single lance file will be generated, containing all the images with their corresponding labels. This makes sure that the various kinds of image datasets are handled properly whether they include splits or not.from lancify.converter import convert_datasetimage_dataset_path = 'weather-classification-data'convert_dataset(    dataset_path=image_dataset_path,    batch_size=10,  # You can adjust the batch size as needed)The convert_dataset function automatically handles the following:  Reading the image data – It reads image files and their metadata (filename, category, data split).  Converting to Lance – The images are converted into the Lance format with proper schema.  Saving the Lance files – Lance files are saved for each dataset split (train, test, validation) if there are splits in the dataset, if not then a single lance file is saved with the combined data with an adequate schema to segregate the data with the respective labels.This method is far more concise than manually iterating over directories, creating schemas, and writing to lance files as we did in the previous version using raw colab.CLI SDKIn addition to using the lancify package programmatically through the imported function, you can also leverage the CLI SDK to convert your image datasets. The SDK offers a CLI for the lancify.To use the CLI, all you need to do is install the package with pip install lancify and then run the lancify command in your terminal and follow the args.What’s Happened Behind the Scenes?To give you a better understanding, here’s a brief overview of what happens when you use lancify:  Image Data: The package reads images from your dataset directory and converts them into a binary format.  Metadata Extraction: Metadata such as the image’s filename, category (label), and data split (train/test/validation) are automatically extracted.  PyArrow RecordBatch: The image data and metadata are packaged into a PyArrow RecordBatch for efficient columnar storage.  Lance Dataset Creation: These RecordBatch objects are then written to Lance datasets, which are optimized for performance and storage.This process mirrors the manual steps we previously took but in a much more user-friendly manner, significantly reducing the boilerplate code that was necessary before when you have to manually handle the colabLoading Your Dataset into PandasOnce your image dataset has been converted into the lance format, you can seamlessly load it into pandas data frames for doing all kind of stuff. Here’s how to do it for the card-classification training lance file..import lanceimport pandas as pd# Load Lance datasetds = lance.dataset('cards-classification/cards-classification_train.lance')table = ds.to_table()# Convert Lance table to Pandas dataframedf = table.to_pandas()print(df.head())This is a simple and efficient way to convert your image datasets to the lance format using the lancify package, and it integrates smoothly into your deep learning projects.Switching to the Lance format makes your data pipelines faster and more efficient, especially when dealing with large image datasets. All you need to do is install the package and run the conversion script on your datasets—no hassle.Just this small change can really speed up your machine learning workflows—data loading and processing become much quicker, which means your models train faster.  If you need a reference, this is a quickie on how to use the lance formatted image datasets for training you deep learning models. And if you’re looking for more ideas, there are plenty of other deep learning recipes built on lance.Trust me, it’s worth it! 🤗",
            "content_html": "<p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/cli-for-lance-converter/lancify-title-image.png?raw=true\" alt=\"cli-lancify-title-image\" /></p><p>A few months ago, I wrote two articles on how the Lance format can supercharge your machine learning workflows. In the <a href=\"https://vipul-maheshwari.github.io/2024/03/29/effortlessly-loading-and-processing-images-with-lance-a-code-walkthrough\">first</a>, I showed how Lance’s columnar storage can make handling large image datasets much more efficient for the ML workflows. Then, I followed up with a <a href=\"https://vipul-maheshwari.github.io/2024/04/09/convert-any-image-dataset-to-lance\">guide</a> on converting datasets like <a href=\"https://www.kaggle.com/datasets/vipulmaheshwarii/cinic-10-lance-dataset?ref=blog.lancedb.com\">cinic</a> and <a href=\"https://www.kaggle.com/datasets/vipulmaheshwarii/mini-imagenet-lance-dataset?ref=blog.lancedb.com\">mini-imagenet</a> into Lance format using a custom Python script in <a href=\"https://colab.research.google.com/drive/12RjdHmp6m9_Lx7YMRiat4_fYWZ2g63gx?usp=sharing&amp;ref=blog.lancedb.com\">Google Colab</a>. While that worked well, it was a bit manual.</p><p>Well, Some of my friends are lazy as <code>fuxcks</code> but excited enough to run the colab and use the Lance formatted datatype for some of their experiments. Being a good friend, I’m excited to share a much easier solution: the <code>lancify</code> Python package.</p><p>I mean it’s literally just running a one command, and boom — your image datasets are in lance format, ready to go. And, just between us, it makes my life a lot easier too.</p><h3 id=\"installing-the-package\">Installing the Package</h3><p>Before diving into the conversion process, let’s start by installing the <code>lancify</code> package. You can easily install it via pip:</p><pre><code class=\"language-bash\">pip install lancify</code></pre><h3 id=\"converting-your-image-dataset-to-lance\">Converting Your Image Dataset to Lance</h3><p>Once you’ve installed the package, converting any image dataset to the Lance format is as simple as running the following Python code. The <code>lancify</code> package abstracts away the complexity of running the colab notebooks manually;</p><pre><code class=\"language-python\">from lancify.converter import convert_dataset# Define the path to your image datasetimage_dataset_path = 'cards-classification'resize_dimensions = (256, 256)  # Example resize dimensionssplits = [\"train\", \"test\", \"valid\"]convert_dataset(    dataset_path=image_dataset_path,    batch_size=10,  # You can adjust the batch size as needed    resize=resize_dimensions,  # Pass resize dimensions if you want to resize images    splits=splits)</code></pre><p>For this demonstration, I have used this <a href=\"https://www.kaggle.com/datasets/gpiosenka/cards-image-datasetclassification\">dataset</a>, which provides flexibility in terms of image resizing and dataset splits. The image resizing is optional; by default, the images are processed with their original dimensions. However, if needed, you can specify a target size, such as 256x256, by passing the desired dimensions. If you prefer to keep the original size, simply pass None for the resize parameter. Regarding dataset splits, if the dataset includes predefined divisions like training, testing, and validation sets, you can pass a list specifying the relevant splits.</p><p>For <a href=\"https://www.kaggle.com/datasets/jehanbhathena/weather-dataset\">datasets</a> that do not have predefined splits, the images are organized by classification labels. In such cases, you only need to provide the dataset path, and a single lance file will be generated, containing all the images with their corresponding labels. This makes sure that the various kinds of image datasets are handled properly whether they include splits or not.</p><pre><code class=\"language-python\">from lancify.converter import convert_datasetimage_dataset_path = 'weather-classification-data'convert_dataset(    dataset_path=image_dataset_path,    batch_size=10,  # You can adjust the batch size as needed)</code></pre><p>The <code>convert_dataset</code> function automatically handles the following:</p><ol>  <li><strong>Reading the image data</strong> – It reads image files and their metadata (filename, category, data split).</li>  <li><strong>Converting to Lance</strong> – The images are converted into the Lance format with proper schema.</li>  <li><strong>Saving the Lance files</strong> – Lance files are saved for each dataset split (train, test, validation) if there are splits in the dataset, if not then a single lance file is saved with the combined data with an adequate schema to segregate the data with the respective labels.</li></ol><p>This method is far more concise than manually iterating over directories, creating schemas, and writing to lance files as we did in the previous version using raw colab.</p><h3 id=\"cli-sdk\">CLI SDK</h3><p>In addition to using the <code>lancify</code> package programmatically through the imported function, you can also leverage the CLI SDK to convert your image datasets. The SDK offers a CLI for the <code>lancify</code>.</p><p>To use the CLI, all you need to do is install the package with <code>pip install lancify</code> and then run the <code>lancify</code> command in your terminal and follow the args.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/cli-for-lance-converter/cli-lancify.png?raw=true\" alt=\"image-cli\" /></p><h3 id=\"whats-happened-behind-the-scenes\">What’s Happened Behind the Scenes?</h3><p>To give you a better understanding, here’s a brief overview of what happens when you use <code>lancify</code>:</p><ul>  <li><strong>Image Data</strong>: The package reads images from your dataset directory and converts them into a binary format.</li>  <li><strong>Metadata Extraction</strong>: Metadata such as the image’s filename, category (label), and data split (train/test/validation) are automatically extracted.</li>  <li><strong>PyArrow RecordBatch</strong>: The image data and metadata are packaged into a PyArrow <code>RecordBatch</code> for efficient columnar storage.</li>  <li><strong>Lance Dataset Creation</strong>: These <code>RecordBatch</code> objects are then written to Lance datasets, which are optimized for performance and storage.</li></ul><p>This process mirrors the manual steps we previously took but in a much more user-friendly manner, significantly reducing the boilerplate code that was necessary before when you have to manually handle the <a href=\"https://colab.research.google.com/drive/12RjdHmp6m9_Lx7YMRiat4_fYWZ2g63gx?usp=sharing#scrollTo=93qlCg6TpcW-\">colab</a></p><h3 id=\"loading-your-dataset-into-pandas\">Loading Your Dataset into Pandas</h3><p>Once your image dataset has been converted into the lance format, you can seamlessly load it into pandas data frames for doing all kind of stuff. Here’s how to do it for the <code>card-classification</code> training lance file..</p><pre><code class=\"language-python\">import lanceimport pandas as pd# Load Lance datasetds = lance.dataset('cards-classification/cards-classification_train.lance')table = ds.to_table()# Convert Lance table to Pandas dataframedf = table.to_pandas()print(df.head())</code></pre><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/cli-for-lance-converter/lance-training.png?raw=true\" alt=\"lance-training\" /></p><p>This is a simple and efficient way to convert your image datasets to the lance format using the <code>lancify</code> package, and it integrates smoothly into your deep learning projects.</p><p>Switching to the Lance format makes your data pipelines faster and more efficient, especially when dealing with large image datasets. All you need to do is install the package and run the conversion script on your datasets—no hassle.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/cli-for-lance-converter/child-meme-lancify.png?raw=true\" alt=\"meme-maker\" /></p><p>Just this small change can really speed up your machine learning workflows—data loading and processing become much quicker, which means your models train faster.  If you need a reference, this is a <a href=\"https://vipul-maheshwari.github.io/2024/06/26/train-a-cnn-with-lancedataset\">quickie</a> on how to use the lance formatted image datasets for training you deep learning models. And if you’re looking for more ideas, there are plenty of other <a href=\"https://github.com/lancedb/lance-deeplearning-recipes\">deep learning recipes</a> built on lance.</p><p>Trust me, it’s worth it! 🤗</p>",
            "url": "http://localhost:4000/2024/12/04/lance-converter-package",
            
            
            
            "tags": ["LanceDB","Dataset","Python","Package"],
            
            "date_published": "2024-12-04T00:00:00+05:30",
            "date_modified": "2024-12-04T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/10/30/improving-rag-with-raptor",
            "title": "Improving RAG with RAPTOR",
            "summary": "This post shows how you can improve a RAG application using the RAPTOR",
            "content_text": "Traditional RAG setups often split documents into fixed-size chunks, but this can lead to problems in maintaining the semantic coherence of the text. If a key concept spans multiple chunks, and we only retrieve one chunk, the LLM might lack the full understanding of the idea, leading to incomplete or misleading responses. As a result, crucial ideas and relationships between concepts may be overlooked, leading to incomplete or inaccurate responses.Additionally, In a flat retrieval structure where all the retrieved chunks are treated equally, this can dilute the importance of critical sections. For example, if one section of the document has key insights but gets buried among less relevant chunks, the model won’t know which parts to prioritize unless we introduce more intelligent weighting or hierarchical structures. I mean it becomes really difficult during the retrieval to weigh which chunk is more important and might be better suitable as a context.What is RAPTOR?RAPTOR, which stands for Recursive Abstractive Processing for Tree Organized Retrieval, is a new technique which solves the problems mentioned before. Think of RAPTOR as a librarian who organizes information in a tree-like structure. Instead of simply stacking books in a pile, it clusters similar titles together, creating a hierarchy that narrows as you ascend. Each cluster of books represents a group of related documents, and at the top of each cluster, there’s a summary that encapsulates the key points from all the books below it. This process continues all the way to the top of the tree, providing a comprehensive view of the information—it’s like having both a magnifying glass and a telescope!To visualize this further, think of the leaves of the tree as document chunks. These chunks are grouped into clusters to generate meaningful summaries, which then become the new leaves of the tree. This recursive process repeats until reaching the top.Key terms to look out forBefore we dive in, let’s quickly review some key terms that will be useful as we explore RAPTOR tech. I just want to put it up here to make sure you are comfortable with the nitty tech details as we go along.  GMM Clustering: Gaussian Mixture Models (GMM) group data into clusters based on statistical probabilities. So instead of rigidly classifying each instance into one category like K-means, GMM generates K-Gaussian distributions that consider the entire training space. This means that each point can belong to one or more distributions.  Dimensionality Reduction: This process simplifies the data by reducing the number of variables while retaining essential features. It’s particularly important for understanding high-dimensional datasets like embeddings.  UMAP: Uniform Manifold Approximation and Projection (UMAP) is a powerful dimensionality reduction algorithm we’ll use to shrink the size of our data point embeddings. This reduction makes it easier for clustering algorithms like GMM to cluster high-dimensional embeddings.  BIC and Elbow Method: Both techniques help identify the optimal number of clusters in a dataset. The Bayesian Information Criterion (BIC) evaluates models based on their fit to the data while penalizing complexity. The Elbow Method plots explained variance against the number of clusters, helping to pinpoint where adding more clusters offers diminishing returns. For our purposes, we’ll leverage both methods to determine the best number of clusters.How it actually works?Now that you’re familiar with the key terms (and if not, no worries—you’ll catch on as we go!), let’s dive into how everything actually works under the hood of RAPTOR.  Starting Documents as Leaves: The leaves of the tree represent a set of initial documents, which are our text chunks.  Embedding and Clustering: The leaves are embedded and clustered. The authors utilize the UMAP dimensionality reduction algorithm to minimize the embedding size of these chunks. For clustering, Gaussian Mixture Models (GMM) are employed to ensure effective grouping, addressing the challenges posed by high-dimensional vector embeddings.  Summarizing Clusters: Once clustered, these groups of similar chunks are summarized into higher-level abstractions nodes. Each cluster acts like a basket for similar documents, and the individual summaries encapsulate the essence of all nodes within that cluster. This process builds from the bottom up, where nodes are clustered together to create summaries that are then passed up the hierarchy.  Recursive Process: This entire procedure is recursive, resulting in a tree structure that transitions from raw documents (the leaves) to more abstract summaries, with each summary derived from the clusters of various nodes.Building the RAPTORNow that we’ve unpacked how it all works (and you’re still with me hopefully, right?), let’s shift gears and talk about how we actually build the RAPTOR tree.Setup and Importspip install lancedb scikit-learn openai torch sentence_transformers tiktoken umap-learn PyPDF2import osimport uuidimport tiktokenimport reimport numpy as npimport pandas as pdimport transformersimport torchimport umap.umap_ as umapimport matplotlib.pyplot as pltfrom openai import OpenAIfrom typing import List, Tuple, Optional, Dictfrom sklearn.mixture import GaussianMixturefrom sentence_transformers import SentenceTransformeropenai_api_key = \"sk-XXXXXXXXXXXXXXX\"client = OpenAI(api_key=openai_api_key)Creating the ChunksSetting up RAPTOR is pretty straightforward and builds on what we’ve already covered. The first step is to break down our textual documents into smaller chunks. Once we have those, we can convert them into dense vector embeddings. To simplify things, our use case example will use just one financial document as a source. Download it and add to a directory. For now, I used a data directory where all the documents can be added. You can of course load any source documents (and alter the query below) in your own experiments!Note: Make sure you use an unencrypted version of this file to avoid errors when PyPDF2 tries to read the file.import osimport PyPDF2# Function to extract text from a PDF filedef extract_pdf_text(file_path):    with open(file_path, 'rb') as file:        reader = PyPDF2.PdfReader(file)        text = \"\"        for page_num in range(len(reader.pages)):            page = reader.pages[page_num]            text += page.extract_text()    return text# Function to split text into chunks with overlapdef split_text(text, chunk_size=1000, chunk_overlap=50):    chunks = []    start = 0    while start &lt; len(text):        end = min(start + chunk_size, len(text))        chunks.append(text[start:end])        start += chunk_size - chunk_overlap    return chunks# Function to process all PDFs in a directorydef process_directory(directory_path, chunk_size=1000, chunk_overlap=50):    all_chunks = []    # Iterate over all PDF files in the directory    for filename in os.listdir(directory_path):        if filename.endswith(\".pdf\"):            file_path = os.path.join(directory_path, filename)            print(f\"Processing file: {file_path}\")                        # Step 1: Extract text from the PDF            pdf_text = extract_pdf_text(file_path)                        # Step 2: Split the extracted text into chunks            chunks = split_text(pdf_text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)                        # Append chunks from this file to the overall list            all_chunks.extend(chunks)        return all_chunksdirectory_path = os.path.join(os.getcwd(), \"data\")  # Specify your directory pathchunk_size = 1000chunk_overlap = 50# Process all PDF files in the directory and get the chunkschunks = process_directory(directory_path, chunk_size=chunk_size, chunk_overlap=chunk_overlap)# Optional: Print the number of chunks and preview some of the chunksprint(f\"Total number of chunks: {len(chunks)}\")for i, chunk in enumerate(chunks[:5]):  # Show first 5 chunks as a preview    print(f\"Chunk {i+1}:\\n{chunk}\\n\")Now that we have our chunks, it’s time to dive into the recursive processing to create summarized nodes. For the embedding part, I’ll be using the all-MiniLM-L6-v2 model from Sentence Transformers, but feel free to choose any embedding model that suits your needs—it’s entirely up to you!embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')Clustering and Dimensionality ReductionNow we have our embedded chunks, it’s time to step on for the next set of tasks. When diving into RAPTOR, one of the biggest hurdles we encounter is the high dimensionality of vector embeddings. Traditional clustering methods like Gaussian Mixture Models (GMM) often struggle with this complexity, making it tough to effectively cluster high-dimensional data chunks. To tackle this challenge, we turn to Uniform Manifold Approximation and Projection (UMAP). UMAP excels at simplifying data while preserving the essential structures that matter most.A key factor in UMAP’s effectiveness is the n_neighbors parameter. This setting dictates how much of the data’s neighborhood UMAP considers during dimensionality reduction. In simpler terms, it helps you choose between zooming in on details or taking a broader view:  Higher n_neighbors: A higher value encourages UMAP to “look at many neighbors,” which helps maintain the global structure of the data. This results in larger, more general clusters.  Lower n_neighbors: Conversely, lowering n_neighbors prompts UMAP to “focus on close relationships,” enabling it to preserve the local structure and form smaller, more detailed clusters.Think of it this way: Imagine you’re at a party. If you take a step back and look around (high n_neighbors), you can see the whole room—where the groups are forming, who’s mingling, and the general vibe. But if you lean in closer to a specific group (low n_neighbors), you can hear their conversation and pick up on the nuances, like inside jokes or shared interests. Both perspectives are valuable; it just depends on what you want to understand.In RAPTOR, we leverage this flexibility in n_neighbors to create a hierarchical clustering structure. We first run UMAP with a higher n_neighbors to identify the global clusters—the broad categories. Then, we narrow the focus by lowering the value to uncover local clusters within those broader groups. This two-step approach ensures we capture both large-scale patterns and intricate details.Well, TL;DR  Dimensionality Reduction helps manage high-dimensional data, and UMAP is our primary tool for that.  The n_neighbors parameter controls the balance between seeing the “big picture” and honing in on local details.  The clustering process begins with global clusters (using high n_neighbors), followed by a focus on local clusters with a lower setting of n_neighbors.def dimensionality_reduction(    embeddings: np.ndarray,    target_dim: int,    clustering_type: str,    metric: str = \"cosine\",) -&gt; np.ndarray:    if clustering_type == \"local\":        n_neighbors = max(2, min(10, len(embeddings) - 1))        min_dist = 0.01    elif clustering_type == \"global\":        n_neighbors = max(2, min(int((len(embeddings) - 1) ** 0.5), len(embeddings) // 10, len(embeddings) - 1))        min_dist = 0.1    else:        raise ValueError(\"clustering_type must be either 'local' or 'global'\")    umap_model = umap.UMAP(        n_neighbors=n_neighbors,        min_dist=min_dist,        n_components=target_dim,        metric=metric,    )    return umap_model.fit_transform(embeddings)I plan to leverage both the Elbow Method and the Bayesian Information Criterion (BIC) to pinpoint the optimal number of clusters for our analysis.def compute_inertia(embeddings: np.ndarray, labels: np.ndarray, centroids: np.ndarray) -&gt; float:    return np.sum(np.min(np.sum((embeddings[:, np.newaxis] - centroids) ** 2, axis=2), axis=1))def optimal_cluster_number(    embeddings: np.ndarray,    max_clusters: int = 50,    random_state: int = SEED) -&gt; int:    max_clusters = min(max_clusters, len(embeddings))    number_of_clusters = np.arange(1, max_clusters + 1)    inertias = []    bic_scores = []        for n in number_of_clusters:        gmm = GaussianMixture(n_components=n, random_state=random_state)        labels = gmm.fit_predict(embeddings)        centroids = gmm.means_        inertia = compute_inertia(embeddings, labels, centroids)        inertias.append(inertia)        bic_scores.append(gmm.bic(embeddings))        inertia_changes = np.diff(inertias)    elbow_optimal = number_of_clusters[np.argmin(inertia_changes) + 1]    bic_optimal = number_of_clusters[np.argmin(bic_scores)]        return max(elbow_optimal, bic_optimal)def gmm_clustering(    embeddings: np.ndarray,     threshold: float,     random_state: int = SEED) -&gt; Tuple[List[np.ndarray], int]:    n_clusters = optimal_cluster_number(embeddings, random_state=random_state)    gm = GaussianMixture(n_components=n_clusters, random_state=random_state, n_init=2)    gm.fit(embeddings)    probs = gm.predict_proba(embeddings)    labels = [np.where(prob &gt; threshold)[0] for prob in probs]     return labels, n_clusters  Tree ConstructionNow that we’ve wrapped up the clustering part, let’s talk about how we build our hierarchical tree. After several rounds of clustering and summarization (while keeping track of how deep we go), here’s what we have:  Leaf Nodes: These are our original text chunks, forming the base of the tree.  Summary Nodes: As we go up the tree, each node acts like a quick summary of its child nodes, capturing the main idea of the cluster.  Hierarchical Embeddings: The summary nodes can also become the new nodes at their level. Each of these nodes gets its own vector embedding, representing the summarized meaning. So, we’re essentially adding more nodes while enriching them with summaries.The process flows nicely: we embed the chunks, reduce their dimensions using UMAP, cluster them with Gaussian Mixture Models, start with a broad overview, and then zoom in for more detailed clusters before summarizing.def clustering_algorithm(    embeddings: np.ndarray,    target_dim: int,    threshold: float,    random_state: int = SEED) -&gt; Tuple[List[np.ndarray], int]:    if len(embeddings) &lt;= target_dim + 1:        return [np.array([0]) for _ in range(len(embeddings))], 1        # Global clustering    reduced_global_embeddings = dimensionality_reduction(embeddings, target_dim, \"global\")    global_clusters, n_global_clusters = gmm_clustering(reduced_global_embeddings, threshold, random_state=random_state)    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]    total_clusters = 0    # Local clustering within each global cluster    for i in range(n_global_clusters):        global_cluster_mask = np.array([i in gc for gc in global_clusters])        global_cluster_embeddings = embeddings[global_cluster_mask]        if len(global_cluster_embeddings) &lt;= target_dim + 1:            # Assign all points in this global cluster to a single local cluster            for idx in np.where(global_cluster_mask)[0]:                all_local_clusters[idx] = np.append(all_local_clusters[idx], total_clusters)            total_clusters += 1            continue        try:            reduced_local_embeddings = dimensionality_reduction(global_cluster_embeddings, target_dim, \"local\")            local_clusters, n_local_clusters = gmm_clustering(reduced_local_embeddings, threshold, random_state=random_state)            # Assign local cluster IDs            for j in range(n_local_clusters):                local_cluster_mask = np.array([j in lc for lc in local_clusters])                global_indices = np.where(global_cluster_mask)[0]                local_indices = global_indices[local_cluster_mask]                for idx in local_indices:                    all_local_clusters[idx] = np.append(all_local_clusters[idx], j + total_clusters)            total_clusters += n_local_clusters        except Exception as e:            print(f\"Error in local clustering for global cluster {i}: {str(e)}\")            # Assign all points in this global cluster to a single local cluster            for idx in np.where(global_cluster_mask)[0]:                all_local_clusters[idx] = np.append(all_local_clusters[idx], total_clusters)            total_clusters += 1    return all_local_clusters, total_clusters     def generate_summary(context: str) -&gt; str:    prompt = f\"\"\"    Provide the Summary for the given context. Here are some additional instructions for you:    Instructions:    1. Don't make things up, Just use the contexts and generate the relevant summary.    2. Don't mix the numbers, Just use the numbers in the context.    3. Don't try to use fancy words, stick to the basics of the language that is being used in the context.    Context: {context}    \"\"\"    response = client.chat.completions.create(        model=\"gpt-4\",        messages=[            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},            {\"role\": \"user\", \"content\": prompt}        ],        max_tokens=200,        n=1,        stop=None,        temperature=0.7    )    summary = response.choices[0].message.content.strip()    return summarydef embed_clusters(    texts: List[str],    target_dim: int = 10,    threshold: float = 0.1) -&gt; pd.DataFrame:    textual_embeddings = np.array(embedding_model.encode(texts))    clusters, number_of_clusters = clustering_algorithm(textual_embeddings, target_dim, threshold)    print(f\"Number of clusters: {number_of_clusters}\")    return pd.DataFrame({        \"texts\": texts,        \"embedding\": list(textual_embeddings),        \"clusters\": clusters    })def embed_cluster_summaries(    texts: List[str],    level: int,    target_dim: int = 10,    threshold: float = 0.1) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:    df_clusters = embed_clusters(texts, target_dim, threshold)    main_list = []        for _, row in df_clusters.iterrows():        for cluster in row[\"clusters\"]:            main_list.append({                \"text\": row[\"texts\"],                \"embedding\": row[\"embedding\"],                \"clusters\": cluster            })        main_df = pd.DataFrame(main_list)    unique_clusters = main_df[\"clusters\"].unique()    if len(unique_clusters) == 0:        return df_clusters, pd.DataFrame(columns=[\"summaries\", \"level\", \"clusters\"])    print(f\"--Generated {len(unique_clusters)} clusters--\")    summaries = []    for cluster in unique_clusters:        text_in_df = main_df[main_df[\"clusters\"] == cluster]        unique_texts = text_in_df[\"text\"].tolist()        text = \"------\\n------\".join(unique_texts)        summary = generate_summary(text)        summaries.append(summary)    df_summaries = pd.DataFrame({        \"summaries\": summaries,        \"level\": [level] * len(summaries),        \"clusters\": unique_clusters    })    return df_clusters, df_summariesdef recursive_embedding_with_cluster_summarization(    texts: List[str],    number_of_levels: int = 3,    level: int = 1,    target_dim: int = 10,    threshold: float = 0.1) -&gt; Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:    if level &gt; number_of_levels:        return {}        results = {}    df_clusters, df_summaries = embed_cluster_summaries(texts, level, target_dim, threshold)    results[level] = (df_clusters, df_summaries)        if df_summaries.empty or len(df_summaries['clusters'].unique()) == 1:        print(f\"No more unique clusters found at level {level}. Stopping recursion.\")        return results        if level &lt; number_of_levels:        next_level_texts = df_summaries['summaries'].tolist()        next_level_results = recursive_embedding_with_cluster_summarization(            next_level_texts,             number_of_levels,             level + 1,            target_dim,            threshold        )        results.update(next_level_results)        return resultsOkay, the code might seem a bit daunting at first glance, but don’t worry! Just give it a couple of looks, and it will start to make sense. Essentially, we’re just following the flow I mentioned earlier.def process_text_hierarchy(    texts: List[str],     number_of_levels: int = 3,    target_dim: int = 10,    threshold: float = 0.1) -&gt; Dict[str, pd.DataFrame]:    hierarchy_results = recursive_embedding_with_cluster_summarization(        texts, number_of_levels, target_dim=target_dim, threshold=threshold    )        processed_results = {}    for level, (df_clusters, df_summaries) in hierarchy_results.items():        if df_clusters.empty or df_summaries.empty:            print(f\"No data for level {level}. Skipping.\")            continue        processed_results[f\"level_{level}_clusters\"] = df_clusters        processed_results[f\"level_{level}_summaries\"] = df_summaries        return processed_resultsresults = process_text_hierarchy(chunks, number_of_levels=3)InferenceNow that we have our tree structure with leaf nodes at the bottom and summarized nodes in between, it’s time to query the RAG. There are two main methods for navigating the RAPTOR tree: Tree Traversal and Collapsed Tree Retrieval.  Tree Traversal Retrieval: This method systematically explores the tree, starting from the root node. It first selects the top-k most relevant root nodes based on their cosine similarity to the query embedding. Then, for each selected root node, its children are considered in the next layer, where the top-k nodes are again selected based on their cosine similarity to the query vector. This process repeats until we reach the leaf nodes. Finally, the text from all the selected nodes is concatenated to form the retrieved context.  Collapsed Tree Retrieval: This approach simplifies things by viewing the tree as a single layer. Here, it directly compares the query embedding to the vector embeddings of all the leaf nodes (the original text chunks) and summary nodes. This method works best for factual, keyword-based queries where you need specific details.In the collapsed tree retrieval, we flatten the tree into one layer, retrieving nodes based on cosine similarity until we reach a specified number of top k documents. In our code, we’ll gather the textual chunks from earlier, along with the summarized nodes at each level for all the clusters, to create one big list of texts that includes both the root documents and the summarized nodes.To be honest, if you look closely, we’ve been essentially adding more data points (chunks) to our RAG setup all along. Using RAPTOR, we now have both the original chunks and the summarized chunks for each cluster. Now, we’ll simply embed all these new data points and store them in a vector database along with their embeddings and use them for RAG.raptor_texts = []for level, row in results.items():    if level.endswith(\"clusters\"):        raptor_texts.extend(row[\"texts\"])    else:        raptor_texts.extend(row[\"summaries\"])        raptor_embeddings = embedding_model.encode(raptor_texts)len(raptor_embeddings)Setting up Vector Database and RAGNow it’s smooth sailing! We’ll just set up a LanceDB vector database to store our embeddings and query our RAG setup.raptor_embeddings = embedding_model.encode(raptor_texts)raptor_dict = {\"texts\": [], \"embeddings\": []}for texts, embeddings in zip(raptor_texts, raptor_embeddings):    raptor_dict[\"texts\"].append(texts)    raptor_dict[\"embeddings\"].append(embeddings.tolist())import lancedbimport pyarrow as pafrom lancedb.pydantic import Vector, LanceModeluri = \"lancedb_database\"db = lancedb.connect(uri)class RAG(LanceModel):    texts : str    embeddings : Vector(384)table_name = \"rag_with_raptor\"raptor_table = db.create_table(table_name, schema = RAG, mode=\"overwrite\")raptor_table.add(rag_raptor_df)raptor_table.create_fts_index(\"texts\", replace=True)Time to generate the results..def generate_results(    query : str,    context_text : str) -&gt; str:    prompt = f\"\"\"    Based on the context provided, use it to answer the query.     query : {query}    Instructions:    1. Don't make things up, Just use the contexts and generate the relevant answer.    2. Don't mix the numbers, Just use the numbers in the context.    3. Don't try to use fancy words, stick to the basics of the language that is being used in the context.        {context_text}    \"\"\"    response = client.chat.completions.create(        model=\"gpt-4\",         messages=[            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers query and give the answers.\"},            {\"role\": \"user\", \"content\": prompt}        ],        max_tokens=200,        n=1,        stop=None,        temperature=0.7    )    answer = response.choices[0].message.content.strip()    return answerquery = \"NTT DATA's net income attributable to shareholders increased from ¥69,227 million in Q3 FY2020 to ¥110,191 million in Q3 FY2021. How does this growth align with their acquisition strategy, particularly considering their stated reasons for acquiring Nexient, LLC and the provisional goodwill recorded in this transaction?\"In our query, there are several key points that must be addressed when crafting the answers. First, we need to note the increase in net income from ¥69,227 million in Q3 FY2020 to ¥110,191 million in Q3 FY2021. Second, we should examine how this growth aligns with NTT DATA’s acquisition strategy, particularly their reasons for acquiring Nexient, LLC, and the provisional goodwill recorded in the transaction. With this context in mind, I created a VANILLA RAG to compare its results with those of RAPTOR RAG.normal_embeddings = embedding_model.encode(chunks) # default chunks from our datanormal_dict = {\"texts\": [], \"embeddings\": []}for texts, embeddings in zip(chunks, normal_embeddings):    normal_dict[\"texts\"].append(texts)    normal_dict[\"embeddings\"].append(embeddings.tolist())    rag_normal_df = pd.DataFrame(normal_dict)table_name = \"rag_without_raptor\"normal_table = db.create_table(table_name, schema = RAG, mode=\"overwrite\")normal_table.add(rag_normal_df)normal_table.create_fts_index(\"texts\", replace=True)With RAPTOR, we now have an increased number of chunks due to the addition of cluster-level summary nodes alongside the default chunks we had earlier.D-Dayraptor_contexts = raptor_table.search(query).limit(5).select([\"texts\"]).to_list()raptor_context_text = \"------\\n\\n\".join([context[\"texts\"] for context in raptor_contexts])raptor_context_text = \"------\\n\\n\" + raptor_context_textnormal_contexts = normal_table.search(query).limit(5).select([\"texts\"]).to_list()normal_context_text = \"------\\n\\n\".join([context[\"texts\"] for context in normal_contexts])normal_context_text = \"------\\n\\n\" + normal_context_textraptor_answer = generate_results(query, raptor_context_text)normal_answer = generate_results(query, normal_context_text)When we are comparing RAPTOR RAG with Vanilla RAG, it’s clear that RAPTOR performs better. Not only does RAPTOR retrieve details about the financial growth, but it also effectively connects this growth to the broader acquisition strategy, pulling relevant context from multiple sources. It excels in situations like this, where the query requires insights from various pages, making it more adept at handling complex, layered information retrieval.And that’s a wrap for this article! If you want to dig into the intricacies of how everything works, I’d suggest checking out the official RAPTOR GitHub repository for more info and resources. For an even deeper dive, the official paper is a great read and highly recommended!  Here is the Google colab for your reference.",
            "content_html": "<p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/improving-raptor-with-rag/raptor-title.png?raw=true\" alt=\"title-image\" /></p><p>Traditional <a href=\"https://vipul-maheshwari.github.io/2024/02/14/rag-application-with-langchain\">RAG</a> setups often split documents into fixed-size chunks, but this can lead to problems in maintaining the semantic coherence of the text. If a key concept spans multiple chunks, and we only retrieve one chunk, the LLM might lack the full understanding of the idea, leading to incomplete or misleading responses. As a result, crucial ideas and relationships between concepts may be overlooked, leading to incomplete or inaccurate responses.</p><p>Additionally, In a flat retrieval structure where all the retrieved chunks are treated equally, this can dilute the importance of critical sections. For example, if one section of the document has key insights but gets buried among less relevant chunks, the model won’t know which parts to prioritize unless we introduce more intelligent weighting or hierarchical structures. I mean it becomes really difficult during the retrieval to weigh which chunk is more important and might be better suitable as a context.</p><h3 id=\"what-is-raptor\">What is RAPTOR?</h3><p>RAPTOR, which stands for Recursive Abstractive Processing for Tree Organized Retrieval, is a new technique which solves the problems mentioned before. Think of RAPTOR as a librarian who organizes information in a tree-like structure. Instead of simply stacking books in a pile, it clusters similar titles together, creating a hierarchy that narrows as you ascend. Each cluster of books represents a group of related documents, and at the top of each cluster, there’s a summary that encapsulates the key points from all the books below it. This process continues all the way to the top of the tree, providing a comprehensive view of the information—it’s like having both a magnifying glass and a telescope!</p><p>To visualize this further, think of the leaves of the tree as document chunks. These chunks are grouped into clusters to generate meaningful summaries, which then become the new leaves of the tree. This recursive process repeats until reaching the top.</p><h3 id=\"key-terms-to-look-out-for\">Key terms to look out for</h3><p>Before we dive in, let’s quickly review some key terms that will be useful as we explore <strong>RAPTOR</strong> tech. I just want to put it up here to make sure you are comfortable with the nitty tech details as we go along.</p><ol>  <li><strong>GMM Clustering</strong>: Gaussian Mixture Models (GMM) group data into clusters based on statistical probabilities. So instead of rigidly classifying each instance into one category like K-means, GMM generates K-Gaussian distributions that consider the entire training space. This means that each point can belong to one or more distributions.</li>  <li><strong>Dimensionality Reduction</strong>: This process simplifies the data by reducing the number of variables while retaining essential features. It’s particularly important for understanding high-dimensional datasets like embeddings.</li>  <li><strong>UMAP</strong>: Uniform Manifold Approximation and Projection (UMAP) is a powerful dimensionality reduction algorithm we’ll use to shrink the size of our data point embeddings. This reduction makes it easier for clustering algorithms like GMM to cluster high-dimensional embeddings.</li>  <li><strong>BIC and Elbow Method</strong>: Both techniques help identify the optimal number of clusters in a dataset. The Bayesian Information Criterion (BIC) evaluates models based on their fit to the data while penalizing complexity. The Elbow Method plots explained variance against the number of clusters, helping to pinpoint where adding more clusters offers diminishing returns. For our purposes, we’ll leverage both methods to determine the best number of clusters.</li></ol><h3 id=\"how-it-actually-works\">How it actually works?</h3><p>Now that you’re familiar with the key terms (and if not, no worries—you’ll catch on as we go!), let’s dive into how everything actually works under the hood of RAPTOR.</p><ul>  <li><strong>Starting Documents as Leaves</strong>: The leaves of the tree represent a set of initial documents, which are our text chunks.</li>  <li><strong>Embedding and Clustering</strong>: The leaves are embedded and clustered. The authors utilize the UMAP dimensionality reduction algorithm to minimize the embedding size of these chunks. For clustering, Gaussian Mixture Models (GMM) are employed to ensure effective grouping, addressing the challenges posed by high-dimensional vector embeddings.</li>  <li><strong>Summarizing Clusters</strong>: Once clustered, these groups of similar chunks are summarized into higher-level abstractions nodes. Each cluster acts like a basket for similar documents, and the individual summaries encapsulate the essence of all nodes within that cluster. This process builds from the bottom up, where nodes are clustered together to create summaries that are then passed up the hierarchy.</li>  <li><strong>Recursive Process</strong>: This entire procedure is recursive, resulting in a tree structure that transitions from raw documents (the leaves) to more abstract summaries, with each summary derived from the clusters of various nodes.</li></ul><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/improving-raptor-with-rag/raptor-workflow.png?raw=true\" alt=\"RAPTOR-workflow\" /></p><h3 id=\"building-the-raptor\">Building the RAPTOR</h3><p>Now that we’ve unpacked how it all works (and you’re still with me hopefully, right?), let’s shift gears and talk about how we actually build the RAPTOR tree.</p><h3 id=\"setup-and-imports\">Setup and Imports</h3><pre><code class=\"language-python\">pip install lancedb scikit-learn openai torch sentence_transformers tiktoken umap-learn PyPDF2</code></pre><pre><code class=\"language-python\">import osimport uuidimport tiktokenimport reimport numpy as npimport pandas as pdimport transformersimport torchimport umap.umap_ as umapimport matplotlib.pyplot as pltfrom openai import OpenAIfrom typing import List, Tuple, Optional, Dictfrom sklearn.mixture import GaussianMixturefrom sentence_transformers import SentenceTransformeropenai_api_key = \"sk-XXXXXXXXXXXXXXX\"client = OpenAI(api_key=openai_api_key)</code></pre><h3 id=\"creating-the-chunks\">Creating the Chunks</h3><p>Setting up RAPTOR is pretty straightforward and builds on what we’ve already covered. The first step is to break down our textual documents into smaller chunks. Once we have those, we can convert them into dense vector embeddings. To simplify things, our use case example will use just <a href=\"https://www.nttdata.com/global/en/-/media/nttdataglobal/1_files/investors/financial-results/2021/fy2021_fs_3q.pdf\">one financial document</a> as a source. Download it and add to a directory. For now, I used a <code>data</code> directory where all the documents can be added. You can of course load any source documents (and alter the query below) in your own experiments!</p><p>Note: Make sure you use an unencrypted version of this file to avoid errors when PyPDF2 tries to read the file.</p><pre><code class=\"language-python\">import osimport PyPDF2# Function to extract text from a PDF filedef extract_pdf_text(file_path):    with open(file_path, 'rb') as file:        reader = PyPDF2.PdfReader(file)        text = \"\"        for page_num in range(len(reader.pages)):            page = reader.pages[page_num]            text += page.extract_text()    return text# Function to split text into chunks with overlapdef split_text(text, chunk_size=1000, chunk_overlap=50):    chunks = []    start = 0    while start &lt; len(text):        end = min(start + chunk_size, len(text))        chunks.append(text[start:end])        start += chunk_size - chunk_overlap    return chunks# Function to process all PDFs in a directorydef process_directory(directory_path, chunk_size=1000, chunk_overlap=50):    all_chunks = []    # Iterate over all PDF files in the directory    for filename in os.listdir(directory_path):        if filename.endswith(\".pdf\"):            file_path = os.path.join(directory_path, filename)            print(f\"Processing file: {file_path}\")                        # Step 1: Extract text from the PDF            pdf_text = extract_pdf_text(file_path)                        # Step 2: Split the extracted text into chunks            chunks = split_text(pdf_text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)                        # Append chunks from this file to the overall list            all_chunks.extend(chunks)        return all_chunksdirectory_path = os.path.join(os.getcwd(), \"data\")  # Specify your directory pathchunk_size = 1000chunk_overlap = 50# Process all PDF files in the directory and get the chunkschunks = process_directory(directory_path, chunk_size=chunk_size, chunk_overlap=chunk_overlap)# Optional: Print the number of chunks and preview some of the chunksprint(f\"Total number of chunks: {len(chunks)}\")for i, chunk in enumerate(chunks[:5]):  # Show first 5 chunks as a preview    print(f\"Chunk {i+1}:\\n{chunk}\\n\")</code></pre><p>Now that we have our chunks, it’s time to dive into the recursive processing to create summarized nodes. For the embedding part, I’ll be using the all-MiniLM-L6-v2 model from Sentence Transformers, but feel free to choose any embedding model that suits your needs—it’s entirely up to you!</p><pre><code class=\"language-python\">embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')</code></pre><h3 id=\"clustering-and-dimensionality-reduction\">Clustering and Dimensionality Reduction</h3><p>Now we have our embedded chunks, it’s time to step on for the next set of tasks. When diving into RAPTOR, one of the biggest hurdles we encounter is the high dimensionality of vector embeddings. Traditional clustering methods like Gaussian Mixture Models (GMM) often struggle with this complexity, making it tough to effectively cluster high-dimensional data chunks. To tackle this challenge, we turn to <strong>Uniform Manifold Approximation and Projection (UMAP)</strong>. UMAP excels at simplifying data while preserving the essential structures that matter most.</p><p>A key factor in UMAP’s effectiveness is the <strong><code>n_neighbors</code></strong> parameter. This setting dictates how much of the data’s neighborhood UMAP considers during dimensionality reduction. In simpler terms, it helps you choose between zooming in on details or taking a broader view:</p><ul>  <li><strong>Higher <code>n_neighbors</code>:</strong> A higher value encourages UMAP to “look at many neighbors,” which helps maintain the <strong>global structure</strong> of the data. This results in larger, more general clusters.</li>  <li><strong>Lower <code>n_neighbors</code>:</strong> Conversely, lowering <code>n_neighbors</code> prompts UMAP to “focus on close relationships,” enabling it to preserve the <strong>local structure</strong> and form smaller, more detailed clusters.</li></ul><p><strong>Think of it this way:</strong> Imagine you’re at a party. If you take a step back and look around (high <code>n_neighbors</code>), you can see the whole room—where the groups are forming, who’s mingling, and the general vibe. But if you lean in closer to a specific group (low <code>n_neighbors</code>), you can hear their conversation and pick up on the nuances, like inside jokes or shared interests. Both perspectives are valuable; it just depends on what you want to understand.</p><p>In RAPTOR, we leverage this flexibility in <code>n_neighbors</code> to create a <strong>hierarchical clustering structure</strong>. We first run UMAP with a higher <code>n_neighbors</code> to identify the <strong>global clusters</strong>—the broad categories. Then, we narrow the focus by lowering the value to uncover <strong>local clusters</strong> within those broader groups. This two-step approach ensures we capture both large-scale patterns and intricate details.</p><h3 id=\"well-tldr\">Well, TL;DR</h3><ol>  <li><strong>Dimensionality Reduction</strong> helps manage high-dimensional data, and UMAP is our primary tool for that.</li>  <li>The <strong><code>n_neighbors</code></strong> parameter controls the balance between seeing the “big picture” and honing in on local details.</li>  <li>The clustering process begins with <strong>global clusters</strong> (using high <code>n_neighbors</code>), followed by a focus on <strong>local clusters</strong> with a lower setting of <code>n_neighbors</code>.</li></ol><pre><code class=\"language-python\">def dimensionality_reduction(    embeddings: np.ndarray,    target_dim: int,    clustering_type: str,    metric: str = \"cosine\",) -&gt; np.ndarray:    if clustering_type == \"local\":        n_neighbors = max(2, min(10, len(embeddings) - 1))        min_dist = 0.01    elif clustering_type == \"global\":        n_neighbors = max(2, min(int((len(embeddings) - 1) ** 0.5), len(embeddings) // 10, len(embeddings) - 1))        min_dist = 0.1    else:        raise ValueError(\"clustering_type must be either 'local' or 'global'\")    umap_model = umap.UMAP(        n_neighbors=n_neighbors,        min_dist=min_dist,        n_components=target_dim,        metric=metric,    )    return umap_model.fit_transform(embeddings)</code></pre><p>I plan to leverage both the Elbow Method and the Bayesian Information Criterion (BIC) to pinpoint the optimal number of clusters for our analysis.</p><pre><code class=\"language-python\">def compute_inertia(embeddings: np.ndarray, labels: np.ndarray, centroids: np.ndarray) -&gt; float:    return np.sum(np.min(np.sum((embeddings[:, np.newaxis] - centroids) ** 2, axis=2), axis=1))def optimal_cluster_number(    embeddings: np.ndarray,    max_clusters: int = 50,    random_state: int = SEED) -&gt; int:    max_clusters = min(max_clusters, len(embeddings))    number_of_clusters = np.arange(1, max_clusters + 1)    inertias = []    bic_scores = []        for n in number_of_clusters:        gmm = GaussianMixture(n_components=n, random_state=random_state)        labels = gmm.fit_predict(embeddings)        centroids = gmm.means_        inertia = compute_inertia(embeddings, labels, centroids)        inertias.append(inertia)        bic_scores.append(gmm.bic(embeddings))        inertia_changes = np.diff(inertias)    elbow_optimal = number_of_clusters[np.argmin(inertia_changes) + 1]    bic_optimal = number_of_clusters[np.argmin(bic_scores)]        return max(elbow_optimal, bic_optimal)def gmm_clustering(    embeddings: np.ndarray,     threshold: float,     random_state: int = SEED) -&gt; Tuple[List[np.ndarray], int]:    n_clusters = optimal_cluster_number(embeddings, random_state=random_state)    gm = GaussianMixture(n_components=n_clusters, random_state=random_state, n_init=2)    gm.fit(embeddings)    probs = gm.predict_proba(embeddings)    labels = [np.where(prob &gt; threshold)[0] for prob in probs]     return labels, n_clusters  </code></pre><h3 id=\"tree-construction\">Tree Construction</h3><p>Now that we’ve wrapped up the clustering part, let’s talk about how we build our hierarchical tree. After several rounds of clustering and summarization (while keeping track of how deep we go), here’s what we have:</p><ul>  <li><strong>Leaf Nodes:</strong> These are our original text chunks, forming the base of the tree.</li>  <li><strong>Summary Nodes:</strong> As we go up the tree, each node acts like a quick summary of its child nodes, capturing the main idea of the cluster.</li>  <li><strong>Hierarchical Embeddings:</strong> The summary nodes can also become the new nodes at their level. Each of these nodes gets its own vector embedding, representing the summarized meaning. So, we’re essentially adding more nodes while enriching them with summaries.</li></ul><p>The process flows nicely: we embed the chunks, reduce their dimensions using UMAP, cluster them with Gaussian Mixture Models, start with a broad overview, and then zoom in for more detailed clusters before summarizing.</p><pre><code class=\"language-python\">def clustering_algorithm(    embeddings: np.ndarray,    target_dim: int,    threshold: float,    random_state: int = SEED) -&gt; Tuple[List[np.ndarray], int]:    if len(embeddings) &lt;= target_dim + 1:        return [np.array([0]) for _ in range(len(embeddings))], 1        # Global clustering    reduced_global_embeddings = dimensionality_reduction(embeddings, target_dim, \"global\")    global_clusters, n_global_clusters = gmm_clustering(reduced_global_embeddings, threshold, random_state=random_state)    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]    total_clusters = 0    # Local clustering within each global cluster    for i in range(n_global_clusters):        global_cluster_mask = np.array([i in gc for gc in global_clusters])        global_cluster_embeddings = embeddings[global_cluster_mask]        if len(global_cluster_embeddings) &lt;= target_dim + 1:            # Assign all points in this global cluster to a single local cluster            for idx in np.where(global_cluster_mask)[0]:                all_local_clusters[idx] = np.append(all_local_clusters[idx], total_clusters)            total_clusters += 1            continue        try:            reduced_local_embeddings = dimensionality_reduction(global_cluster_embeddings, target_dim, \"local\")            local_clusters, n_local_clusters = gmm_clustering(reduced_local_embeddings, threshold, random_state=random_state)            # Assign local cluster IDs            for j in range(n_local_clusters):                local_cluster_mask = np.array([j in lc for lc in local_clusters])                global_indices = np.where(global_cluster_mask)[0]                local_indices = global_indices[local_cluster_mask]                for idx in local_indices:                    all_local_clusters[idx] = np.append(all_local_clusters[idx], j + total_clusters)            total_clusters += n_local_clusters        except Exception as e:            print(f\"Error in local clustering for global cluster {i}: {str(e)}\")            # Assign all points in this global cluster to a single local cluster            for idx in np.where(global_cluster_mask)[0]:                all_local_clusters[idx] = np.append(all_local_clusters[idx], total_clusters)            total_clusters += 1    return all_local_clusters, total_clusters     def generate_summary(context: str) -&gt; str:    prompt = f\"\"\"    Provide the Summary for the given context. Here are some additional instructions for you:    Instructions:    1. Don't make things up, Just use the contexts and generate the relevant summary.    2. Don't mix the numbers, Just use the numbers in the context.    3. Don't try to use fancy words, stick to the basics of the language that is being used in the context.    Context: {context}    \"\"\"    response = client.chat.completions.create(        model=\"gpt-4\",        messages=[            {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},            {\"role\": \"user\", \"content\": prompt}        ],        max_tokens=200,        n=1,        stop=None,        temperature=0.7    )    summary = response.choices[0].message.content.strip()    return summarydef embed_clusters(    texts: List[str],    target_dim: int = 10,    threshold: float = 0.1) -&gt; pd.DataFrame:    textual_embeddings = np.array(embedding_model.encode(texts))    clusters, number_of_clusters = clustering_algorithm(textual_embeddings, target_dim, threshold)    print(f\"Number of clusters: {number_of_clusters}\")    return pd.DataFrame({        \"texts\": texts,        \"embedding\": list(textual_embeddings),        \"clusters\": clusters    })def embed_cluster_summaries(    texts: List[str],    level: int,    target_dim: int = 10,    threshold: float = 0.1) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:    df_clusters = embed_clusters(texts, target_dim, threshold)    main_list = []        for _, row in df_clusters.iterrows():        for cluster in row[\"clusters\"]:            main_list.append({                \"text\": row[\"texts\"],                \"embedding\": row[\"embedding\"],                \"clusters\": cluster            })        main_df = pd.DataFrame(main_list)    unique_clusters = main_df[\"clusters\"].unique()    if len(unique_clusters) == 0:        return df_clusters, pd.DataFrame(columns=[\"summaries\", \"level\", \"clusters\"])    print(f\"--Generated {len(unique_clusters)} clusters--\")    summaries = []    for cluster in unique_clusters:        text_in_df = main_df[main_df[\"clusters\"] == cluster]        unique_texts = text_in_df[\"text\"].tolist()        text = \"------\\n------\".join(unique_texts)        summary = generate_summary(text)        summaries.append(summary)    df_summaries = pd.DataFrame({        \"summaries\": summaries,        \"level\": [level] * len(summaries),        \"clusters\": unique_clusters    })    return df_clusters, df_summariesdef recursive_embedding_with_cluster_summarization(    texts: List[str],    number_of_levels: int = 3,    level: int = 1,    target_dim: int = 10,    threshold: float = 0.1) -&gt; Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:    if level &gt; number_of_levels:        return {}        results = {}    df_clusters, df_summaries = embed_cluster_summaries(texts, level, target_dim, threshold)    results[level] = (df_clusters, df_summaries)        if df_summaries.empty or len(df_summaries['clusters'].unique()) == 1:        print(f\"No more unique clusters found at level {level}. Stopping recursion.\")        return results        if level &lt; number_of_levels:        next_level_texts = df_summaries['summaries'].tolist()        next_level_results = recursive_embedding_with_cluster_summarization(            next_level_texts,             number_of_levels,             level + 1,            target_dim,            threshold        )        results.update(next_level_results)        return results</code></pre><p>Okay, the code might seem a bit daunting at first glance, but don’t worry! Just give it a couple of looks, and it will start to make sense. Essentially, we’re just following the flow I mentioned earlier.</p><pre><code class=\"language-python\">def process_text_hierarchy(    texts: List[str],     number_of_levels: int = 3,    target_dim: int = 10,    threshold: float = 0.1) -&gt; Dict[str, pd.DataFrame]:    hierarchy_results = recursive_embedding_with_cluster_summarization(        texts, number_of_levels, target_dim=target_dim, threshold=threshold    )        processed_results = {}    for level, (df_clusters, df_summaries) in hierarchy_results.items():        if df_clusters.empty or df_summaries.empty:            print(f\"No data for level {level}. Skipping.\")            continue        processed_results[f\"level_{level}_clusters\"] = df_clusters        processed_results[f\"level_{level}_summaries\"] = df_summaries        return processed_resultsresults = process_text_hierarchy(chunks, number_of_levels=3)</code></pre><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/improving-raptor-with-rag/building-a-raptor-tree.png?raw=true\" alt=\"self-made-flow\" /></p><h3 id=\"inference\">Inference</h3><p>Now that we have our tree structure with leaf nodes at the bottom and summarized nodes in between, it’s time to query the RAG. There are two main methods for navigating the RAPTOR tree: Tree Traversal and Collapsed Tree Retrieval.</p><ol>  <li><strong>Tree Traversal Retrieval:</strong> This method systematically explores the tree, starting from the root node. It first selects the top-k most relevant root nodes based on their cosine similarity to the query embedding. Then, for each selected root node, its children are considered in the next layer, where the top-k nodes are again selected based on their cosine similarity to the query vector. This process repeats until we reach the leaf nodes. Finally, the text from all the selected nodes is concatenated to form the retrieved context.</li>  <li><strong>Collapsed Tree Retrieval:</strong> This approach simplifies things by viewing the tree as a single layer. Here, it directly compares the query embedding to the vector embeddings of all the leaf nodes (the original text chunks) and summary nodes. This method works best for factual, keyword-based queries where you need specific details.</li></ol><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/improving-raptor-with-rag/raptor-reference.png?raw=true\" alt=\"raptor-methods\" /></p><p>In the collapsed tree retrieval, we flatten the tree into one layer, retrieving nodes based on cosine similarity until we reach a specified number of <strong><em>top k documents</em></strong>. In our code, we’ll gather the textual chunks from earlier, along with the summarized nodes at each level for all the clusters, to create one big list of texts that includes both the root documents and the summarized nodes.</p><p>To be honest, if you look closely, we’ve been essentially adding more data points (chunks) to our RAG setup all along. Using RAPTOR, we now have both the original chunks and the summarized chunks for each cluster. Now, we’ll simply embed all these new data points and store them in a vector database along with their embeddings and use them for RAG.</p><pre><code class=\"language-python\">raptor_texts = []for level, row in results.items():    if level.endswith(\"clusters\"):        raptor_texts.extend(row[\"texts\"])    else:        raptor_texts.extend(row[\"summaries\"])        raptor_embeddings = embedding_model.encode(raptor_texts)len(raptor_embeddings)</code></pre><h3 id=\"setting-up-vector-database-and-rag\">Setting up Vector Database and RAG</h3><p>Now it’s smooth sailing! We’ll just set up a LanceDB vector database to store our embeddings and query our RAG setup.</p><pre><code class=\"language-python\">raptor_embeddings = embedding_model.encode(raptor_texts)raptor_dict = {\"texts\": [], \"embeddings\": []}for texts, embeddings in zip(raptor_texts, raptor_embeddings):    raptor_dict[\"texts\"].append(texts)    raptor_dict[\"embeddings\"].append(embeddings.tolist())</code></pre><pre><code class=\"language-python\">import lancedbimport pyarrow as pafrom lancedb.pydantic import Vector, LanceModeluri = \"lancedb_database\"db = lancedb.connect(uri)class RAG(LanceModel):    texts : str    embeddings : Vector(384)table_name = \"rag_with_raptor\"raptor_table = db.create_table(table_name, schema = RAG, mode=\"overwrite\")raptor_table.add(rag_raptor_df)raptor_table.create_fts_index(\"texts\", replace=True)</code></pre><p>Time to generate the results..</p><pre><code class=\"language-python\">def generate_results(    query : str,    context_text : str) -&gt; str:    prompt = f\"\"\"    Based on the context provided, use it to answer the query.     query : {query}    Instructions:    1. Don't make things up, Just use the contexts and generate the relevant answer.    2. Don't mix the numbers, Just use the numbers in the context.    3. Don't try to use fancy words, stick to the basics of the language that is being used in the context.        {context_text}    \"\"\"    response = client.chat.completions.create(        model=\"gpt-4\",         messages=[            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers query and give the answers.\"},            {\"role\": \"user\", \"content\": prompt}        ],        max_tokens=200,        n=1,        stop=None,        temperature=0.7    )    answer = response.choices[0].message.content.strip()    return answer</code></pre><pre><code class=\"language-python\">query = \"NTT DATA's net income attributable to shareholders increased from ¥69,227 million in Q3 FY2020 to ¥110,191 million in Q3 FY2021. How does this growth align with their acquisition strategy, particularly considering their stated reasons for acquiring Nexient, LLC and the provisional goodwill recorded in this transaction?\"</code></pre><p>In our query, there are several key points that must be addressed when crafting the answers. First, we need to note the increase in net income from ¥69,227 million in Q3 FY2020 to ¥110,191 million in Q3 FY2021. Second, we should examine how this growth aligns with NTT DATA’s acquisition strategy, particularly their reasons for acquiring Nexient, LLC, and the provisional goodwill recorded in the transaction. With this context in mind, I created a VANILLA RAG to compare its results with those of RAPTOR RAG.</p><pre><code class=\"language-python\">normal_embeddings = embedding_model.encode(chunks) # default chunks from our datanormal_dict = {\"texts\": [], \"embeddings\": []}for texts, embeddings in zip(chunks, normal_embeddings):    normal_dict[\"texts\"].append(texts)    normal_dict[\"embeddings\"].append(embeddings.tolist())    rag_normal_df = pd.DataFrame(normal_dict)table_name = \"rag_without_raptor\"normal_table = db.create_table(table_name, schema = RAG, mode=\"overwrite\")normal_table.add(rag_normal_df)normal_table.create_fts_index(\"texts\", replace=True)</code></pre><p>With RAPTOR, we now have an increased number of chunks due to the addition of cluster-level summary nodes alongside the default chunks we had earlier.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/improving-raptor-with-rag/chunk-comparison.png?raw=true\" alt=\"comparison-of-raptor-chunks\" /></p><h3 id=\"d-day\">D-Day</h3><pre><code class=\"language-python\">raptor_contexts = raptor_table.search(query).limit(5).select([\"texts\"]).to_list()raptor_context_text = \"------\\n\\n\".join([context[\"texts\"] for context in raptor_contexts])raptor_context_text = \"------\\n\\n\" + raptor_context_textnormal_contexts = normal_table.search(query).limit(5).select([\"texts\"]).to_list()normal_context_text = \"------\\n\\n\".join([context[\"texts\"] for context in normal_contexts])normal_context_text = \"------\\n\\n\" + normal_context_textraptor_answer = generate_results(query, raptor_context_text)normal_answer = generate_results(query, normal_context_text)</code></pre><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/improving-raptor-with-rag/raptor-vs-vanilla.png?raw=true\" alt=\"difference-between-raptor-and-vanilla\" /></p><p>When we are comparing RAPTOR RAG with Vanilla RAG, it’s clear that RAPTOR performs better. Not only does RAPTOR retrieve details about the financial growth, but it also effectively connects this growth to the broader acquisition strategy, pulling relevant context from multiple sources. It excels in situations like this, where the query requires insights from various pages, making it more adept at handling complex, layered information retrieval.</p><p>And that’s a wrap for this article! If you want to dig into the intricacies of how everything works, I’d suggest checking out the official RAPTOR <a href=\"https://github.com/parthsarthi03/raptor/tree/master\">GitHub repository</a> for more info and resources. For an even deeper dive, the official <a href=\"https://arxiv.org/pdf/2401.18059\">paper</a> is a great read and highly recommended!  Here is the Google <a href=\"https://colab.research.google.com/drive/1I3WI0U4sgb2nc1QTQm51kThZb2q4MXyr?usp=sharing\">colab</a> for your reference.</p>",
            "url": "http://localhost:4000/2024/10/30/improving-rag-with-raptor",
            
            
            
            "tags": ["LLM","RAG"],
            
            "date_published": "2024-10-30T00:00:00+05:30",
            "date_modified": "2024-10-30T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/08/16/multi-document-agentic-rag",
            "title": "Multi Document Agentic RAG",
            "summary": "This post shows how you can create a Multi Document Agentic RAG using LanceDB.",
            "content_text": "Agentic RAG (Retrieval-Augmented Generation) represents a significant leap in how we handle information. Traditional RAG systems are built to retrieve information and relevant context which are then passively send that to a language model (LLM) to generate responses. However, Agentic RAG takes this further by adding more independence. Now, the system can not only gather data but also make decisions and take actions on its own.Think of it as a shift from simple tools to smarter, more capable systems. Agentic RAG transforms what was once a passive process into an active one, where AI can work towards specific goals without needing constant guidance.How Does an Agentic RAG Work?To understand Agentic RAG, let’s first break down what an “agent” is. Simply put, an agent is a smart system capable of making decisions on its own. When given a question or task, it figures out the best way to handle it by breaking the task into smaller steps and using the right tools to complete it.Now, when we talk about Agentic RAG, we’re taking this concept further. Instead of just retrieving information like regular RAG, Agentic RAG uses intelligent strategies to ensure the system provides the best possible response. It doesn’t stop at giving a basic, context-aware reply. Instead, these agents think through the query, select the right approach, and deliver a more thoughtful and refined answer. It’s like having a team of smart agents working together to solve a problem, one step at a time.Here’s how the process works:  Understanding the Query: The agent starts by analyzing the question to grasp its specifics. It looks at the context, purpose, and intent to determine what information is needed.  Using Memory: The agent checks its memory for relevant information from past tasks that might assist with the current query.  Choosing Tools: Agentic RAG agents are designed to be intuitive. After analyzing the query, they evaluate available tools and resources to select the best method for retrieving the precise information. It’s like having a savvy assistant who knows exactly where to look, even for the most challenging questions.How to Use it?Let’s dive into developing an automotive-themed RAG agent based on our current understanding.Consider a scenario where you own a vehicle and require assistance with tasks ranging from diagnosing issues to planning routine maintenance. Now imagine a specialized agent designed specifically for this purpose. This agent should be capable of interpreting your car’s symptoms, analyzing the issue, and delivering a detailed diagnosis, including potential causes.Furthermore, the bot should assist with identifying specific parts, estimating repair costs, or creating a personalized maintenance schedule based on your vehicle’s mileage and model. This agent must effectively manage a range of tasks by utilizing relevant context from various datasources, employing different tools, and reasoning with the available information to provide accurate and meaningful responses. Given the complexity of these tasks, which involves multiple retrieval and reasoning steps, I am going to use LanceDB to ensure fast retrieval by storing relevant embedded data chunks into it.To meet our data needs, I will use six JSON files, each containing specific types of information for querying. You can get these JSON files here : DataHere’s a brief overview of each file:  car_maintenance.json: Contains details about the car’s maintenance schedule, including relevant tasks based on mileage and estimated time for completion.  car_problems.json: Provides information on various car problems, including a brief description of each issue, required parts for resolution, estimated repair time, and other relevant metadata.  car_parts.json: Lists car parts used for maintenance and diagnosis, detailing their brands, categories, whether they are solid or liquid, and other relevant attributes.  car_diagnosis.json: Outlines the diagnosis process, potential causes based on symptoms, recommended actions, and related problems. The issues and parts mentioned should align with those in car_problems.json and car_parts.json to ensure the agent has relevant context for problem-solving.  car_cost_estimates.json: Provides cost estimates for addressing car problems, based on the issues listed in car_problems.json.  car_models.json: Contains information on common problems associated with specific car models, such as a 2017 Honda Accord with 190,000 kilometers, detailing typical issues that users might encounter in this range.Please review the JSON files to see their structure and feel free to make any changes.How to: Tech StackTo build our ReAct-like agent, we’ll be using a few key tools to make things run smoothly:      LlamaIndex: Think of LlamaIndex as the backbone of our agent. This framework will be central to our implementation. It facilitates the abstraction of agentic logic. If the specifics seem unclear now, they will become more evident as we proceed with the implementation.        Memory Management: When we query the agent, it handles each question on its own without remembering past interactions or in isolation without maintaining state. To address this, we will use memory to retain conversational history. The agent stores chat history in a conversational memory buffer, which by default is a flat list managed by LlamaIndex. This ensures that the agent can refer to past as well as current conversation when deciding on the next set of actions.        Vector Databases: For the retrieval process, we will use VectorDBs. Queries will be embedded and matched semantically against the relevant VectorDB through our retrievers. We will employ LanceDB due to its exceptional retrieval speed and on-disk storage capabilities, which allows for local management of our database. Additionally, being open-source and free, it fits within our budget constraints.        LLM Integration: On the language model side, we’ll go with OpenAI’s GPT-4 for generating responses. For embeddings, we’re using Hugging face which provides seamless integration of local embedding models.  Ok I think this is enough, let’s dive in for our code part..Environment Setupimport osimport tqdmimport jsonimport timefrom typing import List, Optional, Dict, Anyfrom tqdm import tqdmimport loggingfrom llama_index.core import (    SimpleDirectoryReader,     VectorStoreIndex,     StorageContext,    Settings,    Document,)from datetime import datetime, timedeltafrom llama_index.core.node_parser import SentenceSplitterfrom llama_index.vector_stores.lancedb import LanceDBVectorStorefrom llama_index.embeddings.huggingface import HuggingFaceEmbeddingfrom llama_index.llms.openai import OpenAIfrom llama_index.core.tools import FunctionTool, ToolOutputfrom llama_index.core.retrievers import VectorIndexRetrieverfrom llama_index.core.agent import FunctionCallingAgentWorkerfrom llama_index.core.agent import AgentRunnerimport osos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"# Load environment variablesOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')if not OPENAI_API_KEY:    raise ValueError(\"Please set the OPENAI_API_KEY environment variable.\")# LLM setupllm = OpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)# Embedding model setupembed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")# Update the Settings with the new embedding modelSettings.embed_model = embed_modelSettings.chunk_size = 512Make sure your .env file includes the OPENAI_API_KEY. You can adjust and use different LLMs and embedding models as needed. The key is to have an LLM for reasoning and an embedding model to handle data embedding. Feel free to experiment with various models to find the best fit for your needs.Step 1 : Creating our DBsLet’s setup our database which will be used to store our data.# Vector store setupproblems_vector_store = LanceDBVectorStore(    uri='./lancedb',    table_name='problems_table',    mode=\"overwrite\",)parts_vector_store = LanceDBVectorStore(    uri='./lancedb',    table_name='parts_table',    mode=\"overwrite\",)diagnostics_vector_store = LanceDBVectorStore(    uri='./lancedb',    table_name='diagnostics_table',    mode=\"overwrite\",)cost_estimates_vector_store = LanceDBVectorStore(    uri='./lancedb',    table_name='cost_estimates_table',    mode=\"overwrite\",)maintenance_schedules_vector_store = LanceDBVectorStore(    uri='./lancedb',    table_name='maintenance_schedules_table',    mode=\"overwrite\",)cars_vector_store = LanceDBVectorStore(    uri='./lancedb',    table_name='car_maintenance_table',    mode=\"overwrite\",)Since we’re dealing with various types of information, we’ll need multiple tables to organize and retrieve our data effectively. The uri specifies that our data is stored in a database called lancedb, which contains different tables for each type of data. Let’s go ahead and load the data into the appropriate tables.def load_and_index_documents(directory: str, vector_store: LanceDBVectorStore) -&gt; VectorStoreIndex:    \"\"\"Load documents from a directory and index them.\"\"\"    documents = SimpleDirectoryReader(input_dir=directory).load_data()    parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)    nodes = parser.get_nodes_from_documents(documents)    storage_context = StorageContext.from_defaults(vector_store=vector_store)    return VectorStoreIndex(nodes, storage_context=storage_context)def create_retriever(index: VectorStoreIndex) -&gt; VectorIndexRetriever:    \"\"\"Create a retriever from the index.\"\"\"    return index.as_retriever(similarity_top_k=5)# Load and index documentsproblems_index = load_and_index_documents(\"../rag-agentic-system/problems\", problems_vector_store)parts_index = load_and_index_documents(\"../rag-agentic-system/parts\", parts_vector_store)cars_index = load_and_index_documents(\"../rag-agentic-system/cars_models\", cars_vector_store)diagnostics_index = load_and_index_documents(\"../rag-agentic-system/diagnostics\", diagnostics_vector_store)cost_estimates_index = load_and_index_documents(\"../rag-agentic-system/cost_estimates\", cost_estimates_vector_store)maintenance_schedules_index = load_and_index_documents(\"../rag-agentic-system/maintenance_schedules\", maintenance_schedules_vector_store)problems_retriever = create_retriever(problems_index)parts_retriever = create_retriever(parts_index)cars_retriever = create_retriever(cars_index)diagnostics_retriever = create_retriever(diagnostics_index)cost_estimates_retriever = create_retriever(cost_estimates_index)maintenance_retriever = create_retriever(maintenance_schedules_index)Each Vector DB will provide a retriever instance, a Python object that returns a list of documents matching a given query. For example, our problems_retriever will fetch documents related to car problems based on the query, while cars_retriever will help identify common issues faced by customers with their vehicles.Keep in mind that if the bot misses some information or seems to hallucinate, it might be due to missing data in our JSON files. If you spot any inaccuracies or gaps, add the relevant data to the JSON files and re-index them to ensure everything stays up to date.Now, let’s test our retrievers to ensure they’re working correctly.Step 2 : Testing our retrieversLet’s test our cost_estimates_retriever to ensure it’s working properly. We’ll use a query related to a brake problem and check if the retriever returns the correct documents about cost estimates. Additionally, I’ll verify if our query engine is accurately interpreting the query and if, by providing the relevant documents, we are receiving the correct response.query = \"My brake pad isn't working or I don't know, but the brakes are poor, and by the way, what's the cost for the solution?\"query_engine = cost_estimates_index.as_query_engine()response = query_engine.query(query)results = cost_estimates_retriever.retrieve(query)print(f\"Response: {response}\")for result in results:    print(f\"Result - Node ID: {result.node_id}\")    print(f\"Relevant Text: {result.text[:100]}...\")      print(f\"Score: {result.score:.3f}\")This is the output I’m receiving in the cellResponse: The cost for the solution to address the issue with your brake pad would be around $100 to $300 for brake pad replacement.Result - Node ID: 51b91e6e-1243-405b-a742-89e09d78616fRelevant Text: [    {        \"repair\": \"Brake pad replacement\",        \"average_cost\": 150,        \"cost_range\": {            \"min\": 100,            \"max\": 300...Score: 0.584Everything looks good—our retriever is working as expected. We’re ready to move on to the next step.Step 3 : Creating our agentic toolsLlamaIndex Agents are designed to process natural language input to execute actions rather than simply generating responses. The effectiveness of these agents relies on how well we abstract and utilize tools. So, what exactly does “tool” mean in this context? To clarify, imagine tools as weapons given to a warrior in battle. Just as a warrior might choose different weapons based on the opponent’s tactics, tools for our agent are like specialized API interfaces that help the agent interact with data sources or reason through queries to deliver the best possible responses.In LlamaIndex, there are various types of tools. One important type is the FunctionTool, which transforms any user-defined function into a tool, capable of inferring the function’s schema and usage. These tools are essential for our agents, allowing them to reason about queries and perform actions effectively.For each tool, it’s essential to provide a clear description of its purpose and functionality, as this helps the agent use the tool effectively. To start, we will create tools to leverage the retriever objects defined earlier.max_context_information = 200def retrieve_problems(query: str) -&gt; str:    \"\"\"Searches the problem catalog to find relevant automotive problems for the query.\"\"\"    docs = problems_retriever.retrieve(query)    information = str([doc.text[:max_context_information]for doc in docs])    return information    def retrieve_parts(query: str) -&gt; str:    \"\"\"Searches the parts catalog to find relevant parts for the query.\"\"\"    docs = parts_retriever.retrieve(query)    information = str([doc.text[:max_context_information]for doc in docs])    return informationdef retrieve_car_details(make: str, model: str, year: int) -&gt; str:    \"\"\"Retrieves the make, model, and year of the car.\"\"\"    docs = car_details_retriever.retrieve(make, model, year)    information = str([doc.text[:max_context_information]for doc in docs])def diagnose_car_problem(symptoms: str) -&gt; str:    \"\"\"Uses the diagnostics database to find potential causes for given symptoms.\"\"\"    docs = diagnostics_retriever.retrieve(symptoms)    information = str([doc.text[:max_context_information]for doc in docs])    return informationdef estimate_repair_cost(problem: str) -&gt; str:    \"\"\"Provides a cost estimate for a given car problem or repair.\"\"\"    docs = cost_estimates_retriever.retrieve(problem)    information = str([doc.text[:max_context_information]for doc in docs])    return informationdef get_maintenance_schedule(mileage: int) -&gt; str:    \"\"\"Retrieves the recommended maintenance schedule based on mileage.\"\"\"    docs = maintenance_retriever.retrieve(str(mileage))    information = str([doc.text[:max_context_information]for doc in docs])    return informationretrieve_problems_tool = FunctionTool.from_defaults(fn=retrieve_problems)retrieve_parts_tool = FunctionTool.from_defaults(fn=retrieve_parts)diagnostic_tool = FunctionTool.from_defaults(fn=diagnose_car_problem)cost_estimator_tool = FunctionTool.from_defaults(fn=estimate_repair_cost)maintenance_retriever_tool = FunctionTool.from_defaults(fn=get_maintenance_schedule)With the retriever tools now set up, our agent can effectively select the appropriate tool based on the query and fetch the relevant contexts. Next, we’ll create additional helper tools that will complement the existing ones, providing the agent with more context and enhancing its reasoning capabilities.def comprehensive_diagnosis(symptoms: str) -&gt; str:    \"\"\"    Provides a comprehensive diagnosis including possible causes, estimated costs, and required parts.        Args:        symptoms: A string describing the car's symptoms.        Returns:        A string with a comprehensive diagnosis report.    \"\"\"    # Use existing tools    possible_causes = diagnose_car_problem(symptoms)        # Extract the most likely cause (this is a simplification)    likely_cause = possible_causes[0] if possible_causes else \"Unknown issue\"        estimated_cost = estimate_repair_cost(likely_cause)    required_parts = retrieve_parts(likely_cause)        report = f\"Comprehensive Diagnosis Report:\\n\\n\"    report += f\"Symptoms: {symptoms}\\n\\n\"    report += f\"Possible Causes:\\n{possible_causes}\\n\\n\"    report += f\"Most Likely Cause: {likely_cause}\\n\\n\"    report += f\"Estimated Cost:\\n{estimated_cost}\\n\\n\"    report += f\"Required Parts:\\n{required_parts}\\n\\n\"    report += \"Please note that this is an initial diagnosis. For accurate results, please consult with our professional mechanic.\"        return reportdef plan_maintenance(mileage: int, car_make: str, car_model: str, car_year: int) -&gt; str:    \"\"\"    Creates a comprehensive maintenance plan based on the car's mileage and details.        Args:        mileage: The current mileage of the car.        car_make: The make of the car.        car_model: The model of the car.        car_year: The year the car was manufactured.        Returns:        A string with a comprehensive maintenance plan.    \"\"\"    car_details = retrieve_car_details(car_make, car_model, car_year)    car_model_info = get_car_model_info(mileage, car_make, car_model, car_year)        plan = f\"Maintenance Plan for {car_year} {car_make} {car_model} at {mileage} miles:\\n\\n\"    plan += f\"Car Details: {car_details}\\n\\n\"        if car_model_info:        plan += f\"Common Issues:\\n\"        for issue in car_model_info['common_issues']:            plan += f\"- {issue}\\n\"                plan += f\"\\nEstimated Time: {car_model_info['estimated_time']}\\n\\n\"    else:        plan += \"No specific maintenance tasks found for this car model and mileage.\\n\\n\"        plan += \"Please consult with our certified mechanic for a more personalized maintenance plan.\"        return plandef create_calendar_invite(event_type: str, car_details: str, duration: int = 60) -&gt; str:    \"\"\"    Simulates creating a calendar invite for a car maintenance or repair event.        Args:        event_type: The type of event (e.g., \"Oil Change\", \"Brake Inspection\").        car_details: Details of the car (make, model, year).        duration: Duration of the event in minutes (default is 60).        Returns:        A string describing the calendar invite.    \"\"\"    # Simulate scheduling the event for next week    event_date = datetime.now() + timedelta(days=7)    event_time = event_date.replace(hour=10, minute=0, second=0, microsecond=0)        invite = f\"Calendar Invite Created:\\n\\n\"    invite += f\"Event: {event_type} for {car_details}\\n\"    invite += f\"Date: {event_time.strftime('%Y-%m-%d')}\\n\"    invite += f\"Time: {event_time.strftime('%I:%M %p')}\\n\"    invite += f\"Duration: {duration} minutes\\n\"    invite += f\"Location: Your Trusted Auto Shop, 123 Main St, Bengaluru, India\\n\\n\"        return invitedef coordinate_car_care(query: str, car_make: str, car_model: str, car_year: int, mileage: int) -&gt; str:    \"\"\"    Coordinates overall car care by integrating diagnosis, maintenance planning, and scheduling.        Args:        query: The user's query or description of the issue.        car_make: The make of the car.        car_model: The model of the car.        car_year: The year the car was manufactured.        mileage: The current mileage of the car.        Returns:        A string with a comprehensive car care plan.    \"\"\"    car_details = retrieve_car_details(car_make, car_model, car_year)        # Check if it's a problem or routine maintenance    if \"problem\" in query.lower() or \"issue\" in query.lower():        diagnosis = comprehensive_diagnosis(query)        plan = f\"Based on your query, here's a diagnosis:\\n\\n{diagnosis}\\n\\n\"                # Extract the most likely cause (this is a simplification)        likely_cause = diagnosis.split(\"Most Likely Cause:\")[1].split(\"\\n\")[0].strip()                # Create a calendar invite for repair        invite = create_calendar_invite(f\"Repair: {likely_cause}\", car_details)        plan += f\"I've prepared a calendar invite for the repair:\\n\\n{invite}\\n\\n\"    else:        maintenance_plan = plan_maintenance(mileage, car_make, car_model, car_year)        plan = f\"Here's your maintenance plan:\\n\\n{maintenance_plan}\\n\\n\"                # Create a calendar invite for the next maintenance task        next_task = maintenance_plan.split(\"Task:\")[1].split(\"\\n\")[0].strip()        invite = create_calendar_invite(f\"Maintenance: {next_task}\", car_details)        plan += f\"I've prepared a calendar invite for your next maintenance task:\\n\\n{invite}\\n\\n\"        plan += \"Remember to consult with a professional mechanic for personalized advice and service.\"        return planAdditionally, we’ll implement some helper functions that, while not tools themselves, will be used internally within the tools to support the logic and enhance their functionality.def get_car_model_info(mileage: int, car_make: str, car_model: str, car_year: int) -&gt; dict:    \"\"\"Retrieve car model information from cars_models.json.\"\"\"    with open('cars_models/cars_models.json', 'r') as file:        car_models = json.load(file)    for car in car_models:                if (car['car_make'].lower() == car_make.lower() and car['car_model'].lower() == car_model.lower() and car['car_year'] == car_year):            return car    return {}def retrieve_car_details(make: str, model: str, year: int) -&gt; str:    \"\"\"Retrieves the make, model, and year of the car and return the common issues if any.\"\"\"    car_details = get_car_model_info(0, make, model, year)  # Using 0 for mileage to get general details    if car_details:        return f\"{year} {make} {model} - Common Issues: {', '.join(car_details['common_issues'])}\"    return f\"{year} {make} {model} - No common issues found.\"Here are the additional tools in their complete formcomprehensive_diagnostic_tool = FunctionTool.from_defaults(fn=comprehensive_diagnosis)maintenance_planner_tool = FunctionTool.from_defaults(fn=plan_maintenance)calendar_invite_tool = FunctionTool.from_defaults(fn=create_calendar_invite)car_care_coordinator_tool = FunctionTool.from_defaults(fn=coordinate_car_care)retrieve_car_details_tool = FunctionTool.from_defaults(fn=retrieve_car_details)Now, let’s combine all these tools into a comprehensive tools list, which we will pass to our agent to utilize.tools = [    retrieve_problems_tool,    retrieve_parts_tool,    diagnostic_tool,    cost_estimator_tool,    maintenance_schedule_tool,    comprehensive_diagnostic_tool,    maintenance_planner_tool,    calendar_invite_tool,    car_care_coordinator_tool,    retrieve_car_details_tool]Step 4 : Creating the AgentNow that we’ve defined the tools, we’re ready to create the agent. With LlamaIndex, this involves setting up an Agent reasoning loop. Basically, this loop allows our agent to handle complex questions that might require multiple steps or clarifications. Essentially, our agent can reason through tools and complete tasks across several stages.LlamaIndex provides two main components for creating an agent: AgentRunner and AgentWorkers.The AgentRunner acts as the orchestrator, like in a symphony, managing the overall process. It handles the current state, conversational memory, and tasks, and it runs steps for each task while providing a high-level user interface on what’s going on. On the other hand, AgentWorkers are responsible for the operational side. They select and use the tools and choose the LLM to interact with these tools effectively.Now, let’s set up both the AgentRunner and AgentWorker to bring our agent to life.# Function to reset the agent's memorydef reset_agent_memory():    global agent_worker, agent    agent_worker = FunctionCallingAgentWorker.from_tools(        tools,         llm=llm,         verbose=True    )    agent = AgentRunner(agent_worker)# Initialize the agentreset_agent_memory()Every time you call reset_agent_memory(), a new, fresh agent is created, ready to reason through and act on the user’s query.With everything now set up—our tools, an agent for reasoning, and databases for retrieving relevant context—let’s test to see if our agent can handle simple questions effectively.Step 5 : D-DayLet’s ask the agent a straightforward question related to car maintenance based on the mileage count and see how well it handles it.response = agent.query(    \"My car has 60,000 miles on it. What maintenance should I be doing now, and how much will it cost?\")and the response I got isAdded user message to memory: My car has 60,000 miles on it. What maintenance should I be doing now, and how much will it cost?=== Calling Function ===Calling function: get_maintenance_schedule with args: {\"mileage\": 60000}=== Calling Function ===Calling function: estimate_repair_cost with args: {\"problem\": \"Oil and filter change\"}=== Calling Function ===Calling function: estimate_repair_cost with args: {\"problem\": \"Tire rotation\"}=== Calling Function ===Calling function: estimate_repair_cost with args: {\"problem\": \"Air filter replacement\"}=== Calling Function ===Calling function: estimate_repair_cost with args: {\"problem\": \"Brake inspection\"}=== LLM Response ===At 60,000 miles, the recommended maintenance tasks for your car are:1. Oil and filter change: This typically costs around $250.2. Tire rotation: The average cost for this service is around $50.3. Air filter replacement: This usually costs about $70.4. Brake inspection: The cost for this can vary, but it's typically around $100.Please note that these are average costs and can vary based on your location and the specific make and model of your car. It's always a good idea to get a few quotes from different service providers.Well, this is amazing! The agent effectively understood the query and provided an excellent response. Notice how it first called the maintenance_schedule_tool, which utilized the get_maintenance_schedule retriever object to gather context on the relevant maintenance schedule, including different tasks based on the car’s mileage. This context was then used by the cost_estimator_tool.The best part is that it passed the relevant parameters—problems extracted from the maintenance_schedule_tool—to the cost estimator tool, deciding on its own based on the user query. Finally, with all the gathered context, it produced a comprehensive response that perfectly addresses the user’s needs.Btw, If you want the agent to retain the context of previous conversations, replace .query with .chat to ensure context is preserved. Keep in mind that the context size is limited by the information you provide when calling the retrievers. Watch out for the max_context_information parameter in the retrievers to avoid exceeding the token limits for the LLMs.And that’s it! You’ve successfully created an agentic RAG that not only understands the user’s query but also delivers a well-reasoned and contextually accurate answer. Here is the colab for this example: ",
            "content_html": "<p>Agentic RAG (Retrieval-Augmented Generation) represents a significant leap in how we handle information. Traditional <a href=\"https://vipul-maheshwari.github.io/2024/02/14/rag-application-with-langchain\">RAG</a> systems are built to retrieve information and relevant context which are then passively send that to a language model (LLM) to generate responses. However, Agentic RAG takes this further by adding more independence. Now, the system can not only gather data but also make decisions and take actions on its own.</p><p>Think of it as a shift from simple tools to smarter, more capable systems. Agentic RAG transforms what was once a passive process into an active one, where AI can work towards specific goals without needing constant guidance.</p><h3 id=\"how-does-an-agentic-rag-work\">How Does an Agentic RAG Work?</h3><p>To understand Agentic RAG, let’s first break down what an “agent” is. Simply put, an agent is a smart system capable of making decisions on its own. When given a question or task, it figures out the best way to handle it by breaking the task into smaller steps and using the right tools to complete it.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multi-document-agentic-rag/whole-process.png?raw=true\" alt=\"Multi Document Agentic RAG\" /></p><p>Now, when we talk about Agentic RAG, we’re taking this concept further. Instead of just retrieving information like regular RAG, Agentic RAG uses intelligent strategies to ensure the system provides the best possible response. It doesn’t stop at giving a basic, context-aware reply. Instead, these agents think through the query, select the right approach, and deliver a more thoughtful and refined answer. It’s like having a team of smart agents working together to solve a problem, one step at a time.</p><p>Here’s how the process works:</p><ol>  <li><strong>Understanding the Query</strong>: The agent starts by analyzing the question to grasp its specifics. It looks at the context, purpose, and intent to determine what information is needed.</li>  <li><strong>Using Memory</strong>: The agent checks its memory for relevant information from past tasks that might assist with the current query.</li>  <li><strong>Choosing Tools</strong>: Agentic RAG agents are designed to be intuitive. After analyzing the query, they evaluate available tools and resources to select the best method for retrieving the precise information. It’s like having a savvy assistant who knows exactly where to look, even for the most challenging questions.</li></ol><h3 id=\"how-to-use-it\">How to Use it?</h3><p>Let’s dive into developing an automotive-themed RAG agent based on our current understanding.</p><p>Consider a scenario where you own a vehicle and require assistance with tasks ranging from diagnosing issues to planning routine maintenance. Now imagine a specialized agent designed specifically for this purpose. This agent should be capable of interpreting your car’s symptoms, analyzing the issue, and delivering a detailed diagnosis, including potential causes.</p><p>Furthermore, the bot should assist with identifying specific parts, estimating repair costs, or creating a personalized maintenance schedule based on your vehicle’s mileage and model. This agent must effectively manage a range of tasks by utilizing relevant context from various datasources, employing different tools, and reasoning with the available information to provide accurate and meaningful responses. Given the complexity of these tasks, which involves multiple retrieval and reasoning steps, I am going to use <a href=\"https://vipul-maheshwari.github.io/2024/03/15/embedded-databases\">LanceDB</a> to ensure fast retrieval by storing relevant embedded data chunks into it.</p><p>To meet our data needs, I will use six JSON files, each containing specific types of information for querying. You can get these JSON files here : <a href=\"https://github.com/lancedb/vectordb-recipes/tree/main/examples/multi-document-agentic-rag/json_files?ref=blog.lancedb.com\">Data</a></p><p>Here’s a brief overview of each file:</p><ol>  <li><strong>car_maintenance.json</strong>: Contains details about the car’s maintenance schedule, including relevant tasks based on mileage and estimated time for completion.</li>  <li><strong>car_problems.json</strong>: Provides information on various car problems, including a brief description of each issue, required parts for resolution, estimated repair time, and other relevant metadata.</li>  <li><strong>car_parts.json</strong>: Lists car parts used for maintenance and diagnosis, detailing their brands, categories, whether they are solid or liquid, and other relevant attributes.</li>  <li><strong>car_diagnosis.json</strong>: Outlines the diagnosis process, potential causes based on symptoms, recommended actions, and related problems. The issues and parts mentioned should align with those in <strong>car_problems.json</strong> and <strong>car_parts.json</strong> to ensure the agent has relevant context for problem-solving.</li>  <li><strong>car_cost_estimates.json</strong>: Provides cost estimates for addressing car problems, based on the issues listed in <strong>car_problems.json</strong>.</li>  <li><strong>car_models.json</strong>: Contains information on common problems associated with specific car models, such as a 2017 Honda Accord with 190,000 kilometers, detailing typical issues that users might encounter in this range.</li></ol><p>Please review the JSON files to see their structure and feel free to make any changes.</p><h3 id=\"how-to-tech-stack\">How to: Tech Stack</h3><p>To build our ReAct-like agent, we’ll be using a few key tools to make things run smoothly:</p><ol>  <li>    <p><strong>LlamaIndex</strong>: Think of <a href=\"https://www.llamaindex.ai/\">LlamaIndex</a> as the backbone of our agent. This framework will be central to our implementation. It facilitates the abstraction of agentic logic. If the specifics seem unclear now, they will become more evident as we proceed with the implementation.</p>  </li>  <li>    <p><strong>Memory Management</strong>: When we query the agent, it handles each question on its own without remembering past interactions or in isolation without maintaining state. To address this, we will use memory to retain conversational history. The agent stores chat history in a conversational memory buffer, which by default is a flat list managed by LlamaIndex. This ensures that the agent can refer to past as well as current conversation when deciding on the next set of actions.</p>  </li>  <li>    <p><strong>Vector Databases</strong>: For the retrieval process, we will use VectorDBs. Queries will be embedded and matched semantically against the relevant VectorDB through our retrievers. We will employ <a href=\"https://github.com/lancedb/lancedb\">LanceDB</a> due to its exceptional retrieval speed and on-disk storage capabilities, which allows for local management of our database. Additionally, being open-source and free, it fits within our budget constraints.</p>  </li>  <li>    <p><strong>LLM Integration</strong>: On the language model side, we’ll go with OpenAI’s GPT-4 for generating responses. For embeddings, we’re using Hugging face which provides seamless integration of local embedding models.</p>  </li></ol><p>Ok I think this is enough, let’s dive in for our code part..</p><h3 id=\"environment-setup\">Environment Setup</h3><pre><code class=\"language-python\">import osimport tqdmimport jsonimport timefrom typing import List, Optional, Dict, Anyfrom tqdm import tqdmimport loggingfrom llama_index.core import (    SimpleDirectoryReader,     VectorStoreIndex,     StorageContext,    Settings,    Document,)from datetime import datetime, timedeltafrom llama_index.core.node_parser import SentenceSplitterfrom llama_index.vector_stores.lancedb import LanceDBVectorStorefrom llama_index.embeddings.huggingface import HuggingFaceEmbeddingfrom llama_index.llms.openai import OpenAIfrom llama_index.core.tools import FunctionTool, ToolOutputfrom llama_index.core.retrievers import VectorIndexRetrieverfrom llama_index.core.agent import FunctionCallingAgentWorkerfrom llama_index.core.agent import AgentRunnerimport osos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"# Load environment variablesOPENAI_API_KEY = os.getenv('OPENAI_API_KEY')if not OPENAI_API_KEY:    raise ValueError(\"Please set the OPENAI_API_KEY environment variable.\")# LLM setupllm = OpenAI(model=\"gpt-4\", api_key=OPENAI_API_KEY)# Embedding model setupembed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")# Update the Settings with the new embedding modelSettings.embed_model = embed_modelSettings.chunk_size = 512</code></pre><p>Make sure your <code>.env</code> file includes the <code>OPENAI_API_KEY</code>. You can adjust and use different LLMs and embedding models as needed. The key is to have an LLM for reasoning and an embedding model to handle data embedding. Feel free to experiment with various models to find the best fit for your needs.</p><h3 id=\"step-1--creating-our-dbs\">Step 1 : Creating our DBs</h3><p>Let’s setup our database which will be used to store our data.</p><pre><code class=\"language-python\"># Vector store setupproblems_vector_store = LanceDBVectorStore(    uri='./lancedb',    table_name='problems_table',    mode=\"overwrite\",)parts_vector_store = LanceDBVectorStore(    uri='./lancedb',    table_name='parts_table',    mode=\"overwrite\",)diagnostics_vector_store = LanceDBVectorStore(    uri='./lancedb',    table_name='diagnostics_table',    mode=\"overwrite\",)cost_estimates_vector_store = LanceDBVectorStore(    uri='./lancedb',    table_name='cost_estimates_table',    mode=\"overwrite\",)maintenance_schedules_vector_store = LanceDBVectorStore(    uri='./lancedb',    table_name='maintenance_schedules_table',    mode=\"overwrite\",)cars_vector_store = LanceDBVectorStore(    uri='./lancedb',    table_name='car_maintenance_table',    mode=\"overwrite\",)</code></pre><p>Since we’re dealing with various types of information, we’ll need multiple tables to organize and retrieve our data effectively. The <code>uri</code> specifies that our data is stored in a database called <code>lancedb</code>, which contains different tables for each type of data. Let’s go ahead and load the data into the appropriate tables.</p><pre><code class=\"language-python\">def load_and_index_documents(directory: str, vector_store: LanceDBVectorStore) -&gt; VectorStoreIndex:    \"\"\"Load documents from a directory and index them.\"\"\"    documents = SimpleDirectoryReader(input_dir=directory).load_data()    parser = SentenceSplitter(chunk_size=1024, chunk_overlap=200)    nodes = parser.get_nodes_from_documents(documents)    storage_context = StorageContext.from_defaults(vector_store=vector_store)    return VectorStoreIndex(nodes, storage_context=storage_context)def create_retriever(index: VectorStoreIndex) -&gt; VectorIndexRetriever:    \"\"\"Create a retriever from the index.\"\"\"    return index.as_retriever(similarity_top_k=5)# Load and index documentsproblems_index = load_and_index_documents(\"../rag-agentic-system/problems\", problems_vector_store)parts_index = load_and_index_documents(\"../rag-agentic-system/parts\", parts_vector_store)cars_index = load_and_index_documents(\"../rag-agentic-system/cars_models\", cars_vector_store)diagnostics_index = load_and_index_documents(\"../rag-agentic-system/diagnostics\", diagnostics_vector_store)cost_estimates_index = load_and_index_documents(\"../rag-agentic-system/cost_estimates\", cost_estimates_vector_store)maintenance_schedules_index = load_and_index_documents(\"../rag-agentic-system/maintenance_schedules\", maintenance_schedules_vector_store)problems_retriever = create_retriever(problems_index)parts_retriever = create_retriever(parts_index)cars_retriever = create_retriever(cars_index)diagnostics_retriever = create_retriever(diagnostics_index)cost_estimates_retriever = create_retriever(cost_estimates_index)maintenance_retriever = create_retriever(maintenance_schedules_index)</code></pre><p>Each Vector DB will provide a retriever instance, a Python object that returns a list of documents matching a given query. For example, our problems_retriever will fetch documents related to car problems based on the query, while cars_retriever will help identify common issues faced by customers with their vehicles.</p><p>Keep in mind that if the bot misses some information or seems to hallucinate, it might be due to missing data in our JSON files. If you spot any inaccuracies or gaps, add the relevant data to the JSON files and re-index them to ensure everything stays up to date.</p><p>Now, let’s test our retrievers to ensure they’re working correctly.</p><h3 id=\"step-2--testing-our-retrievers\">Step 2 : Testing our retrievers</h3><p>Let’s test our <code>cost_estimates_retriever</code> to ensure it’s working properly. We’ll use a query related to a brake problem and check if the retriever returns the correct documents about cost estimates. Additionally, I’ll verify if our query engine is accurately interpreting the query and if, by providing the relevant documents, we are receiving the correct response.</p><pre><code class=\"language-python\">query = \"My brake pad isn't working or I don't know, but the brakes are poor, and by the way, what's the cost for the solution?\"query_engine = cost_estimates_index.as_query_engine()response = query_engine.query(query)results = cost_estimates_retriever.retrieve(query)print(f\"Response: {response}\")for result in results:    print(f\"Result - Node ID: {result.node_id}\")    print(f\"Relevant Text: {result.text[:100]}...\")      print(f\"Score: {result.score:.3f}\")</code></pre><p>This is the output I’m receiving in the cell</p><pre><code class=\"language-text\">Response: The cost for the solution to address the issue with your brake pad would be around $100 to $300 for brake pad replacement.Result - Node ID: 51b91e6e-1243-405b-a742-89e09d78616fRelevant Text: [    {        \"repair\": \"Brake pad replacement\",        \"average_cost\": 150,        \"cost_range\": {            \"min\": 100,            \"max\": 300...Score: 0.584</code></pre><p>Everything looks good—our retriever is working as expected. We’re ready to move on to the next step.</p><h3 id=\"step-3--creating-our-agentic-tools\">Step 3 : Creating our agentic tools</h3><p>LlamaIndex Agents are designed to process natural language input to execute actions rather than simply generating responses. The effectiveness of these agents relies on how well we abstract and utilize tools. So, what exactly does “tool” mean in this context? To clarify, imagine tools as weapons given to a warrior in battle. Just as a warrior might choose different weapons based on the opponent’s tactics, tools for our agent are like specialized API interfaces that help the agent interact with data sources or reason through queries to deliver the best possible responses.</p><p>In LlamaIndex, there are various types of tools. One important type is the <code>FunctionTool</code>, which transforms any user-defined function into a tool, capable of inferring the function’s schema and usage. These tools are essential for our agents, allowing them to reason about queries and perform actions effectively.</p><p>For each tool, it’s essential to provide a clear description of its purpose and functionality, as this helps the agent use the tool effectively. To start, we will create tools to leverage the retriever objects defined earlier.</p><pre><code class=\"language-python\">max_context_information = 200def retrieve_problems(query: str) -&gt; str:    \"\"\"Searches the problem catalog to find relevant automotive problems for the query.\"\"\"    docs = problems_retriever.retrieve(query)    information = str([doc.text[:max_context_information]for doc in docs])    return information    def retrieve_parts(query: str) -&gt; str:    \"\"\"Searches the parts catalog to find relevant parts for the query.\"\"\"    docs = parts_retriever.retrieve(query)    information = str([doc.text[:max_context_information]for doc in docs])    return informationdef retrieve_car_details(make: str, model: str, year: int) -&gt; str:    \"\"\"Retrieves the make, model, and year of the car.\"\"\"    docs = car_details_retriever.retrieve(make, model, year)    information = str([doc.text[:max_context_information]for doc in docs])def diagnose_car_problem(symptoms: str) -&gt; str:    \"\"\"Uses the diagnostics database to find potential causes for given symptoms.\"\"\"    docs = diagnostics_retriever.retrieve(symptoms)    information = str([doc.text[:max_context_information]for doc in docs])    return informationdef estimate_repair_cost(problem: str) -&gt; str:    \"\"\"Provides a cost estimate for a given car problem or repair.\"\"\"    docs = cost_estimates_retriever.retrieve(problem)    information = str([doc.text[:max_context_information]for doc in docs])    return informationdef get_maintenance_schedule(mileage: int) -&gt; str:    \"\"\"Retrieves the recommended maintenance schedule based on mileage.\"\"\"    docs = maintenance_retriever.retrieve(str(mileage))    information = str([doc.text[:max_context_information]for doc in docs])    return informationretrieve_problems_tool = FunctionTool.from_defaults(fn=retrieve_problems)retrieve_parts_tool = FunctionTool.from_defaults(fn=retrieve_parts)diagnostic_tool = FunctionTool.from_defaults(fn=diagnose_car_problem)cost_estimator_tool = FunctionTool.from_defaults(fn=estimate_repair_cost)maintenance_retriever_tool = FunctionTool.from_defaults(fn=get_maintenance_schedule)</code></pre><p>With the retriever tools now set up, our agent can effectively select the appropriate tool based on the query and fetch the relevant contexts. Next, we’ll create additional helper tools that will complement the existing ones, providing the agent with more context and enhancing its reasoning capabilities.</p><pre><code class=\"language-python\">def comprehensive_diagnosis(symptoms: str) -&gt; str:    \"\"\"    Provides a comprehensive diagnosis including possible causes, estimated costs, and required parts.        Args:        symptoms: A string describing the car's symptoms.        Returns:        A string with a comprehensive diagnosis report.    \"\"\"    # Use existing tools    possible_causes = diagnose_car_problem(symptoms)        # Extract the most likely cause (this is a simplification)    likely_cause = possible_causes[0] if possible_causes else \"Unknown issue\"        estimated_cost = estimate_repair_cost(likely_cause)    required_parts = retrieve_parts(likely_cause)        report = f\"Comprehensive Diagnosis Report:\\n\\n\"    report += f\"Symptoms: {symptoms}\\n\\n\"    report += f\"Possible Causes:\\n{possible_causes}\\n\\n\"    report += f\"Most Likely Cause: {likely_cause}\\n\\n\"    report += f\"Estimated Cost:\\n{estimated_cost}\\n\\n\"    report += f\"Required Parts:\\n{required_parts}\\n\\n\"    report += \"Please note that this is an initial diagnosis. For accurate results, please consult with our professional mechanic.\"        return reportdef plan_maintenance(mileage: int, car_make: str, car_model: str, car_year: int) -&gt; str:    \"\"\"    Creates a comprehensive maintenance plan based on the car's mileage and details.        Args:        mileage: The current mileage of the car.        car_make: The make of the car.        car_model: The model of the car.        car_year: The year the car was manufactured.        Returns:        A string with a comprehensive maintenance plan.    \"\"\"    car_details = retrieve_car_details(car_make, car_model, car_year)    car_model_info = get_car_model_info(mileage, car_make, car_model, car_year)        plan = f\"Maintenance Plan for {car_year} {car_make} {car_model} at {mileage} miles:\\n\\n\"    plan += f\"Car Details: {car_details}\\n\\n\"        if car_model_info:        plan += f\"Common Issues:\\n\"        for issue in car_model_info['common_issues']:            plan += f\"- {issue}\\n\"                plan += f\"\\nEstimated Time: {car_model_info['estimated_time']}\\n\\n\"    else:        plan += \"No specific maintenance tasks found for this car model and mileage.\\n\\n\"        plan += \"Please consult with our certified mechanic for a more personalized maintenance plan.\"        return plandef create_calendar_invite(event_type: str, car_details: str, duration: int = 60) -&gt; str:    \"\"\"    Simulates creating a calendar invite for a car maintenance or repair event.        Args:        event_type: The type of event (e.g., \"Oil Change\", \"Brake Inspection\").        car_details: Details of the car (make, model, year).        duration: Duration of the event in minutes (default is 60).        Returns:        A string describing the calendar invite.    \"\"\"    # Simulate scheduling the event for next week    event_date = datetime.now() + timedelta(days=7)    event_time = event_date.replace(hour=10, minute=0, second=0, microsecond=0)        invite = f\"Calendar Invite Created:\\n\\n\"    invite += f\"Event: {event_type} for {car_details}\\n\"    invite += f\"Date: {event_time.strftime('%Y-%m-%d')}\\n\"    invite += f\"Time: {event_time.strftime('%I:%M %p')}\\n\"    invite += f\"Duration: {duration} minutes\\n\"    invite += f\"Location: Your Trusted Auto Shop, 123 Main St, Bengaluru, India\\n\\n\"        return invitedef coordinate_car_care(query: str, car_make: str, car_model: str, car_year: int, mileage: int) -&gt; str:    \"\"\"    Coordinates overall car care by integrating diagnosis, maintenance planning, and scheduling.        Args:        query: The user's query or description of the issue.        car_make: The make of the car.        car_model: The model of the car.        car_year: The year the car was manufactured.        mileage: The current mileage of the car.        Returns:        A string with a comprehensive car care plan.    \"\"\"    car_details = retrieve_car_details(car_make, car_model, car_year)        # Check if it's a problem or routine maintenance    if \"problem\" in query.lower() or \"issue\" in query.lower():        diagnosis = comprehensive_diagnosis(query)        plan = f\"Based on your query, here's a diagnosis:\\n\\n{diagnosis}\\n\\n\"                # Extract the most likely cause (this is a simplification)        likely_cause = diagnosis.split(\"Most Likely Cause:\")[1].split(\"\\n\")[0].strip()                # Create a calendar invite for repair        invite = create_calendar_invite(f\"Repair: {likely_cause}\", car_details)        plan += f\"I've prepared a calendar invite for the repair:\\n\\n{invite}\\n\\n\"    else:        maintenance_plan = plan_maintenance(mileage, car_make, car_model, car_year)        plan = f\"Here's your maintenance plan:\\n\\n{maintenance_plan}\\n\\n\"                # Create a calendar invite for the next maintenance task        next_task = maintenance_plan.split(\"Task:\")[1].split(\"\\n\")[0].strip()        invite = create_calendar_invite(f\"Maintenance: {next_task}\", car_details)        plan += f\"I've prepared a calendar invite for your next maintenance task:\\n\\n{invite}\\n\\n\"        plan += \"Remember to consult with a professional mechanic for personalized advice and service.\"        return plan</code></pre><p>Additionally, we’ll implement some helper functions that, while not tools themselves, will be used internally within the tools to support the logic and enhance their functionality.</p><pre><code class=\"language-python\">def get_car_model_info(mileage: int, car_make: str, car_model: str, car_year: int) -&gt; dict:    \"\"\"Retrieve car model information from cars_models.json.\"\"\"    with open('cars_models/cars_models.json', 'r') as file:        car_models = json.load(file)    for car in car_models:                if (car['car_make'].lower() == car_make.lower() and car['car_model'].lower() == car_model.lower() and car['car_year'] == car_year):            return car    return {}def retrieve_car_details(make: str, model: str, year: int) -&gt; str:    \"\"\"Retrieves the make, model, and year of the car and return the common issues if any.\"\"\"    car_details = get_car_model_info(0, make, model, year)  # Using 0 for mileage to get general details    if car_details:        return f\"{year} {make} {model} - Common Issues: {', '.join(car_details['common_issues'])}\"    return f\"{year} {make} {model} - No common issues found.\"</code></pre><p>Here are the additional tools in their complete form</p><pre><code class=\"language-python\">comprehensive_diagnostic_tool = FunctionTool.from_defaults(fn=comprehensive_diagnosis)maintenance_planner_tool = FunctionTool.from_defaults(fn=plan_maintenance)calendar_invite_tool = FunctionTool.from_defaults(fn=create_calendar_invite)car_care_coordinator_tool = FunctionTool.from_defaults(fn=coordinate_car_care)retrieve_car_details_tool = FunctionTool.from_defaults(fn=retrieve_car_details)</code></pre><p>Now, let’s combine all these tools into a comprehensive tools list, which we will pass to our agent to utilize.</p><pre><code class=\"language-python\">tools = [    retrieve_problems_tool,    retrieve_parts_tool,    diagnostic_tool,    cost_estimator_tool,    maintenance_schedule_tool,    comprehensive_diagnostic_tool,    maintenance_planner_tool,    calendar_invite_tool,    car_care_coordinator_tool,    retrieve_car_details_tool]</code></pre><h3 id=\"step-4--creating-the-agent\">Step 4 : Creating the Agent</h3><p>Now that we’ve defined the tools, we’re ready to create the agent. With LlamaIndex, this involves setting up an Agent reasoning loop. Basically, this loop allows our agent to handle complex questions that might require multiple steps or clarifications. Essentially, our agent can reason through tools and complete tasks across several stages.</p><p>LlamaIndex provides two main components for creating an agent: <code>AgentRunner</code> and <code>AgentWorkers</code>.</p><p>The <code>AgentRunner</code> acts as the orchestrator, like in a symphony, managing the overall process. It handles the current state, conversational memory, and tasks, and it runs steps for each task while providing a high-level user interface on what’s going on. On the other hand, <code>AgentWorkers</code> are responsible for the operational side. They select and use the tools and choose the LLM to interact with these tools effectively.</p><p>Now, let’s set up both the AgentRunner and AgentWorker to bring our agent to life.</p><pre><code class=\"language-python\"># Function to reset the agent's memorydef reset_agent_memory():    global agent_worker, agent    agent_worker = FunctionCallingAgentWorker.from_tools(        tools,         llm=llm,         verbose=True    )    agent = AgentRunner(agent_worker)# Initialize the agentreset_agent_memory()</code></pre><p>Every time you call <code>reset_agent_memory()</code>, a new, fresh agent is created, ready to reason through and act on the user’s query.</p><p>With everything now set up—our tools, an agent for reasoning, and databases for retrieving relevant context—let’s test to see if our agent can handle simple questions effectively.</p><h3 id=\"step-5--d-day\">Step 5 : D-Day</h3><p>Let’s ask the agent a straightforward question related to car maintenance based on the mileage count and see how well it handles it.</p><pre><code class=\"language-python\">response = agent.query(    \"My car has 60,000 miles on it. What maintenance should I be doing now, and how much will it cost?\")</code></pre><p>and the response I got is</p><pre><code class=\"language-python\">Added user message to memory: My car has 60,000 miles on it. What maintenance should I be doing now, and how much will it cost?=== Calling Function ===Calling function: get_maintenance_schedule with args: {\"mileage\": 60000}=== Calling Function ===Calling function: estimate_repair_cost with args: {\"problem\": \"Oil and filter change\"}=== Calling Function ===Calling function: estimate_repair_cost with args: {\"problem\": \"Tire rotation\"}=== Calling Function ===Calling function: estimate_repair_cost with args: {\"problem\": \"Air filter replacement\"}=== Calling Function ===Calling function: estimate_repair_cost with args: {\"problem\": \"Brake inspection\"}=== LLM Response ===At 60,000 miles, the recommended maintenance tasks for your car are:1. Oil and filter change: This typically costs around $250.2. Tire rotation: The average cost for this service is around $50.3. Air filter replacement: This usually costs about $70.4. Brake inspection: The cost for this can vary, but it's typically around $100.Please note that these are average costs and can vary based on your location and the specific make and model of your car. It's always a good idea to get a few quotes from different service providers.</code></pre><p>Well, this is amazing! The agent effectively understood the query and provided an excellent response. Notice how it first called the <code>maintenance_schedule_tool</code>, which utilized the <code>get_maintenance_schedule</code> retriever object to gather context on the relevant maintenance schedule, including different tasks based on the car’s mileage. This context was then used by the <code>cost_estimator_tool</code>.</p><p>The best part is that it passed the relevant parameters—problems extracted from the <code>maintenance_schedule_tool</code>—to the cost estimator tool, deciding on its own based on the user query. Finally, with all the gathered context, it produced a comprehensive response that perfectly addresses the user’s needs.</p><p>Btw, If you want the agent to retain the context of previous conversations, replace <code>.query</code> with <code>.chat</code> to ensure context is preserved. Keep in mind that the context size is limited by the information you provide when calling the retrievers. Watch out for the <code>max_context_information</code> parameter in the retrievers to avoid exceeding the token limits for the LLMs.</p><p>And that’s it! You’ve successfully created an agentic RAG that not only understands the user’s query but also delivers a well-reasoned and contextually accurate answer. Here is the colab for this example: <img src=\"https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/multi-document-agentic-rag/main.ipynb?ref=blog.lancedb.com#scrollTo=Q1Z8S3epC5So\" alt=\"colab\" /></p>",
            "url": "http://localhost:4000/2024/08/16/multi-document-agentic-rag",
            
            
            
            "tags": ["LLM","RAG","LanceDB"],
            
            "date_published": "2024-08-16T00:00:00+05:30",
            "date_modified": "2024-08-16T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/07/14/zero-shot-image-classification",
            "title": "Zero Shot Image Classification with LanceDB and OpenAI's CLIP",
            "summary": "This post shows how you can do the image classification without training a image model",
            "content_text": "Imagine an AI having a conversation in a language it was never explicitly taught or suddenly playing a new game without any practice. In essence, if an AI can handle a task it hasn’t been directly trained for, that’s what we call zero-shot capability.Zero-Shot classificationThere are many state-of-the-art (SOTA) computer vision models that excel at various classification tasks, such as identifying animals, cars, fraud, and products in e-commerce. They can handle almost any image classification job. However, these models are often specialized and need fine-tuning for different use cases to be truly effective.Fine-tuning can be challenging; it requires a well-labeled dataset, and if your use case is specific to an enterprise, it may also need significant computing power.So, what does “Zero-Shot image classification” really means? Imagine a deep learning model trained only to distinguish between cats and dogs. Now, if you show it a picture of a person lounging on the couch playing video games, and the model identifies it as a “corporate employee enjoying a Sunday afternoon,” that’s zero-shot image classification. It means the model can correctly identify something it was never specifically trained to recognize. To help you follow along, here is the complete architecture..FundamentalsTo make this work, we need a multimodal embedding model and a vector database. Let’s start with something called CLIP, which stands for Contrastive Language-Image Pre-Training. Think of CLIP as a smart box that can understand different types of files. Whether you give it an image or text, it can grasp the context behind them all.But how it’s working behind the scenes? Consider there are two smaller boxes in that box: a Text Encoder and an Image Encoder. When OpenAI trained CLIP, they made sure these two encoders understand text and images in the same vector space.They achieved this by training the model to place similar image-text pairs close together in vector space while separating the vectors of non-pairs. Although OpenAI hasn’t specified the exact data used, the CLIP paper mentions that the model was trained on 400 million image-text pairs collected from the internet. This extensive training gives the model an impressive ability to understand relevant image-text pairs.So, here’s what we get from using CLIP:  Instead of datasets with specific class labels, CLIP only needs image-text pairs, where the text describes the image.  Instead of training a CNN to get features from an image, CLIP uses more expressive text descriptions, which can provide additional features.The authors of CLIP demonstrated its superior zero-shot classification performance by comparing it to the ResNet-101 model trained specifically on ImageNet. When both models were tested on other datasets derived from ImageNet, CLIP outperformed the state-of-the-art ResNet-101, showing a better understanding of the dataset than the fine-tuned version of ResNet-101 trained on ImageNet data.Reasoning of CLIPSo, the implementation is quite straightforward. But before going into that, Let’s just quickly understand how a CNN works.Initially, each image in a traditional classification model has assigned class labels. We input these images into the model along with their respective class labels as the expected outputs. Through training, the model’s weights are adjusted based on calculated losses. Over time, the model learns to distinguish between various images by recognizing distinct features.However, zero-shot classification takes this concept further by utilizing two key components: a Text Encoder and an Image Encoder. Yes those two small boxes that I described earlier, Now these encoders produce n-dimensional vectors for both images and text, mapping them to the same vector space. This means the n-dimensional vector of an image of a “cat” would be semantically similar to the vector of a text description like “a photo of a cat”.By leveraging this shared vector space, zero-shot classification enables the model to classify images into categories it hasn’t explicitly seen during training. Instead of relying solely on predefined class labels, the model can compare the vector representation of a new image to vector representations of textual descriptions of various categories.To enhance the effectiveness of our zero-shot classification, we should transform our class labels from simple words like “cat,” “dog,” and “horse” into more descriptive phrases such as “a photo of a cat,” “a photo of a dog,” or “a photo of a horse.” This transformation is crucial because it mirrors the text-image pairs used during the model’s pretraining phase. OpenAI used prompts like \"a photo of a {label}\" paired with each label to create these image-text pairs.[1]By adopting a similar approach, our classification task aligns more closely with the model’s pretrained understanding of how images relate to their textual descriptions.Final thoughtsLet’s take a step back and solidify our understanding before implementation. The CLIP model is pre-trained on a massive dataset of image-text pairs, learning that “a photo of a cat” corresponds to an actual image of a cat, and vice versa. This means whenever we feed an image or text into CLIP, we can expect it to grasp the relevance between the two.Now, if you want to get into the nitty-gritty of the algorithm, it’s not overly complex. At its core, CLIP encodes each image and text as a n-dimensional embedding vector. Let’s say T1 is the vector for “a photo of a cat”, T2 for “a photo of a bird”, and T3 for “a photo of a horse”. If we have an image of a cat with embedding V1, the similarity score between V1 and T1 should be the highest among all text embeddings. This high similarity tells us that the V1 vector indeed represents “a photo of a cat”.So, when we pass an image of a cat to our CLIP model, it should reason like “this is a cat, I know this already”. Or if we input an image of bananas on a table, it might get the nerve and put up something like “I think this image shows bananas placed on a table”. Pretty cool, right?We’ve achieved our goal of classifying images without explicitly training a model on specific categories. And this is how CLIP does the heavy lifting for us, leveraging its pre-training to generalize to a wide range of concepts and enable zero-shot classification.Using LanceDBTo bring our zero-shot classification system to life, we need a robust Vector Database to store our label embeddings. The process is straightforward: we’ll transform our simple text labels like “cat” into more descriptive phrases such as “a photo of a cat”, fetch their CLIP embeddings, and store these in our database. When it comes time to classify a new image, we’ll retrieve its embedding from CLIP and perform a cosine similarity calculation against all our stored label embeddings in our DB. The label with the closest match becomes our predicted class.For this crucial task, I’ve opted for LanceDB, an impressive open-source vector database that’s like a super-smart data lake for managing complex information. LanceDB shines when we are handling complex data like our vector embeddings with an exceptional performance in fetching and storage, and the best part? It won’t cost you a dime.But LanceDB’s appeal goes beyond just being free and open-source. Its unparalleled scalability, efficient on-disk storage, and serverless capabilities make it a standout choice. These features are part of a broader trend of columnar databases that are rapidly transforming ML workflows. I’ve actually written an in-depth article exploring the game-changing capabilities of these kind of databases. If you’re curious about how they’re revolutionizing the field, I highly recommend giving it a read!ImplementationWith all the tools at our disposal, let’s move on to a practical example of using CLIP for zero-shot image classification with the LanceDB vector database. For this demonstration, I’ll use the  uoft-cs/cifar100 dataset from Hugging Face Datasets.from datasets import load_datasetimagedata = load_dataset(    'uoft-cs/cifar100',    split=\"test\")imagedataLet’s see original label names# labels names labels = imagedata.info.features['fine_label'].namesprint(len(labels))labels100['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach',... 'whale', 'willow_tree', 'wolf', 'woman', 'worm']Looks good! We have 100 classes to classify images from, which would require a lot of computing power if you go for traditional CNN. However, let’s proceed with our zero-shot image classification approach.Let’s generate the relevant textual descriptions for our labels# generate sentencesclip_labels = [f\"a photo of a {label}\" for label in labels]clip_labels['a photo of a apple', 'a photo of a aquarium_fish', 'a photo of a baby', 'a photo of a bear', 'a photo of a beaver', 'a photo of a bed', 'a photo of a bee', 'a photo of a beetle', 'a photo of a bicycle', 'a photo of a bottle', 'a photo of a bowl', 'a photo of a boy', 'a photo of a bridge', 'a photo of a bus', 'a photo of a butterfly', 'a photo of a camel', 'a photo of a can', 'a photo of a castle', 'a photo of a caterpillar', 'a photo of a cattle', 'a photo of a chair', 'a photo of a chimpanzee', 'a photo of a clock', 'a photo of a cloud', 'a photo of a cockroach',... 'a photo of a whale', 'a photo of a willow_tree', 'a photo of a wolf', 'a photo of a woman', 'a photo of a worm']Now let’s initialize our CLIP embedding model, I will use the CLIP implementation from hugginface.# initializationfrom transformers import CLIPProcessor, CLIPModelmodel_id = \"openai/clip-vit-large-patch14\"processor = CLIPProcessor.from_pretrained(model_id)model = CLIPModel.from_pretrained(model_id)import torch# if you have CUDA set it to the active device like thisdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"# move the model to the devicemodel.to(device)If you’re new to Transformers, remember that computers understand numbers, not text. We’ll convert our text descriptions into integer representations called input IDs, where each number stands for a word or subword, more formally tokens.  We’ll also need an attention mask to help the transformer focus on relevant parts of the input.For more details, you can read about transformers here.# create label tokenslabel_tokens = processor(    text=clip_labels,    padding=True,    return_tensors='pt').to(device)# Print the label tokens with the corresponding text for i in range(5):    token_ids = label_tokens['input_ids'][i]    print(f\"Token ID : {token_ids}, Text : {processor.decode(token_ids, skip_special_tokens=False)}\")Token ID : tensor([49406,   320,  1125,   539,   320,  3055, 49407, 49407, 49407]), Text : &lt;|startoftext|&gt;a photo of a apple &lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;Token ID : tensor([49406,   320,  1125,   539,   320, 16814,   318,  2759, 49407]), Text : &lt;|startoftext|&gt;a photo of a aquarium _ fish &lt;|endoftext|&gt;Token ID : tensor([49406,   320,  1125,   539,   320,  1794, 49407, 49407, 49407]), Text : &lt;|startoftext|&gt;a photo of a baby &lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;Token ID : tensor([49406,   320,  1125,   539,   320,  4298, 49407, 49407, 49407]), Text : &lt;|startoftext|&gt;a photo of a bear &lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;Token ID : tensor([49406,   320,  1125,   539,   320, 22874, 49407, 49407, 49407]), Text : &lt;|startoftext|&gt;a photo of a beaver &lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;Now let’s get our CLIP embeddings for our text labels# encode tokens to sentence embeddings from CLIPwith torch.no_grad():    label_emb = model.get_text_features(**label_tokens) # passing the label text as in \"a photo of a cat\" to get it's relevant embedding from clip model# Move embeddings to CPU and convert to numpy arraylabel_emb = label_emb.detach().cpu().numpy()label_emb.shape(100, 768)We now have a 768-dimensional vector for each of our 100 text class sentences. However, to improve our results when calculating similarities, we need to normalize these embeddings.Normalization helps ensure that all vectors are on the same scale, preventing longer vectors from dominating the similarity calculations simply due to their magnitude. We achieve this by dividing each vector by the square root of the sum of the squares of its elements. This process, known as L2 normalization, adjusts the length of our vectors while preserving their directional information, making our similarity comparisons more accurate and reliable.import numpy as np# normalizationlabel_emb = label_emb / np.linalg.norm(label_emb, axis=0)label_emb.min(), label_emb.max()Ok, let’s see a random image from our datasetimport randomindex = random.randint(0, len(imagedata)-1)selected_image = imagedata[index]['img']selected_imageWhen you execute this code, you’ll be presented with a visual representation of a data point from our dataset. In my case, the output displayed a pixelated image of a whale.Before we can analyze our image with CLIP, we need to preprocess it properly. First, we’ll run the image through our CLIP processor. This step ensures the image is resized first, then the pixels are normalized, then converting it into the tensor and finally adding a batch dimension. All of these things are settled up for the model.image = processor(    text=None,    images=imagedata[index]['img'],    return_tensors='pt')['pixel_values'].to(device)image.shapetorch.Size([1, 3, 224, 224])Now here this shape represents a 4-dimensional tensor:  1: Batch size (1 image in this case)  3: Number of color channels (Red, Green, Blue)  224: Height of the image in pixels  224: Width of the image in pixelsSo, we have one image, with 3 color channels, and dimensions of 224x224 pixels. Now we’ll use CLIP to generate an embedding - a numerical representation of our image’s features. This embedding is what we’ll use for our classification task.img_emb = model.get_image_features(image)img_emb.shapetorch.Size([1, 768])This gives us 768 dimensional embedding to us, that’s our Image Embedding.  Only thing that is left for now is to use LanceDB to store our labels, with their corresponding embeddings and do the vector search for our Image Embedding on that database.. Here how it looks in the whole goimport lancedbimport numpy as npdata = []for label_name, embedding in zip(labels, label_emb):    data.append({\"label\": label_name, \"vector\": embedding})db = lancedb.connect(\"./.lancedb\")table = db.create_table(\"zero_shot_table\", data, mode=\"Overwrite\")# Prepare the query embeddingquery_embedding = img_emb.squeeze().detach().cpu().numpy()# Perform the searchresults = (table.search(query_embedding)           .limit(10)           .to_pandas())print(results.head(n=10))|   label         | vector | distance ||-----------------|-----------------------------------------------------------|-------------|| whale           | [0.05180167, 0.008572296, -0.00027403078, -0.12351207, ...]| 447.551605  || dolphin         | [0.09493398, 0.02598409, 0.0057568997, -0.13548125, ...]| 451.570709  || aquarium_fish   | [-0.094619915, 0.13643932, 0.030785343, 0.12217164, ...]| 451.694672  || skunk           | [0.1975818, -0.04034014, 0.023241673, 0.03933424, ...]| 452.987640  || crab            | [0.05123004, 0.0696855, 0.016390173, -0.02554354, ...]| 454.392456  || chimpanzee      | [0.04187969, 0.0196794, -0.038968336, 0.10017315, ...]| 454.870697  || ray             | [0.10485967, 0.023477506, 0.06709562, -0.08323726, ...]| 454.880524  || sea             | [-0.08117988, 0.059666794, 0.09419422, -0.18542227, ...]| 454.975311  || shark           | [-0.01027703, -0.06132377, 0.060097754, -0.2388756, ...]| 455.291901  || keyboard        | [-0.18453166, 0.05200073, 0.07468183, -0.08227961, ...]| 455.424866  |Here are the results everyone: all set and confirmed. Our initial accurate prediction is a whale, demonstrating the closest resemblance between the label and the image with minimal distance, just as we had hoped. What’s truly remarkable is that we achieved this without running a single epoch for a CNN model. That’s zero shot classification for you fellas. Here is your colab for your reference. See you in next one.",
            "content_html": "<p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/zero-shot-image-classification-with-lancedb/cat.png?raw=true\" alt=\"cat-woah\" /></p><p>Imagine an AI having a conversation in a language it was never explicitly taught or suddenly playing a new game without any practice. In essence, if an AI can handle a task it hasn’t been directly trained for, that’s what we call zero-shot capability.</p><h3 id=\"zero-shot-classification\">Zero-Shot classification</h3><p>There are many state-of-the-art (SOTA) computer vision models that excel at various classification tasks, such as identifying animals, cars, fraud, and products in e-commerce. They can handle almost any image classification job. However, these models are often specialized and need fine-tuning for different use cases to be truly effective.</p><p>Fine-tuning can be challenging; it requires a well-labeled dataset, and if your use case is specific to an enterprise, it may also need significant computing power.</p><p>So, what does “Zero-Shot image classification” really means? Imagine a deep learning model trained only to distinguish between cats and dogs. Now, if you show it a picture of a person lounging on the couch playing video games, and the model identifies it as a “corporate employee enjoying a Sunday afternoon,” that’s zero-shot image classification. It means the model can correctly identify something it was never specifically trained to recognize. To help you follow along, here is the complete architecture..</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/zero-shot-image-classification-with-lancedb/zero-shot-image-classification.png?raw=true\" alt=\"architecture\" /></p><h3 id=\"fundamentals\">Fundamentals</h3><p>To make this work, we need a multimodal embedding model and a vector database. Let’s start with something called CLIP, which stands for <strong>Contrastive Language-Image Pre-Training.</strong> Think of CLIP as a smart box that can understand different types of files. Whether you give it an image or text, it can grasp the context behind them all.</p><p>But how it’s working behind the scenes? Consider there are two smaller boxes in that box: a <strong>Text Encoder</strong> and an <strong>Image Encoder</strong>. When OpenAI trained CLIP, they made sure these two encoders understand text and images in the same vector space.</p><p>They achieved this by training the model to place similar image-text pairs close together in vector space while separating the vectors of non-pairs. Although OpenAI hasn’t specified the exact data used, the CLIP paper mentions that the model was trained on 400 million image-text pairs collected from the internet. This extensive training gives the model an impressive ability to understand relevant image-text pairs.</p><p>So, here’s what we get from using CLIP:</p><ol>  <li>Instead of datasets with specific class labels, CLIP only needs <strong>image-text pairs</strong>, where the text describes the image.</li>  <li>Instead of training a CNN to get features from an image, CLIP uses more expressive text descriptions, which can provide additional features.</li></ol><p>The authors of CLIP demonstrated its superior zero-shot classification performance by comparing it to the ResNet-101 model trained specifically on ImageNet. When both models were tested on other datasets derived from ImageNet, CLIP outperformed the state-of-the-art ResNet-101, showing a better understanding of the dataset than the fine-tuned version of ResNet-101 trained on ImageNet data.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/zero-shot-image-classification-with-lancedb/comparison-matrix.png?raw=true\" alt=\"image-classification-matrix\" /></p><h3 id=\"reasoning-of-clip\">Reasoning of CLIP</h3><p>So, the implementation is quite straightforward. But before going into that, Let’s just quickly understand how a CNN works.</p><p>Initially, each image in a traditional classification model has assigned class labels. We input these images into the model along with their respective class labels as the expected outputs. Through training, the model’s weights are adjusted based on calculated losses. Over time, the model learns to distinguish between various images by recognizing distinct features.</p><p>However, zero-shot classification takes this concept further by utilizing two key components: a Text Encoder and an Image Encoder. Yes those two small boxes that I described earlier, Now these encoders produce n-dimensional vectors for both images and text, mapping them to the same vector space. This means the n-dimensional vector of an image of a “cat” would be semantically similar to the vector of a text description like “a photo of a cat”.</p><p>By leveraging this shared vector space, zero-shot classification enables the model to classify images into categories it hasn’t explicitly seen during training. Instead of relying solely on predefined class labels, the model can compare the vector representation of a new image to vector representations of textual descriptions of various categories.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/zero-shot-image-classification-with-lancedb/same-vector-space.png?raw=true\" alt=\"same-vector-space.png\" /></p><p>To enhance the effectiveness of our zero-shot classification, we should transform our class labels from simple words like “cat,” “dog,” and “horse” into more descriptive phrases such as “a photo of a cat,” “a photo of a dog,” or “a photo of a horse.” This transformation is crucial because it mirrors the text-image pairs used during the model’s pretraining phase. OpenAI used prompts like <code>\"a photo of a {label}\"</code> paired with each label to create these image-text pairs.<a href=\"https://openai.com/index/clip/\">[1]</a></p><p>By adopting a similar approach, our classification task aligns more closely with the model’s pretrained understanding of how images relate to their textual descriptions.</p><h3 id=\"final-thoughts\">Final thoughts</h3><p>Let’s take a step back and solidify our understanding before implementation. The CLIP model is pre-trained on a massive dataset of image-text pairs, learning that “a photo of a cat” corresponds to an actual image of a cat, and vice versa. This means whenever we feed an image or text into CLIP, we can expect it to grasp the relevance between the two.</p><p>Now, if you want to get into the nitty-gritty of the algorithm, it’s not overly complex. At its core, CLIP encodes each image and text as a n-dimensional embedding vector. Let’s say T1 is the vector for “a photo of a cat”, T2 for “a photo of a bird”, and T3 for “a photo of a horse”. If we have an image of a cat with embedding V1, the similarity score between V1 and T1 should be the highest among all text embeddings. This high similarity tells us that the V1 vector indeed represents “a photo of a cat”.</p><p>So, when we pass an image of a cat to our CLIP model, it should reason like “this is a cat, I know this already”. Or if we input an image of bananas on a table, it might get the nerve and put up something like “I think this image shows bananas placed on a table”. Pretty cool, right?</p><p>We’ve achieved our goal of classifying images without explicitly training a model on specific categories. And this is how CLIP does the heavy lifting for us, leveraging its pre-training to generalize to a wide range of concepts and enable zero-shot classification.</p><h3 id=\"using-lancedb\">Using LanceDB</h3><p>To bring our zero-shot classification system to life, we need a robust Vector Database to store our label embeddings. The process is straightforward: we’ll transform our simple text labels like “cat” into more descriptive phrases such as “a photo of a cat”, fetch their CLIP embeddings, and store these in our database. When it comes time to classify a new image, we’ll retrieve its embedding from CLIP and perform a cosine similarity calculation against all our stored label embeddings in our DB. The label with the closest match becomes our predicted class.</p><p>For this crucial task, I’ve opted for LanceDB, an impressive open-source vector database that’s like a super-smart data lake for managing complex information. LanceDB shines when we are handling complex data like our vector embeddings with an exceptional performance in fetching and storage, and the best part? It won’t cost you a dime.</p><p>But LanceDB’s appeal goes beyond just being free and open-source. Its unparalleled scalability, efficient on-disk storage, and serverless capabilities make it a standout choice. These features are part of a broader trend of columnar databases that are rapidly transforming ML workflows. I’ve actually written an in-depth <a href=\"https://vipul-maheshwari.github.io/2024/03/15/embedded-databases\">article</a> exploring the game-changing capabilities of these kind of databases. If you’re curious about how they’re revolutionizing the field, I highly recommend giving it a read!</p><h3 id=\"implementation\">Implementation</h3><p>With all the tools at our disposal, let’s move on to a practical example of using CLIP for zero-shot image classification with the LanceDB vector database. For this demonstration, I’ll use the  <code>uoft-cs/cifar100</code> dataset from Hugging Face Datasets.</p><pre><code class=\"language-python\">from datasets import load_datasetimagedata = load_dataset(    'uoft-cs/cifar100',    split=\"test\")imagedata</code></pre><p>Let’s see original label names</p><pre><code class=\"language-python\"># labels names labels = imagedata.info.features['fine_label'].namesprint(len(labels))labels</code></pre><pre><code class=\"language-python\">100['apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach',... 'whale', 'willow_tree', 'wolf', 'woman', 'worm']</code></pre><p>Looks good! We have 100 classes to classify images from, which would require a lot of computing power if you go for traditional CNN. However, let’s proceed with our zero-shot image classification approach.</p><p>Let’s generate the relevant textual descriptions for our labels</p><pre><code class=\"language-python\"># generate sentencesclip_labels = [f\"a photo of a {label}\" for label in labels]clip_labels</code></pre><pre><code class=\"language-python\">['a photo of a apple', 'a photo of a aquarium_fish', 'a photo of a baby', 'a photo of a bear', 'a photo of a beaver', 'a photo of a bed', 'a photo of a bee', 'a photo of a beetle', 'a photo of a bicycle', 'a photo of a bottle', 'a photo of a bowl', 'a photo of a boy', 'a photo of a bridge', 'a photo of a bus', 'a photo of a butterfly', 'a photo of a camel', 'a photo of a can', 'a photo of a castle', 'a photo of a caterpillar', 'a photo of a cattle', 'a photo of a chair', 'a photo of a chimpanzee', 'a photo of a clock', 'a photo of a cloud', 'a photo of a cockroach',... 'a photo of a whale', 'a photo of a willow_tree', 'a photo of a wolf', 'a photo of a woman', 'a photo of a worm']</code></pre><p>Now let’s initialize our CLIP embedding model, I will use the CLIP implementation from hugginface.</p><pre><code class=\"language-python\"># initializationfrom transformers import CLIPProcessor, CLIPModelmodel_id = \"openai/clip-vit-large-patch14\"processor = CLIPProcessor.from_pretrained(model_id)model = CLIPModel.from_pretrained(model_id)</code></pre><pre><code class=\"language-python\">import torch# if you have CUDA set it to the active device like thisdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"# move the model to the devicemodel.to(device)</code></pre><p>If you’re new to Transformers, remember that computers understand numbers, not text. We’ll convert our text descriptions into integer representations called input IDs, where each number stands for a word or subword, more formally <code>tokens</code>.  We’ll also need an attention mask to help the transformer focus on relevant parts of the input.</p><p>For more details, you can read about transformers <a href=\"https://huggingface.co/docs/transformers/en/index\">here</a>.</p><pre><code class=\"language-python\"># create label tokenslabel_tokens = processor(    text=clip_labels,    padding=True,    return_tensors='pt').to(device)# Print the label tokens with the corresponding text for i in range(5):    token_ids = label_tokens['input_ids'][i]    print(f\"Token ID : {token_ids}, Text : {processor.decode(token_ids, skip_special_tokens=False)}\")</code></pre><pre><code class=\"language-python\">Token ID : tensor([49406,   320,  1125,   539,   320,  3055, 49407, 49407, 49407]), Text : &lt;|startoftext|&gt;a photo of a apple &lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;Token ID : tensor([49406,   320,  1125,   539,   320, 16814,   318,  2759, 49407]), Text : &lt;|startoftext|&gt;a photo of a aquarium _ fish &lt;|endoftext|&gt;Token ID : tensor([49406,   320,  1125,   539,   320,  1794, 49407, 49407, 49407]), Text : &lt;|startoftext|&gt;a photo of a baby &lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;Token ID : tensor([49406,   320,  1125,   539,   320,  4298, 49407, 49407, 49407]), Text : &lt;|startoftext|&gt;a photo of a bear &lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;Token ID : tensor([49406,   320,  1125,   539,   320, 22874, 49407, 49407, 49407]), Text : &lt;|startoftext|&gt;a photo of a beaver &lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;</code></pre><p>Now let’s get our CLIP embeddings for our text labels</p><pre><code class=\"language-python\"># encode tokens to sentence embeddings from CLIPwith torch.no_grad():    label_emb = model.get_text_features(**label_tokens) # passing the label text as in \"a photo of a cat\" to get it's relevant embedding from clip model# Move embeddings to CPU and convert to numpy arraylabel_emb = label_emb.detach().cpu().numpy()label_emb.shape</code></pre><pre><code class=\"language-python\">(100, 768)</code></pre><p>We now have a 768-dimensional vector for each of our 100 text class sentences. However, to improve our results when calculating similarities, we need to normalize these embeddings.</p><p>Normalization helps ensure that all vectors are on the same scale, preventing longer vectors from dominating the similarity calculations simply due to their magnitude. We achieve this by dividing each vector by the square root of the sum of the squares of its elements. This process, known as L2 normalization, adjusts the length of our vectors while preserving their directional information, making our similarity comparisons more accurate and reliable.</p><pre><code class=\"language-python\">import numpy as np# normalizationlabel_emb = label_emb / np.linalg.norm(label_emb, axis=0)label_emb.min(), label_emb.max()</code></pre><p>Ok, let’s see a random image from our dataset</p><pre><code class=\"language-python\">import randomindex = random.randint(0, len(imagedata)-1)selected_image = imagedata[index]['img']selected_image</code></pre><p>When you execute this code, you’ll be presented with a visual representation of a data point from our dataset. In my case, the output displayed a pixelated image of a whale.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/zero-shot-image-classification-with-lancedb/whale.png?raw=true\" alt=\"whale\" /></p><p>Before we can analyze our image with CLIP, we need to preprocess it properly. First, we’ll run the image through our CLIP processor. This step ensures the image is resized first, then the pixels are normalized, then converting it into the tensor and finally adding a batch dimension. All of these things are settled up for the model.</p><pre><code class=\"language-python\">image = processor(    text=None,    images=imagedata[index]['img'],    return_tensors='pt')['pixel_values'].to(device)image.shape</code></pre><pre><code class=\"language-python\">torch.Size([1, 3, 224, 224])</code></pre><p>Now here this shape represents a 4-dimensional tensor:</p><ul>  <li><strong>1:</strong> Batch size (1 image in this case)</li>  <li><strong>3:</strong> Number of color channels (Red, Green, Blue)</li>  <li><strong>224:</strong> Height of the image in pixels</li>  <li><strong>224:</strong> Width of the image in pixels</li></ul><p>So, we have one image, with 3 color channels, and dimensions of 224x224 pixels. Now we’ll use CLIP to generate an embedding - a numerical representation of our image’s features. This embedding is what we’ll use for our classification task.</p><pre><code class=\"language-python\">img_emb = model.get_image_features(image)img_emb.shape</code></pre><pre><code class=\"language-python\">torch.Size([1, 768])</code></pre><p>This gives us 768 dimensional embedding to us, that’s our Image Embedding.  Only thing that is left for now is to use LanceDB to store our labels, with their corresponding embeddings and do the vector search for our Image Embedding on that database.. Here how it looks in the whole go</p><pre><code class=\"language-python\">import lancedbimport numpy as npdata = []for label_name, embedding in zip(labels, label_emb):    data.append({\"label\": label_name, \"vector\": embedding})db = lancedb.connect(\"./.lancedb\")table = db.create_table(\"zero_shot_table\", data, mode=\"Overwrite\")# Prepare the query embeddingquery_embedding = img_emb.squeeze().detach().cpu().numpy()# Perform the searchresults = (table.search(query_embedding)           .limit(10)           .to_pandas())print(results.head(n=10))</code></pre><pre><code class=\"language-python\">|   label         | vector | distance ||-----------------|-----------------------------------------------------------|-------------|| whale           | [0.05180167, 0.008572296, -0.00027403078, -0.12351207, ...]| 447.551605  || dolphin         | [0.09493398, 0.02598409, 0.0057568997, -0.13548125, ...]| 451.570709  || aquarium_fish   | [-0.094619915, 0.13643932, 0.030785343, 0.12217164, ...]| 451.694672  || skunk           | [0.1975818, -0.04034014, 0.023241673, 0.03933424, ...]| 452.987640  || crab            | [0.05123004, 0.0696855, 0.016390173, -0.02554354, ...]| 454.392456  || chimpanzee      | [0.04187969, 0.0196794, -0.038968336, 0.10017315, ...]| 454.870697  || ray             | [0.10485967, 0.023477506, 0.06709562, -0.08323726, ...]| 454.880524  || sea             | [-0.08117988, 0.059666794, 0.09419422, -0.18542227, ...]| 454.975311  || shark           | [-0.01027703, -0.06132377, 0.060097754, -0.2388756, ...]| 455.291901  || keyboard        | [-0.18453166, 0.05200073, 0.07468183, -0.08227961, ...]| 455.424866  |</code></pre><p>Here are the results everyone: all set and confirmed. Our initial accurate prediction is a whale, demonstrating the closest resemblance between the label and the image with minimal distance, just as we had hoped. What’s truly remarkable is that we achieved this without running a single epoch for a CNN model. That’s zero shot classification for you fellas. Here is your <a href=\"https://colab.research.google.com/github/lancedb/vectordb-recipes/blob/main/examples/zero-shot-image-classification/main.ipynb?ref=blog.lancedb.com\">colab</a> for your reference. See you in next one.</p>",
            "url": "http://localhost:4000/2024/07/14/zero-shot-image-classification",
            
            
            
            "tags": ["LLM","Deep Learning","LanceDB"],
            
            "date_published": "2024-07-14T00:00:00+05:30",
            "date_modified": "2024-07-14T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/06/26/train-a-cnn-with-lancedataset",
            "title": "Train a CNN classification model with Lance Dataset",
            "summary": "This post shows how you can train a CNN model with Lance Dataset",
            "content_text": "In this previous post, I showed you how you can convert any Image Dataset to Lance format for faster retrieval and faster I/O operations. But can we use the same Lance formatted image dataset to train an image classification model? Well here it comes…Lance Format: Saga for efficient image datasetsCNNs are widely used for the image related tasks in AI world. They’re great at figuring out what’s in a picture, spotting objects, and even breaking down images into meaningful parts. What makes them so useful is how they can learn important visual clues on their own, without needing humans intervention.But when we’re dealing with massive image collections, just handling all that data can be a real headache. That’s where Lance file format comes in - it provides a clever new way to package up image data perfectly in the deep learning ecosystem for all our needs. The Lance format offers several key advantages that make it a powerful choice for machine learning applications, some of them are:  Lance uses a compressed columnar format, offering efficient storage, fast data loading, and quick random access, making it ideal for large-scale image datasets.  It supports diverse data types, including images, and text facilitating the processing of different modalities in machine learning pipelines.  Lance stores data on disk, ensuring persistence through system failures and enhancing privacy and security by allowing local storage and access.  It provides high-performance random access, up to 100 times faster than Parquet.  Lance enables vector search, finding nearest neighbors in under 1 millisecond, and integrates OLAP queries with vector search.  It features zero-copy, automatic versioning, which manages data versions automatically and reduces redundancy.Integrating Lance and Convolutional Neural NetworksIf you’ve been working with Convolutional Neural Networks (CNNs) for image classification, you know that data loading and preprocessing can be a real headache. But what if I told you there’s a way to make this process smoother and faster? Enter the Lance format.In my previous post, I walked through the process of converting popular image datasets like cinic-10 and mini-imagenet to the Lance format. If you haven’t read that yet, I highly recommend you do so before continuing here. It’ll give you the foundation you need to fully appreciate what we’re about to dive into.Now, let’s take the next step: using Lance-formatted data to train a CNN for image classification. We’ll use the cinic-10 dataset as our example, but the principles apply to other datasets as well.Before we jump in, it’s important to understand how the Lance + PyTorch approach differs from the standard PyTorch method. Traditionally, PyTorch users rely on Torchvision’s ImageFolder to handle image and label loading. The Lance approach, however, requires us to create a custom dataset class. This class is designed to load binary-format images and their corresponding labels directly from the Lance dataset for creating the dataloaders.You might be wondering, “Is it worth the effort to switch?” The answer is a resounding yes, especially if you’re dealing with large datasets or need faster training times. Lance’s secret weapon is its lightning-fast random access capability. This means that Lance dataloaders can feed data to your CNN much quicker than standard PyTorch dataloaders, potentially shaving hours off your training time.In the following sections, we’ll dive into the details of implementing the Lance + PyTorch approach. By the end, you’ll have a powerful new tool in your deep learning toolkit that can significantly streamline your image classification workflows and reduce your model training time.Load the Lance files to create the dataloadersLance-formatted image data is stored in binary format, which isn’t directly usable by Convolutional Neural Networks (CNNs). We need to convert this data into a format CNNs can process, such as PIL Image objects. Here’s the process we’ll follow:  Retrieve the binary image data: Extract the relevant image data from the Lance files.  Convert to PIL Image: Transform the binary data into a PIL Image object, creating a readable image format.  Handle grayscale images: Convert any grayscale images to RGB format for compatibility with CNNs that expect 3-channel color images.  Apply transformations: Use the provided transform function to apply necessary transformations like resizing or normalization.  Determine the labels: Look up the class index for each image’s label in the provided list of classes.  Return the data: Provide the transformed image and its corresponding label for CNN training.To streamline this process, we’ll create a custom dataset class. This class will handle all these steps efficiently, preparing the Lance-formatted data for use with a CNN.This custom dataset class manages all the necessary steps to prepare our Lance-formatted data for use with a CNN model. It essentially iterates over the dataset to retrieve the relevant images and labels. By using this class, we can easily integrate the Lance data into your PyTorch-based training pipeline.# Define the image classesclasses = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')# transformation function transform_train = transforms.Compose([    transforms.Resize((32, 32)),    transforms.RandomHorizontalFlip(),    transforms.ToTensor(),    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])transform_test = transforms.Compose([    transforms.ToTensor(),    transforms.Resize((32, 32)),    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])transform_val = transforms.Compose([    transforms.ToTensor(),    transforms.Resize((32, 32)),    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])# Define the custom dataset classclass CustomImageDataset(data.Dataset):    def __init__(self, classes, lance_dataset, transform=None):        self.classes = classes        self.ds = lance.dataset(lance_dataset)        self.transform = transform    def __len__(self):        return self.ds.count_rows()    def __getitem__(self, idx):        raw_data = self.ds.take([idx], columns=['image', 'label']).to_pydict()        img_data, label = raw_data['image'][0], raw_data['label'][0]        img = Image.open(io.BytesIO(img_data))        # Convert grayscale images to RGB        if img.mode != 'RGB':            img = img.convert('RGB')        if self.transform:            img = self.transform(img)        label = self.classes.index(label)        return img, labelNow that we have our custom Dataset class set up, we’re ready to proceed with training our model using the Lance dataset.Using Lance dataset with CNNs: Putting It All TogetherNow that we’ve created our custom dataset class, integrating Lance dataset into our CNN training process becomes straightforward. Here’s how it all comes together:  Import the custom dataset class into our CNN script.  Load the lance dataset and create lance dataloaders.  Use the lance dataloaders instead of the standard dataloaders to train our model.From this point, the process follows a standard CNN training workflow. For our example, I’ve chosen to use ResNet-34 as our CNN architecture to enhance accuracy.class Net(nn.Module):    def __init__(self, num_classes):        super(Net, self).__init__()        self.resnet = models.resnet34(pretrained=True)        num_ftrs = self.resnet.fc.in_features        self.resnet.fc = nn.Linear(num_ftrs, num_classes)    def forward(self, x):        return self.resnet(x)With this setup, we can train a CNN on our Lance dataset using just a single script.One key advantage of using Lance-backed training is its performance. Compared to traditional methods, Lance-formatted data offers significant improvements in training speed. Here is the result when I compared the training time for 3 epochs with Lance vs Vanilla dataloadersThis shows an extensive improvement in training time for the Lance dataloaders as compared to the Vanilla ones.Here is the complete notebook for the reference. For those wanting to explore further, there’s a repository showcasing various deep learning techniques that utilize Lance-formatted data. This resource can be valuable for expanding your understanding and application of Lance file format in different machine learning contexts.",
            "content_html": "<p>In this <a href=\"https://vipul-maheshwari.github.io/2024/04/09/convert-any-image-dataset-to-lance\">previous</a> post, I showed you how you can convert any Image Dataset to Lance format for faster retrieval and faster I/O operations. But can we use the same Lance formatted image dataset to train an image classification model? Well here it comes…</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/train-a-cnn-with-lance-dataset/training_a_cnn.png?raw=true\" alt=\"front-image\" /></p><h3 id=\"lance-format-saga-for-efficient-image-datasets\">Lance Format: Saga for efficient image datasets</h3><p>CNNs are widely used for the image related tasks in AI world. They’re great at figuring out what’s in a picture, spotting objects, and even breaking down images into meaningful parts. What makes them so useful is how they can learn important visual clues on their own, without needing humans intervention.</p><p>But when we’re dealing with massive image collections, just handling all that data can be a real headache. That’s where Lance file format comes in - it provides a clever new way to package up image data perfectly in the deep learning ecosystem for all our needs. The Lance format offers several key advantages that make it a powerful choice for machine learning applications, some of them are:</p><ol>  <li>Lance uses a compressed columnar format, offering efficient storage, fast data loading, and quick random access, making it ideal for large-scale image datasets.</li>  <li>It supports diverse data types, including images, and text facilitating the processing of different modalities in machine learning pipelines.</li>  <li>Lance stores data on disk, ensuring persistence through system failures and enhancing privacy and security by allowing local storage and access.</li>  <li>It provides high-performance random access, up to 100 times faster than Parquet.</li>  <li>Lance enables vector search, finding nearest neighbors in under 1 millisecond, and integrates OLAP queries with vector search.</li>  <li>It features zero-copy, automatic versioning, which manages data versions automatically and reduces redundancy.</li></ol><h3 id=\"integrating-lance-and-convolutional-neural-networks\">Integrating Lance and Convolutional Neural Networks</h3><p>If you’ve been working with Convolutional Neural Networks (CNNs) for image classification, you know that data loading and preprocessing can be a real headache. But what if I told you there’s a way to make this process smoother and faster? Enter the Lance format.</p><p>In my previous post, I walked through the process of converting popular image datasets like <a href=\"https://www.kaggle.com/datasets/vipulmaheshwarii/cinic-10-lance-dataset?ref=blog.lancedb.com\">cinic-10</a> and <a href=\"https://www.kaggle.com/datasets/vipulmaheshwarii/mini-imagenet-lance-dataset\">mini-imagenet</a> to the Lance format. If you haven’t read that yet, I highly recommend you do so before continuing here. It’ll give you the foundation you need to fully appreciate what we’re about to dive into.</p><p>Now, let’s take the next step: using Lance-formatted data to train a CNN for image classification. We’ll use the cinic-10 dataset as our example, but the principles apply to other datasets as well.</p><p>Before we jump in, it’s important to understand how the Lance + PyTorch approach differs from the standard PyTorch method. Traditionally, PyTorch users rely on Torchvision’s ImageFolder to handle image and label loading. The Lance approach, however, requires us to create a custom dataset class. This class is designed to load binary-format images and their corresponding labels directly from the Lance dataset for creating the dataloaders.</p><p>You might be wondering, “Is it worth the effort to switch?” The answer is a resounding yes, especially if you’re dealing with large datasets or need faster training times. Lance’s secret weapon is its lightning-fast random access capability. This means that Lance dataloaders can feed data to your CNN much quicker than standard PyTorch dataloaders, potentially shaving hours off your training time.</p><p>In the following sections, we’ll dive into the details of implementing the Lance + PyTorch approach. By the end, you’ll have a powerful new tool in your deep learning toolkit that can significantly streamline your image classification workflows and reduce your model training time.</p><h3 id=\"load-the-lance-files-to-create-the-dataloaders\">Load the Lance files to create the dataloaders</h3><p>Lance-formatted image data is stored in binary format, which isn’t directly usable by Convolutional Neural Networks (CNNs). We need to convert this data into a format CNNs can process, such as PIL Image objects. Here’s the process we’ll follow:</p><ol>  <li>Retrieve the binary image data: Extract the relevant image data from the Lance files.</li>  <li>Convert to PIL Image: Transform the binary data into a PIL Image object, creating a readable image format.</li>  <li>Handle grayscale images: Convert any grayscale images to RGB format for compatibility with CNNs that expect 3-channel color images.</li>  <li>Apply transformations: Use the provided transform function to apply necessary transformations like resizing or normalization.</li>  <li>Determine the labels: Look up the class index for each image’s label in the provided list of classes.</li>  <li>Return the data: Provide the transformed image and its corresponding label for CNN training.</li></ol><p>To streamline this process, we’ll create a custom dataset class. This class will handle all these steps efficiently, preparing the Lance-formatted data for use with a CNN.</p><p>This custom dataset class manages all the necessary steps to prepare our Lance-formatted data for use with a CNN model. It essentially iterates over the dataset to retrieve the relevant images and labels. By using this class, we can easily integrate the Lance data into your PyTorch-based training pipeline.</p><pre><code class=\"language-python\"># Define the image classesclasses = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')# transformation function transform_train = transforms.Compose([    transforms.Resize((32, 32)),    transforms.RandomHorizontalFlip(),    transforms.ToTensor(),    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])transform_test = transforms.Compose([    transforms.ToTensor(),    transforms.Resize((32, 32)),    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])transform_val = transforms.Compose([    transforms.ToTensor(),    transforms.Resize((32, 32)),    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])</code></pre><pre><code class=\"language-python\"># Define the custom dataset classclass CustomImageDataset(data.Dataset):    def __init__(self, classes, lance_dataset, transform=None):        self.classes = classes        self.ds = lance.dataset(lance_dataset)        self.transform = transform    def __len__(self):        return self.ds.count_rows()    def __getitem__(self, idx):        raw_data = self.ds.take([idx], columns=['image', 'label']).to_pydict()        img_data, label = raw_data['image'][0], raw_data['label'][0]        img = Image.open(io.BytesIO(img_data))        # Convert grayscale images to RGB        if img.mode != 'RGB':            img = img.convert('RGB')        if self.transform:            img = self.transform(img)        label = self.classes.index(label)        return img, label</code></pre><p>Now that we have our custom Dataset class set up, we’re ready to proceed with training our model using the Lance dataset.</p><h3 id=\"using-lance-dataset-with-cnns-putting-it-all-together\">Using Lance dataset with CNNs: Putting It All Together</h3><p>Now that we’ve created our custom dataset class, integrating Lance dataset into our CNN training process becomes straightforward. Here’s how it all comes together:</p><ol>  <li>Import the custom dataset class into our CNN script.</li>  <li>Load the lance dataset and create lance dataloaders.</li>  <li>Use the lance dataloaders instead of the standard dataloaders to train our model.</li></ol><p>From this point, the process follows a standard CNN training workflow. For our example, I’ve chosen to use ResNet-34 as our CNN architecture to enhance accuracy.</p><pre><code class=\"language-python\">class Net(nn.Module):    def __init__(self, num_classes):        super(Net, self).__init__()        self.resnet = models.resnet34(pretrained=True)        num_ftrs = self.resnet.fc.in_features        self.resnet.fc = nn.Linear(num_ftrs, num_classes)    def forward(self, x):        return self.resnet(x)</code></pre><p>With this setup, we can train a CNN on our Lance dataset using just a single script.</p><p>One key advantage of using Lance-backed training is its performance. Compared to traditional methods, Lance-formatted data offers significant improvements in training speed. Here is the result when I compared the training time for 3 epochs with Lance vs Vanilla dataloaders</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/train-a-cnn-with-lance-dataset/epoch_duration.png?raw=true\" alt=\"epoch_duration\" /></p><p>This shows an extensive improvement in training time for the Lance dataloaders as compared to the Vanilla ones.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/train-a-cnn-with-lance-dataset/shocking_cat.png?raw=true\" alt=\"shocking_cat\" /></p><p>Here is the complete <a href=\"https://github.com/lancedb/lance-deeplearning-recipes/blob/main/community-examples/cnn-model-with-lance-dataset.ipynb\">notebook</a> for the reference. For those wanting to explore further, there’s a <a href=\"https://github.com/lancedb/lance-deeplearning-recipes\">repository</a> showcasing various deep learning techniques that utilize Lance-formatted data. This resource can be valuable for expanding your understanding and application of Lance file format in different machine learning contexts.</p>",
            "url": "http://localhost:4000/2024/06/26/train-a-cnn-with-lancedataset",
            
            
            
            "tags": ["LLM","Deep Learning","LanceDB"],
            
            "date_published": "2024-06-26T00:00:00+05:30",
            "date_modified": "2024-06-26T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/05/31/movie-recommendation-system-with-rag-and-genre-classification",
            "title": "Create a Movie Recommendation System using RAG and Genre Classification",
            "summary": "This post gives a detailed view on how you can use the embeddings to create a Movie Recommendation sys",
            "content_text": "This article provides a comprehensive guide on creating a movie recommendation system by using vector similarity search and multi-label genre classification.Here’s what we cover below:  Data ingestion and preprocessing techniques for movie metadata  Training a Doc2Vec model for Embeddings  Training a Neural network for genre classification task  Using Doc2Vec, LanceDB and the trained classifier get the relevant recommendationsLet’s get started!Why use Embeddings for Recommendation System?Scrolling through streaming platforms can be frustrating when the movie suggestions don’t match our interests. Building recommendation systems is a complex task, as there isn’t one metric that can measure the quality of recommendations. To improve this, we can combine embeddings and VectorDB for better recommendations.These embeddings serve dual purposes: they can either be directly used as input to a classification model for genre classification or stored in a VectorDB for retrieval purposes. By storing embeddings in a VectorDB, efficient retrieval and query search for recommendations become possible at a later stage.This architecture offers a holistic understanding of the underlying processes involved.Data Ingestion and preprocessing techniques for movie metadataOur initial task involves gathering and organizing information about movies. This includes gathering extensive details such as the movie’s type, plot summary, genres, audience ratings, and more.Fortunately, we have access to a robust dataset on Kaggle containing information from various sources for approximately 45,000 movies. To follow along, please download the data from Kaggle and place it inside your working directory.if you require additional data, you can supplement the dataset by extracting information from platforms like Rotten Tomatoes, IMDb, or even box-office records.Our next step is to extract the core details from this dataset and generate a universal summary for each movie. Initially, I’ll combine the movie’s title, genres, and overviews into a single textual string. Then, this text will be tagged to create TaggedDocument instances, which will be utilized to train the Doc2Vec model later on.Before moving forward, let’s install the relevant libraries to make our life easier.pip install torch scikit-learn pylance lancedb nltk gensim scipy==1.12Next, we’ll proceed with the ingestion and preprocessing of the data. To simplify the process, we’ll work with chunks of 1000 movies at a time. For clarity, we’ll only include movie indices with non-null values for genres, accurate titles, and complete overviews. This approach ensures that we’re working with high-quality, relevant data for our analysis.import torchimport pandas as pdimport numpy as npfrom gensim.models.doc2vec import Doc2Vec, TaggedDocumentfrom nltk.tokenize import word_tokenizefrom tqdm import tqdmimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import DataLoader, TensorDatasetfrom gensim.models.doc2vec import Doc2Vec, TaggedDocumentfrom nltk.tokenize import word_tokenizefrom sklearn.preprocessing import MultiLabelBinarizerfrom sklearn.model_selection import train_test_splitfrom tqdm import tqdmimport nltknltk.download('punkt')# Read data from CSV filemovie_data = pd.read_csv('movies_metadata.csv', low_memory=False)device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')def preprocess_data(movie_data_chunk):    tagged_docs = []    valid_indices = []    movie_info = []    # Wrap your loop with tqdm    for i, row in tqdm(movie_data_chunk.iterrows(), total=len(movie_data_chunk)):        try:            # Constructing movie text            movies_text = ''            genres = ', '.join([genre['name'] for genre in eval(row['genres'])])            movies_text += \"Overview: \" + row['overview'] + '\\n'            movies_text += \"Genres: \" + genres + '\\n'            movies_text += \"Title: \" + row['title'] + '\\n'            tagged_docs.append(TaggedDocument(words=word_tokenize(movies_text.lower()), tags=[str(i)]))            valid_indices.append(i)            movie_info.append((row['title'], genres))        except Exception as e:            continue    return tagged_docs, valid_indices, movie_infoGenerating embeddings using Doc2VecNext, we’ll utilize the Doc2Vec model to generate embeddings for each movie based on the preprocessed text. We’ll allow the Doc2Vec model to train for several epochs to capture the essence of the various movies and their metadata in the multidimensional latent space. This process will help us represent each movie in a way that captures its unique characteristics and context.def train_doc2vec_model(tagged_data, num_epochs=20):    # Initialize Doc2Vec model    doc2vec_model = Doc2Vec(vector_size=100, min_count=2, epochs=num_epochs)    doc2vec_model.build_vocab(tqdm(tagged_data, desc=\"Building Vocabulary\"))    for epoch in range(num_epochs):        doc2vec_model.train(tqdm(tagged_data, desc=f\"Epoch {epoch+1}\"), total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)        return doc2vec_model# Preprocess data and extract genres for the first 1000 movieschunk_size = 1000tagged_data = []valid_indices = []movie_info = []for chunk_start in range(0, len(movie_data), chunk_size):    movie_data_chunk = movie_data.iloc[chunk_start:chunk_start+chunk_size]    chunk_tagged_data, chunk_valid_indices, chunk_movie_info = preprocess_data(movie_data_chunk)    tagged_data.extend(chunk_tagged_data)    valid_indices.extend(chunk_valid_indices)    movie_info.extend(chunk_movie_info)doc2vec_model = train_doc2vec_model(tagged_data)The train_doc2vec_model function trains a Doc2Vec model on the tagged movie data, producing 100-dimensional embeddings for each movie. These embeddings act as input features for the neural network.With our current training setup, we are sure that movies with identical genres and similar kinds of overviews will be positioned closer to each other in the latent space, reflecting their thematic and content similarities.Extracting the unique genre labelsNext, our focus shifts to compiling the names of relevant movies along with their genres. Now wonTo illustrate this, Let’s consider a movie with three genres: ‘Drama’, ‘Comedy’, and ‘Horror’. Using the MultiLabelBinarizer, we’ll represent these genres with lists of 0s and 1s. If a movie belongs to a particular genre, it will be assigned a 1; if it doesn’t, it will receive a 0. Now each row in our dataset will indicate which genres are associated with a specific movie. This approach simplifies the genre representation for easier analysis.Let’s take the movie “Top Gun Maverick” as a reference. We’ll associate its genres using binary encoding. Suppose this movie is categorized only under ‘drama’, not ‘comedy’ or ‘horror’. When we apply the MultiLabelBinarizer, the representation would be: Drama: 1, Comedy: 0, Horror: 0. This signifies that “Top Gun Maverick” is classified as a drama but not as a comedy or horror. We’ll replicate this process for all the movies in our dataset to identify the unique genre labels present in our data.Training a Neural Network for genre classification taskWe’ll define a neural network consisting of four linear layers with ReLU activations. The final layer utilizes softmax activation to generate probability scores for various genres. If your objective is primarily classification within the genre spectrum, where you input a movie description to determine its relevant genres, you can establish a threshold value for the multi-label softmax output. This allows you to select the top ‘n’ genres with the highest probabilities.Here’s the neural network class, hyperparameter settings, and the corresponding training loop for training our model.# Extract genre labels for the valid indicesgenres_list = []for i in valid_indices:    row = movie_data.loc[i]    genres = [genre['name'] for genre in eval(row['genres'])]    genres_list.append(genres)mlb = MultiLabelBinarizer()genre_labels = mlb.fit_transform(genres_list)embeddings = []for i in valid_indices:    embeddings.append(doc2vec_model.dv[str(i)])X_train, X_test, y_train, y_test = train_test_split(embeddings, genre_labels, test_size=0.2, random_state=42)X_train_np = np.array(X_train, dtype=np.float32)y_train_np = np.array(y_train, dtype=np.float32)X_test_np = np.array(X_test, dtype=np.float32)y_test_np = np.array(y_test, dtype=np.float32)X_train_tensor = torch.tensor(X_train_np)y_train_tensor = torch.tensor(y_train_np)X_test_tensor = torch.tensor(X_test_np)y_test_tensor = torch.tensor(y_test_np)class GenreClassifier(nn.Module):    def __init__(self, input_size, output_size):        super(GenreClassifier, self).__init__()        self.fc1 = nn.Linear(input_size, 512)        self.bn1 = nn.BatchNorm1d(512)        self.fc2 = nn.Linear(512, 256)        self.bn2 = nn.BatchNorm1d(256)        self.fc3 = nn.Linear(256, 128)        self.bn3 = nn.BatchNorm1d(128)        self.fc4 = nn.Linear(128, output_size)        self.relu = nn.ReLU()        self.dropout = nn.Dropout(p=0.2)  # Adjust the dropout rate as needed    def forward(self, x):        x = self.fc1(x)        x = self.bn1(x)        x = self.relu(x)        x = self.dropout(x)        x = self.fc2(x)        x = self.bn2(x)        x = self.relu(x)        x = self.dropout(x)        x = self.fc3(x)        x = self.bn3(x)        x = self.relu(x)        x = self.dropout(x)        x = self.fc4(x)        return x# Move model to the selected devicemodel = GenreClassifier(input_size=100, output_size=len(mlb.classes_)).to(device)# Define loss function and optimizercriterion = nn.BCEWithLogitsLoss()optimizer = optim.Adam(model.parameters(), lr=0.001)# Training loopepochs = 50batch_size = 64train_dataset = TensorDataset(X_train_tensor.to(device), y_train_tensor.to(device))train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)for epoch in range(epochs):    model.train()    running_loss = 0.0    for inputs, labels in train_loader:        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device        optimizer.zero_grad()        outputs = model(inputs)        loss = criterion(outputs, labels)        loss.backward()        optimizer.step()        running_loss += loss.item() * inputs.size(0)    epoch_loss = running_loss / len(train_loader.dataset)    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}')That’s it! We’ve successfully trained a neural network for our genre classification task. Let’s test how our model is performing on the genre classification task.from sklearn.metrics import f1_scoremodel.eval()with torch.no_grad():    X_test_tensor, y_test_tensor = X_test_tensor.to(device), y_test_tensor.to(device)  # Move test data to device    outputs = model(X_test_tensor)    test_loss = criterion(outputs, y_test_tensor)    print(f'Test Loss: {test_loss.item():.4f}')thresholds = [0.1] * len(mlb.classes_)thresholds_tensor = torch.tensor(thresholds, device=device).unsqueeze(0)# Convert the outputs to binary predictions using varying thresholdspredicted_labels = (outputs &gt; thresholds_tensor).cpu().numpy()# Convert binary predictions and actual labels to multi-label formatpredicted_multilabels = mlb.inverse_transform(predicted_labels)actual_multilabels = mlb.inverse_transform(y_test_np)# Print the Predicted and Actual Labels for each moviefor i, (predicted, actual) in enumerate(zip(predicted_multilabels, actual_multilabels)):    print(f'Movie {i+1}:')    print(f'    Predicted Labels: {predicted}')    print(f'    Actual Labels: {actual}')# Compute F1-scoref1 = f1_score(y_test_np, predicted_labels, average='micro')print(f'F1-score: {f1:.4f}')# Saving the trained modeltorch.save(model.state_dict(), 'trained_model.pth')A movie recommendation systemTo build a movie recommendation system, we’ll allow users to input a movie name, and based on that, we’ll return relevant recommendations. We’ll save our Doc2Vec embeddings in a vector database to achieve this. When a user inputs a query in the form of a new movie, we’ll first locate its embeddings in our vector database. Once we have this, we’ll find the ‘n’ number of movies whose embeddings are similar to our query movie. We can assess the similarity using various search algorithms like cosine similarity or finding the least Euclidean distance.We’ll organize the data and the embeddings into a CSV file.import lancedbimport numpy as npimport pandas as pddata = []for i in valid_indices:    embedding = doc2vec_model.dv[str(i)]    title, genres = movie_info[valid_indices.index(i)]    data.append({\"title\": title, \"genres\": genres, \"vector\": embedding.tolist()})db = lancedb.connect(\".db\")tbl = db.create_table(\"doc2vec_embeddings\", data, mode=\"Overwrite\")db[\"doc2vec_embeddings\"].head()Essentially, we establish a connection to LanceDB, set up our table, and add our movie data to it.Each row in the table represents a single movie, with columns containing data like the title, genres, overview, and embeddings. For each movie title, we check if it exists in our dataset. If it does, we perform a cosine similarity search on all other movies and return the top 10 most relevant titles. This columnar format makes it easy to store and retrieve information for various tasks involving embeddings.Using Doc2Vec Embeddings to get the relevant recommendations.Our recommendation engine combines a neural network-based genre prediction model with a vector similarity search to provide relevant movie recommendations.For a given query movie, first, we use our trained neural network to predict its genres. Based on these predicted genres, we filter our movie database to include only those movies that share at least one genre with the query movie, achieved by constructing an appropriate SQL filter.We then perform a vector similarity search on this filtered subset to retrieve the most similar movies based on their vector representations. This approach ensures that the recommended movies are not only similar in terms of their vector characteristics but also share genre preferences with the query movie, resulting in more relevant and personalized recommendations.# Function to get genres for a single movie querydef get_genres_for_query(model, query_embedding, mlb, thresholds, device):    model.eval()    with torch.no_grad():        query_tensor = torch.tensor(query_embedding, dtype=torch.float32).unsqueeze(0).to(device)        outputs = model(query_tensor)        thresholds = [0.001] * len(mlb.classes_)        thresold_tensor = torch.tensor(thresholds, device=device).unsqueeze(0)        predicted_labels = (outputs &gt;= thresold_tensor).cpu().numpy()        predicted_multilabels = mlb.inverse_transform(predicted_labels)        return predicted_multilabelsdef movie_genre_prediction(movie_title):    movie_index = movie_data.index[movie_data['title'] == movie_title].tolist()[0]    query_embedding = doc2vec_model.dv[str(movie_index)]    predicted_genres = get_genres_for_query(model, query_embedding, mlb, [0.1] * len(mlb.classes_), device=device)    return predicted_genresAnd now, after all the groundwork, we’ve arrived at the final piece of the puzzle. Let’s generate some relevant recommendations using embeddings and LanceDB.def get_recommendations(title):    pd_data = pd.DataFrame(data)    title_vector = pd_data[pd_data[\"title\"] == title][\"vector\"].values[0]    predicted_genres = movie_genre_prediction(title)    genres_movie = predicted_genres[0]  # Assuming predicted_genres is available    genre_conditions = [f\"genres LIKE '%{genre}%'\" for genre in genres_movie]    where_clause = \" OR \".join(genre_conditions)    result = (        tbl.search(title_vector)        .metric(\"cosine\")        .limit(10)        .where(where_clause)        .to_pandas()    )    return result[[\"title\"]]get_recommendations(\"Toy Story\")Some of the recommended movies are remarkably close matches. For example, when looking at “Toy Story,” which falls under “animation” and “family” movies, our recommendation system can find other movies in the similar genres.That being said, here is the colab link for the complete reference.",
            "content_html": "<p>This article provides a comprehensive guide on creating a movie recommendation system by using vector similarity search and multi-label genre classification.</p><p>Here’s what we cover below:</p><ol>  <li>Data ingestion and preprocessing techniques for movie metadata</li>  <li>Training a Doc2Vec model for Embeddings</li>  <li>Training a Neural network for genre classification task</li>  <li>Using Doc2Vec, LanceDB and the trained classifier get the relevant recommendations</li></ol><p>Let’s get started!</p><h3 id=\"why-use-embeddings-for-recommendation-system\">Why use Embeddings for Recommendation System?</h3><p>Scrolling through streaming platforms can be frustrating when the movie suggestions don’t match our interests. Building recommendation systems is a complex task, as there isn’t one metric that can measure the quality of recommendations. To improve this, we can combine embeddings and VectorDB for better recommendations.</p><p>These embeddings serve dual purposes: they can either be directly used as input to a classification model for genre classification or stored in a VectorDB for retrieval purposes. By storing embeddings in a VectorDB, efficient retrieval and query search for recommendations become possible at a later stage.</p><p>This architecture offers a holistic understanding of the underlying processes involved.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/movie-recommendation-using-rag/architecture_recommendation.png?raw=true\" alt=\"image\" /></p><h3 id=\"data-ingestion-and-preprocessing-techniques-for-movie-metadata\">Data Ingestion and preprocessing techniques for movie metadata</h3><p>Our initial task involves gathering and organizing information about movies. This includes gathering extensive details such as the movie’s type, plot summary, genres, audience ratings, and more.</p><p>Fortunately, we have access to a robust dataset on <a href=\"https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset?ref=blog.lancedb.com\">Kaggle</a> containing information from various sources for approximately 45,000 movies. To follow along, please download the data from Kaggle and place it inside your working directory.</p><p>if you require additional data, you can supplement the dataset by extracting information from platforms like Rotten Tomatoes, IMDb, or even box-office records.</p><p>Our next step is to extract the core details from this dataset and generate a universal summary for each movie. Initially, I’ll combine the movie’s title, genres, and overviews into a single textual string. Then, this text will be tagged to create TaggedDocument instances, which will be utilized to train the Doc2Vec model later on.</p><p>Before moving forward, let’s install the relevant libraries to make our life easier.</p><pre><code class=\"language-python\">pip install torch scikit-learn pylance lancedb nltk gensim scipy==1.12</code></pre><p>Next, we’ll proceed with the ingestion and preprocessing of the data. To simplify the process, we’ll work with chunks of 1000 movies at a time. For clarity, we’ll only include movie indices with non-null values for genres, accurate titles, and complete overviews. This approach ensures that we’re working with high-quality, relevant data for our analysis.</p><pre><code class=\"language-python\">import torchimport pandas as pdimport numpy as npfrom gensim.models.doc2vec import Doc2Vec, TaggedDocumentfrom nltk.tokenize import word_tokenizefrom tqdm import tqdmimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import DataLoader, TensorDatasetfrom gensim.models.doc2vec import Doc2Vec, TaggedDocumentfrom nltk.tokenize import word_tokenizefrom sklearn.preprocessing import MultiLabelBinarizerfrom sklearn.model_selection import train_test_splitfrom tqdm import tqdmimport nltknltk.download('punkt')# Read data from CSV filemovie_data = pd.read_csv('movies_metadata.csv', low_memory=False)device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')def preprocess_data(movie_data_chunk):    tagged_docs = []    valid_indices = []    movie_info = []    # Wrap your loop with tqdm    for i, row in tqdm(movie_data_chunk.iterrows(), total=len(movie_data_chunk)):        try:            # Constructing movie text            movies_text = ''            genres = ', '.join([genre['name'] for genre in eval(row['genres'])])            movies_text += \"Overview: \" + row['overview'] + '\\n'            movies_text += \"Genres: \" + genres + '\\n'            movies_text += \"Title: \" + row['title'] + '\\n'            tagged_docs.append(TaggedDocument(words=word_tokenize(movies_text.lower()), tags=[str(i)]))            valid_indices.append(i)            movie_info.append((row['title'], genres))        except Exception as e:            continue    return tagged_docs, valid_indices, movie_info</code></pre><h3 id=\"generating-embeddings-using-doc2vec\">Generating embeddings using Doc2Vec</h3><p>Next, we’ll utilize the Doc2Vec model to generate embeddings for each movie based on the preprocessed text. We’ll allow the Doc2Vec model to train for several epochs to capture the essence of the various movies and their metadata in the multidimensional latent space. This process will help us represent each movie in a way that captures its unique characteristics and context.</p><pre><code class=\"language-python\">def train_doc2vec_model(tagged_data, num_epochs=20):    # Initialize Doc2Vec model    doc2vec_model = Doc2Vec(vector_size=100, min_count=2, epochs=num_epochs)    doc2vec_model.build_vocab(tqdm(tagged_data, desc=\"Building Vocabulary\"))    for epoch in range(num_epochs):        doc2vec_model.train(tqdm(tagged_data, desc=f\"Epoch {epoch+1}\"), total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)        return doc2vec_model# Preprocess data and extract genres for the first 1000 movieschunk_size = 1000tagged_data = []valid_indices = []movie_info = []for chunk_start in range(0, len(movie_data), chunk_size):    movie_data_chunk = movie_data.iloc[chunk_start:chunk_start+chunk_size]    chunk_tagged_data, chunk_valid_indices, chunk_movie_info = preprocess_data(movie_data_chunk)    tagged_data.extend(chunk_tagged_data)    valid_indices.extend(chunk_valid_indices)    movie_info.extend(chunk_movie_info)doc2vec_model = train_doc2vec_model(tagged_data)</code></pre><p>The <code>train_doc2vec_model</code> function trains a Doc2Vec model on the tagged movie data, producing 100-dimensional embeddings for each movie. These embeddings act as input features for the neural network.</p><p>With our current training setup, we are sure that movies with identical genres and similar kinds of overviews will be positioned closer to each other in the latent space, reflecting their thematic and content similarities.</p><h3 id=\"extracting-the-unique-genre-labels\">Extracting the unique genre labels</h3><p>Next, our focus shifts to compiling the names of relevant movies along with their genres. Now won</p><p>To illustrate this, Let’s consider a movie with three genres: ‘Drama’, ‘Comedy’, and ‘Horror’. Using the <code>MultiLabelBinarizer</code>, we’ll represent these genres with lists of 0s and 1s. If a movie belongs to a particular genre, it will be assigned a 1; if it doesn’t, it will receive a 0. Now each row in our dataset will indicate which genres are associated with a specific movie. This approach simplifies the genre representation for easier analysis.</p><p>Let’s take the movie “Top Gun Maverick” as a reference. We’ll associate its genres using binary encoding. Suppose this movie is categorized only under ‘drama’, not ‘comedy’ or ‘horror’. When we apply the MultiLabelBinarizer, the representation would be: Drama: 1, Comedy: 0, Horror: 0. This signifies that “Top Gun Maverick” is classified as a drama but not as a comedy or horror. We’ll replicate this process for all the movies in our dataset to identify the unique genre labels present in our data.</p><h3 id=\"training-a-neural-network-for-genre-classification-task\">Training a Neural Network for genre classification task</h3><p>We’ll define a neural network consisting of four linear layers with ReLU activations. The final layer utilizes softmax activation to generate probability scores for various genres. If your objective is primarily classification within the genre spectrum, where you input a movie description to determine its relevant genres, you can establish a threshold value for the multi-label softmax output. This allows you to select the top ‘n’ genres with the highest probabilities.</p><p>Here’s the neural network class, hyperparameter settings, and the corresponding training loop for training our model.</p><pre><code class=\"language-python\"># Extract genre labels for the valid indicesgenres_list = []for i in valid_indices:    row = movie_data.loc[i]    genres = [genre['name'] for genre in eval(row['genres'])]    genres_list.append(genres)mlb = MultiLabelBinarizer()genre_labels = mlb.fit_transform(genres_list)embeddings = []for i in valid_indices:    embeddings.append(doc2vec_model.dv[str(i)])X_train, X_test, y_train, y_test = train_test_split(embeddings, genre_labels, test_size=0.2, random_state=42)X_train_np = np.array(X_train, dtype=np.float32)y_train_np = np.array(y_train, dtype=np.float32)X_test_np = np.array(X_test, dtype=np.float32)y_test_np = np.array(y_test, dtype=np.float32)X_train_tensor = torch.tensor(X_train_np)y_train_tensor = torch.tensor(y_train_np)X_test_tensor = torch.tensor(X_test_np)y_test_tensor = torch.tensor(y_test_np)class GenreClassifier(nn.Module):    def __init__(self, input_size, output_size):        super(GenreClassifier, self).__init__()        self.fc1 = nn.Linear(input_size, 512)        self.bn1 = nn.BatchNorm1d(512)        self.fc2 = nn.Linear(512, 256)        self.bn2 = nn.BatchNorm1d(256)        self.fc3 = nn.Linear(256, 128)        self.bn3 = nn.BatchNorm1d(128)        self.fc4 = nn.Linear(128, output_size)        self.relu = nn.ReLU()        self.dropout = nn.Dropout(p=0.2)  # Adjust the dropout rate as needed    def forward(self, x):        x = self.fc1(x)        x = self.bn1(x)        x = self.relu(x)        x = self.dropout(x)        x = self.fc2(x)        x = self.bn2(x)        x = self.relu(x)        x = self.dropout(x)        x = self.fc3(x)        x = self.bn3(x)        x = self.relu(x)        x = self.dropout(x)        x = self.fc4(x)        return x# Move model to the selected devicemodel = GenreClassifier(input_size=100, output_size=len(mlb.classes_)).to(device)# Define loss function and optimizercriterion = nn.BCEWithLogitsLoss()optimizer = optim.Adam(model.parameters(), lr=0.001)# Training loopepochs = 50batch_size = 64train_dataset = TensorDataset(X_train_tensor.to(device), y_train_tensor.to(device))train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)for epoch in range(epochs):    model.train()    running_loss = 0.0    for inputs, labels in train_loader:        inputs, labels = inputs.to(device), labels.to(device)  # Move data to device        optimizer.zero_grad()        outputs = model(inputs)        loss = criterion(outputs, labels)        loss.backward()        optimizer.step()        running_loss += loss.item() * inputs.size(0)    epoch_loss = running_loss / len(train_loader.dataset)    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}')</code></pre><p>That’s it! We’ve successfully trained a neural network for our genre classification task. Let’s test how our model is performing on the genre classification task.</p><pre><code class=\"language-python\">from sklearn.metrics import f1_scoremodel.eval()with torch.no_grad():    X_test_tensor, y_test_tensor = X_test_tensor.to(device), y_test_tensor.to(device)  # Move test data to device    outputs = model(X_test_tensor)    test_loss = criterion(outputs, y_test_tensor)    print(f'Test Loss: {test_loss.item():.4f}')thresholds = [0.1] * len(mlb.classes_)thresholds_tensor = torch.tensor(thresholds, device=device).unsqueeze(0)# Convert the outputs to binary predictions using varying thresholdspredicted_labels = (outputs &gt; thresholds_tensor).cpu().numpy()# Convert binary predictions and actual labels to multi-label formatpredicted_multilabels = mlb.inverse_transform(predicted_labels)actual_multilabels = mlb.inverse_transform(y_test_np)# Print the Predicted and Actual Labels for each moviefor i, (predicted, actual) in enumerate(zip(predicted_multilabels, actual_multilabels)):    print(f'Movie {i+1}:')    print(f'    Predicted Labels: {predicted}')    print(f'    Actual Labels: {actual}')# Compute F1-scoref1 = f1_score(y_test_np, predicted_labels, average='micro')print(f'F1-score: {f1:.4f}')# Saving the trained modeltorch.save(model.state_dict(), 'trained_model.pth')</code></pre><h3 id=\"a-movie-recommendation-system\">A movie recommendation system</h3><p>To build a movie recommendation system, we’ll allow users to input a movie name, and based on that, we’ll return relevant recommendations. We’ll save our Doc2Vec embeddings in a vector database to achieve this. When a user inputs a query in the form of a new movie, we’ll first locate its embeddings in our vector database. Once we have this, we’ll find the ‘n’ number of movies whose embeddings are similar to our query movie. We can assess the similarity using various search algorithms like cosine similarity or finding the least Euclidean distance.</p><p>We’ll organize the data and the embeddings into a CSV file.</p><pre><code class=\"language-python\">import lancedbimport numpy as npimport pandas as pddata = []for i in valid_indices:    embedding = doc2vec_model.dv[str(i)]    title, genres = movie_info[valid_indices.index(i)]    data.append({\"title\": title, \"genres\": genres, \"vector\": embedding.tolist()})db = lancedb.connect(\".db\")tbl = db.create_table(\"doc2vec_embeddings\", data, mode=\"Overwrite\")db[\"doc2vec_embeddings\"].head()</code></pre><p>Essentially, we establish a connection to LanceDB, set up our table, and add our movie data to it.</p><p>Each row in the table represents a single movie, with columns containing data like the title, genres, overview, and embeddings. For each movie title, we check if it exists in our dataset. If it does, we perform a cosine similarity search on all other movies and return the top 10 most relevant titles. This columnar format makes it easy to store and retrieve information for various tasks involving embeddings.</p><h3 id=\"using-doc2vec-embeddings-to-get-the-relevant-recommendations\">Using Doc2Vec Embeddings to get the relevant recommendations.</h3><p>Our recommendation engine combines a neural network-based genre prediction model with a vector similarity search to provide relevant movie recommendations.</p><p>For a given query movie, first, we use our trained neural network to predict its genres. Based on these predicted genres, we filter our movie database to include only those movies that share at least one genre with the query movie, achieved by constructing an appropriate SQL filter.</p><p>We then perform a vector similarity search on this filtered subset to retrieve the most similar movies based on their vector representations. This approach ensures that the recommended movies are not only similar in terms of their vector characteristics but also share genre preferences with the query movie, resulting in more relevant and personalized recommendations.</p><pre><code class=\"language-python\"># Function to get genres for a single movie querydef get_genres_for_query(model, query_embedding, mlb, thresholds, device):    model.eval()    with torch.no_grad():        query_tensor = torch.tensor(query_embedding, dtype=torch.float32).unsqueeze(0).to(device)        outputs = model(query_tensor)        thresholds = [0.001] * len(mlb.classes_)        thresold_tensor = torch.tensor(thresholds, device=device).unsqueeze(0)        predicted_labels = (outputs &gt;= thresold_tensor).cpu().numpy()        predicted_multilabels = mlb.inverse_transform(predicted_labels)        return predicted_multilabelsdef movie_genre_prediction(movie_title):    movie_index = movie_data.index[movie_data['title'] == movie_title].tolist()[0]    query_embedding = doc2vec_model.dv[str(movie_index)]    predicted_genres = get_genres_for_query(model, query_embedding, mlb, [0.1] * len(mlb.classes_), device=device)    return predicted_genres</code></pre><p>And now, after all the groundwork, we’ve arrived at the final piece of the puzzle. Let’s generate some relevant recommendations using embeddings and LanceDB.</p><pre><code class=\"language-python\">def get_recommendations(title):    pd_data = pd.DataFrame(data)    title_vector = pd_data[pd_data[\"title\"] == title][\"vector\"].values[0]    predicted_genres = movie_genre_prediction(title)    genres_movie = predicted_genres[0]  # Assuming predicted_genres is available    genre_conditions = [f\"genres LIKE '%{genre}%'\" for genre in genres_movie]    where_clause = \" OR \".join(genre_conditions)    result = (        tbl.search(title_vector)        .metric(\"cosine\")        .limit(10)        .where(where_clause)        .to_pandas()    )    return result[[\"title\"]]</code></pre><pre><code class=\"language-python\">get_recommendations(\"Toy Story\")</code></pre><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/movie-recommendation-using-rag/toy_story_recommendation.png?raw=true\" alt=\"movie_recommendation\" /></p><p>Some of the recommended movies are remarkably close matches. For example, when looking at “Toy Story,” which falls under “animation” and “family” movies, our recommendation system can find other movies in the similar genres.</p><p>That being said, here is the <a href=\"https://colab.research.google.com/drive/1ouQdHw26mqiMS8L6dFAsMSxycgpUj0R8?usp=sharing&amp;ref=blog.lancedb.com\">colab</a> link for the complete reference.</p>",
            "url": "http://localhost:4000/2024/05/31/movie-recommendation-system-with-rag-and-genre-classification",
            
            
            
            "tags": ["LLM","RAG"],
            
            "date_published": "2024-05-31T00:00:00+05:30",
            "date_modified": "2024-05-31T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/04/09/convert-any-image-dataset-to-lance",
            "title": "Convert any Image dataset to Lance",
            "summary": "This post gives a detailed overview of how we can convert any image dataset to lance",
            "content_text": "In our previous article, we explored the remarkable capabilities of the Lance format, a modern, columnar data storage solution designed to revolutionize the way we work with large image datasets in machine learning. For the same purpose, I have converted the cinic and mini-imagenet datasets to their lance versions. For this write-up, I will use the example of cinic dataset to explain how to convert any image dataset into the Lance format with a single script and  unlocking the full potential of this powerful technology.just in case, here are the cinic and mini-imagenet datasets in lance.Processing ImagesThe process_images function is the heart of our data conversion process. It is responsible for iterating over the image files in the specified dataset, reading the data of each image, and converting it into a PyArrow RecordBatch object on the binary scale. This function also extracts additional metadata, such as the filename, category, and data type (e.g., train, test, or validation), and stores it alongside the image data.def process_images(data_type):    # Get the current directory path    images_folder = os.path.join(\"cinic\", data_type)    # Define schema for RecordBatch    schema = pa.schema([('image', pa.binary()),                         ('filename', pa.string()),                         ('category', pa.string()),                         ('data_type', pa.string())])    # Iterate over the categories within each data type    for category in os.listdir(images_folder):        category_folder = os.path.join(images_folder, category)                # Iterate over the images within each category        for filename in tqdm(os.listdir(category_folder), desc=f\"Processing {data_type} - {category}\"):            # Construct the full path to the image            image_path = os.path.join(category_folder, filename)            # Read and convert the image to a binary format            with open(image_path, 'rb') as f:                binary_data = f.read()            image_array = pa.array([binary_data], type=pa.binary())            filename_array = pa.array([filename], type=pa.string())            category_array = pa.array([category], type=pa.string())            data_type_array = pa.array([data_type], type=pa.string())            # Yield RecordBatch for each image            yield pa.RecordBatch.from_arrays(                [image_array, filename_array, category_array, data_type_array],                schema=schema            )By leveraging the PyArrow library, the process_images function ensures that the image data is represented in a format that is compatible with the Lance format. The use of RecordBatch objects allows for efficient data structuring and enables seamless integration with the subsequent steps of the conversion process.One of the key features of this function is its ability to handle datasets with a hierarchical structure. It iterates over the categories within each data type, ensuring that the metadata associated with each image is accurately captured and preserved. This attention to detail is crucial, as it allows us to maintain the rich contextual information of us image dataset, which can be invaluable for tasks like classification, object detection, or semantic segmentation.Writing to LanceThe write_to_lance function takes the data generated by the process_images function and writes it to Lance datasets, one for each data type (e.g., train, test, validation). This step is where the true power of the Lance format is unleashed.The function first creates a PyArrow schema that defines the structure of the data to be stored in the Lance format. This schema includes the image data, as well as the associated metadata (filename, category, and data type). By specifying the schema upfront, the script ensures that the data is stored in a consistent and organized manner, making it easier to retrieve and work with in the future.def write_to_lance():    # Create an empty RecordBatchIterator    schema = pa.schema([        pa.field(\"image\", pa.binary()),        pa.field(\"filename\", pa.string()),        pa.field(\"category\", pa.string()),        pa.field(\"data_type\", pa.string())    ])    # Specify the path where you want to save the Lance files    images_folder = \"cinic\"        for data_type in ['train', 'test', 'val']:        lance_file_path = os.path.join(images_folder, f\"cinic_{data_type}.lance\")                reader = pa.RecordBatchReader.from_batches(schema, process_images(data_type))        lance.write_dataset(            reader,            lance_file_path,            schema,        )Next, the function iterates through the different data types, creating a Lance dataset file for each one. The lance.write_dataset function is then used to write the RecordBatchReader, generated from the process_images function, to the respective Lance dataset files.The benefits of this approach are numerous. By storing the data in the Lance format, you can take advantage of its columnar storage and compression techniques, resulting in significantly reduced storage requirements. Additionally, the optimized data layout and indexing capabilities of Lance enable lightning-fast data loading times, improving the overall performance and responsiveness of your machine learning pipelines.Loading into PandasThe final step in the process is to load the data from the Lance datasets into Pandas DataFrames, making the image data easily accessible for further processing and analysis in your machine learning workflows.The loading_into_pandas function demonstrates this process. It first locates the Lance dataset files, created in the previous step, and creates a Lance dataset object for each data type. The function then iterates over the batches of data, converting them into Pandas DataFrames and concatenating them into a single DataFrame for each data type.def loading_into_pandas():    # Load Lance files from the same folder    current_dir = os.getcwd()    images_folder = os.path.join(current_dir, \"cinic\")        data_frames = {}  # Dictionary to store DataFrames for each data type        for data_type in ['test', 'train', 'val']:        uri = os.path.join(images_folder, f\"cinic_{data_type}.lance\")        ds = lance.dataset(uri)        # Accumulate data from batches into a list        data = []        for batch in tqdm(ds.to_batches(columns=[\"image\", \"filename\", \"category\", \"data_type\"], batch_size=10), desc=f\"Loading {data_type} batches\"):            tbl = batch.to_pandas()            data.append(tbl)        # Concatenate all DataFrames into a single DataFrame        df = pd.concat(data, ignore_index=True)                # Store the DataFrame in the dictionary        data_frames[data_type] = df                print(f\"Pandas DataFrame for {data_type} is ready\")        print(\"Total Rows: \", df.shape[0])        return data_framesThis approach offers several advantages. By loading the data in batches, the function can efficiently handle large-scale image datasets without running into memory constraints. Additionally, the use of Pandas DataFrames provides a familiar and intuitive interface for working with the data, allowing you to leverage the rich ecosystem of Pandas-compatible libraries and tools for data manipulation, visualization, and analysis.Moreover, the function stores the DataFrames in a list, indexed by the data type. This structure enables us to easily access the specific subsets of your dataset (e.g., train, test, validation) as needed, further streamlining your machine learning workflows. I mean it’s too smooth guys.Putting It All TogetherBy running the provided script, you can convert your image datasets, whether they are industry-standard benchmarks or your own custom collections, into the powerful Lance format. This transformation unlocks a new level of efficiency and performance, empowering you to supercharge your machine learning projects.  I have used the same script for the mini-imagenet too, make sure your data directory looks like thishere is the complete script for your reference..import osimport pandas as pdimport pyarrow as paimport lanceimport timefrom tqdm import tqdmdef process_images(data_type):    # Get the current directory path    images_folder = os.path.join(\"cinic\", data_type)    # Define schema for RecordBatch    schema = pa.schema([('image', pa.binary()),                         ('filename', pa.string()),                         ('category', pa.string()),                         ('data_type', pa.string())])    # Iterate over the categories within each data type    for category in os.listdir(images_folder):        category_folder = os.path.join(images_folder, category)                # Iterate over the images within each category        for filename in tqdm(os.listdir(category_folder), desc=f\"Processing {data_type} - {category}\"):            # Construct the full path to the image            image_path = os.path.join(category_folder, filename)            # Read and convert the image to a binary format            with open(image_path, 'rb') as f:                binary_data = f.read()            image_array = pa.array([binary_data], type=pa.binary())            filename_array = pa.array([filename], type=pa.string())            category_array = pa.array([category], type=pa.string())            data_type_array = pa.array([data_type], type=pa.string())            # Yield RecordBatch for each image            yield pa.RecordBatch.from_arrays(                [image_array, filename_array, category_array, data_type_array],                schema=schema            )# Function to write PyArrow Table to Lance datasetdef write_to_lance():    # Create an empty RecordBatchIterator    schema = pa.schema([        pa.field(\"image\", pa.binary()),        pa.field(\"filename\", pa.string()),        pa.field(\"category\", pa.string()),        pa.field(\"data_type\", pa.string())    ])    # Specify the path where you want to save the Lance files    images_folder = \"cinic\"        for data_type in ['train', 'test', 'val']:        lance_file_path = os.path.join(images_folder, f\"cinic_{data_type}.lance\")                reader = pa.RecordBatchReader.from_batches(schema, process_images(data_type))        lance.write_dataset(            reader,            lance_file_path,            schema,        )def loading_into_pandas():    # Load Lance files from the same folder    current_dir = os.getcwd()    print(current_dir)    images_folder = os.path.join(current_dir, \"cinic\")        data_frames = {}  # Dictionary to store DataFrames for each data type        for data_type in ['test', 'train', 'val']:        uri = os.path.join(images_folder, f\"cinic_{data_type}.lance\")        ds = lance.dataset(uri)        # Accumulate data from batches into a list        data = []        for batch in tqdm(ds.to_batches(columns=[\"image\", \"filename\", \"category\", \"data_type\"], batch_size=10), desc=f\"Loading {data_type} batches\"):            tbl = batch.to_pandas()            data.append(tbl)        # Concatenate all DataFrames into a single DataFrame        df = pd.concat(data, ignore_index=True)                # Store the DataFrame in the dictionary        data_frames[data_type] = df                print(f\"Pandas DataFrame for {data_type} is ready\")        print(\"Total Rows: \", df.shape[0])        return data_framesif __name__ == \"__main__\":    start = time.time()    write_to_lance()    data_frames = loading_into_pandas()    end = time.time()    print(f\"Time(sec): {end - start}\")Take the different splits of the train, test and validation through different dataframes and utilize the information for your next image classifcation tasktrain = data_frames['train']test = data_frames['test']val = data_frames['val']and this is how the training dataframe looks liketrain.head()image\tfilename\tcategory\tdata_type\timage\tfilename\tcategory\tdata_type0\tb'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...\tn02130308_1836.png\tcat\ttrain1\tb'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...\tcifar10-train-21103.png\tcat\ttrain2\tb'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...\tcifar10-train-44957.png\tcat\ttrain3\tb'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...\tn02129604_14997.png\tcat\ttrain4\tb'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...\tn02123045_1463.png\tcat\ttrainThe benefits of this approach are numerous:  Storage Efficiency: The columnar storage and compression techniques employed by Lance result in significantly reduced storage requirements, making it an ideal choice for handling large-scale image datasets.  Fast Data Loading: The optimized data layout and indexing capabilities of Lance enable lightning-fast data loading times, improving the overall performance and responsiveness of your machine learning pipelines.  Random Access: The ability to selectively load specific data subsets from the Lance dataset allows for efficient data augmentation techniques and custom data loading strategies tailored to your unique requirements.  Unified Data Format: Lance can store diverse data types, such as images, text, and numerical data, in a single, streamlined format. This flexibility is invaluable in machine learning, where different modalities of data often need to be processed together.By adopting the Lance format, we can literally elevate our machine learning workflow to new heights, unlocking unprecedented levels of efficiency, performance, and flexibility. Take the first step by running the provided script and converting your image datasets to the Lance format – the future of machine learning data management is awaiting for you, who knows if you find your second love with lance format.",
            "content_html": "<p>In our <a href=\"https://vipul-maheshwari.github.io/2024/03/29/effortlessly-loading-and-processing-images-with-lance-a-code-walkthrough\">previous</a> article, we explored the remarkable capabilities of the Lance format, a modern, columnar data storage solution designed to revolutionize the way we work with large image datasets in machine learning. For the same purpose, I have converted the <code>cinic</code> and <code>mini-imagenet</code> datasets to their lance versions. For this write-up, I will use the example of <code>cinic</code> dataset to explain how to convert any image dataset into the Lance format with a single script and  unlocking the full potential of this powerful technology.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/convert-any-image-dataset-to-lance/meme_reaction.png?raw=true\" alt=\"lance_converter\" /></p><p>just in case, here are the <a href=\"https://www.kaggle.com/datasets/vipulmaheshwarii/cinic-10-lance-dataset\">cinic</a> and <a href=\"https://www.kaggle.com/datasets/vipulmaheshwarii/mini-imagenet-lance-dataset\">mini-imagenet</a> datasets in lance.</p><h3 id=\"processing-images\">Processing Images</h3><p>The <code>process_images</code> function is the heart of our data conversion process. It is responsible for iterating over the image files in the specified dataset, reading the data of each image, and converting it into a PyArrow RecordBatch object on the binary scale. This function also extracts additional metadata, such as the filename, category, and data type (e.g., train, test, or validation), and stores it alongside the image data.</p><pre><code class=\"language-python\">def process_images(data_type):    # Get the current directory path    images_folder = os.path.join(\"cinic\", data_type)    # Define schema for RecordBatch    schema = pa.schema([('image', pa.binary()),                         ('filename', pa.string()),                         ('category', pa.string()),                         ('data_type', pa.string())])    # Iterate over the categories within each data type    for category in os.listdir(images_folder):        category_folder = os.path.join(images_folder, category)                # Iterate over the images within each category        for filename in tqdm(os.listdir(category_folder), desc=f\"Processing {data_type} - {category}\"):            # Construct the full path to the image            image_path = os.path.join(category_folder, filename)            # Read and convert the image to a binary format            with open(image_path, 'rb') as f:                binary_data = f.read()            image_array = pa.array([binary_data], type=pa.binary())            filename_array = pa.array([filename], type=pa.string())            category_array = pa.array([category], type=pa.string())            data_type_array = pa.array([data_type], type=pa.string())            # Yield RecordBatch for each image            yield pa.RecordBatch.from_arrays(                [image_array, filename_array, category_array, data_type_array],                schema=schema            )</code></pre><p>By leveraging the PyArrow library, the <code>process_images</code> function ensures that the image data is represented in a format that is compatible with the Lance format. The use of <code>RecordBatch</code> objects allows for efficient data structuring and enables seamless integration with the subsequent steps of the conversion process.</p><p>One of the key features of this function is its ability to handle datasets with a hierarchical structure. It iterates over the categories within each data type, ensuring that the metadata associated with each image is accurately captured and preserved. This attention to detail is crucial, as it allows us to maintain the rich contextual information of us image dataset, which can be invaluable for tasks like classification, object detection, or semantic segmentation.</p><h3 id=\"writing-to-lance\">Writing to Lance</h3><p>The <code>write_to_lance</code> function takes the data generated by the <code>process_images</code> function and writes it to Lance datasets, one for each data type (e.g., train, test, validation). This step is where the true power of the Lance format is unleashed.</p><p>The function first creates a PyArrow schema that defines the structure of the data to be stored in the Lance format. This schema includes the image data, as well as the associated metadata (filename, category, and data type). By specifying the schema upfront, the script ensures that the data is stored in a consistent and organized manner, making it easier to retrieve and work with in the future.</p><pre><code class=\"language-python\">def write_to_lance():    # Create an empty RecordBatchIterator    schema = pa.schema([        pa.field(\"image\", pa.binary()),        pa.field(\"filename\", pa.string()),        pa.field(\"category\", pa.string()),        pa.field(\"data_type\", pa.string())    ])    # Specify the path where you want to save the Lance files    images_folder = \"cinic\"        for data_type in ['train', 'test', 'val']:        lance_file_path = os.path.join(images_folder, f\"cinic_{data_type}.lance\")                reader = pa.RecordBatchReader.from_batches(schema, process_images(data_type))        lance.write_dataset(            reader,            lance_file_path,            schema,        )</code></pre><p>Next, the function iterates through the different data types, creating a Lance dataset file for each one. The <code>lance.write_dataset</code> function is then used to write the <code>RecordBatchReader</code>, generated from the <code>process_images</code> function, to the respective Lance dataset files.</p><p>The benefits of this approach are numerous. By storing the data in the Lance format, you can take advantage of its columnar storage and compression techniques, resulting in significantly reduced storage requirements. Additionally, the optimized data layout and indexing capabilities of Lance enable lightning-fast data loading times, improving the overall performance and responsiveness of your machine learning pipelines.</p><h3 id=\"loading-into-pandas\">Loading into Pandas</h3><p>The final step in the process is to load the data from the Lance datasets into Pandas DataFrames, making the image data easily accessible for further processing and analysis in your machine learning workflows.</p><p>The <code>loading_into_pandas</code> function demonstrates this process. It first locates the Lance dataset files, created in the previous step, and creates a Lance dataset object for each data type. The function then iterates over the batches of data, converting them into Pandas DataFrames and concatenating them into a single DataFrame for each data type.</p><pre><code class=\"language-python\">def loading_into_pandas():    # Load Lance files from the same folder    current_dir = os.getcwd()    images_folder = os.path.join(current_dir, \"cinic\")        data_frames = {}  # Dictionary to store DataFrames for each data type        for data_type in ['test', 'train', 'val']:        uri = os.path.join(images_folder, f\"cinic_{data_type}.lance\")        ds = lance.dataset(uri)        # Accumulate data from batches into a list        data = []        for batch in tqdm(ds.to_batches(columns=[\"image\", \"filename\", \"category\", \"data_type\"], batch_size=10), desc=f\"Loading {data_type} batches\"):            tbl = batch.to_pandas()            data.append(tbl)        # Concatenate all DataFrames into a single DataFrame        df = pd.concat(data, ignore_index=True)                # Store the DataFrame in the dictionary        data_frames[data_type] = df                print(f\"Pandas DataFrame for {data_type} is ready\")        print(\"Total Rows: \", df.shape[0])        return data_frames</code></pre><p>This approach offers several advantages. By loading the data in batches, the function can efficiently handle large-scale image datasets without running into memory constraints. Additionally, the use of Pandas DataFrames provides a familiar and intuitive interface for working with the data, allowing you to leverage the rich ecosystem of Pandas-compatible libraries and tools for data manipulation, visualization, and analysis.</p><p>Moreover, the function stores the DataFrames in a list, indexed by the data type. This structure enables us to easily access the specific subsets of your dataset (e.g., train, test, validation) as needed, further streamlining your machine learning workflows. I mean it’s too smooth guys.</p><h3 id=\"putting-it-all-together\">Putting It All Together</h3><p>By running the provided script, you can convert your image datasets, whether they are industry-standard benchmarks or your own custom collections, into the powerful Lance format. This transformation unlocks a new level of efficiency and performance, empowering you to supercharge your machine learning projects.  I have used the same script for the <code>mini-imagenet</code> too, make sure your data directory looks like this</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/convert-any-image-dataset-to-lance/cinic_image.png?raw=true\" alt=\"data_folders\" /></p><p>here is the complete script for your reference..</p><pre><code class=\"language-python\">import osimport pandas as pdimport pyarrow as paimport lanceimport timefrom tqdm import tqdmdef process_images(data_type):    # Get the current directory path    images_folder = os.path.join(\"cinic\", data_type)    # Define schema for RecordBatch    schema = pa.schema([('image', pa.binary()),                         ('filename', pa.string()),                         ('category', pa.string()),                         ('data_type', pa.string())])    # Iterate over the categories within each data type    for category in os.listdir(images_folder):        category_folder = os.path.join(images_folder, category)                # Iterate over the images within each category        for filename in tqdm(os.listdir(category_folder), desc=f\"Processing {data_type} - {category}\"):            # Construct the full path to the image            image_path = os.path.join(category_folder, filename)            # Read and convert the image to a binary format            with open(image_path, 'rb') as f:                binary_data = f.read()            image_array = pa.array([binary_data], type=pa.binary())            filename_array = pa.array([filename], type=pa.string())            category_array = pa.array([category], type=pa.string())            data_type_array = pa.array([data_type], type=pa.string())            # Yield RecordBatch for each image            yield pa.RecordBatch.from_arrays(                [image_array, filename_array, category_array, data_type_array],                schema=schema            )# Function to write PyArrow Table to Lance datasetdef write_to_lance():    # Create an empty RecordBatchIterator    schema = pa.schema([        pa.field(\"image\", pa.binary()),        pa.field(\"filename\", pa.string()),        pa.field(\"category\", pa.string()),        pa.field(\"data_type\", pa.string())    ])    # Specify the path where you want to save the Lance files    images_folder = \"cinic\"        for data_type in ['train', 'test', 'val']:        lance_file_path = os.path.join(images_folder, f\"cinic_{data_type}.lance\")                reader = pa.RecordBatchReader.from_batches(schema, process_images(data_type))        lance.write_dataset(            reader,            lance_file_path,            schema,        )def loading_into_pandas():    # Load Lance files from the same folder    current_dir = os.getcwd()    print(current_dir)    images_folder = os.path.join(current_dir, \"cinic\")        data_frames = {}  # Dictionary to store DataFrames for each data type        for data_type in ['test', 'train', 'val']:        uri = os.path.join(images_folder, f\"cinic_{data_type}.lance\")        ds = lance.dataset(uri)        # Accumulate data from batches into a list        data = []        for batch in tqdm(ds.to_batches(columns=[\"image\", \"filename\", \"category\", \"data_type\"], batch_size=10), desc=f\"Loading {data_type} batches\"):            tbl = batch.to_pandas()            data.append(tbl)        # Concatenate all DataFrames into a single DataFrame        df = pd.concat(data, ignore_index=True)                # Store the DataFrame in the dictionary        data_frames[data_type] = df                print(f\"Pandas DataFrame for {data_type} is ready\")        print(\"Total Rows: \", df.shape[0])        return data_framesif __name__ == \"__main__\":    start = time.time()    write_to_lance()    data_frames = loading_into_pandas()    end = time.time()    print(f\"Time(sec): {end - start}\")</code></pre><p>Take the different splits of the train, test and validation through different dataframes and utilize the information for your next image classifcation task</p><pre><code class=\"language-python\">train = data_frames['train']test = data_frames['test']val = data_frames['val']</code></pre><p>and this is how the training dataframe looks like</p><pre><code class=\"language-python\">train.head()</code></pre><pre><code>image\tfilename\tcategory\tdata_type\timage\tfilename\tcategory\tdata_type0\tb'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...\tn02130308_1836.png\tcat\ttrain1\tb'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...\tcifar10-train-21103.png\tcat\ttrain2\tb'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...\tcifar10-train-44957.png\tcat\ttrain3\tb'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...\tn02129604_14997.png\tcat\ttrain4\tb'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\...\tn02123045_1463.png\tcat\ttrain</code></pre><p>The benefits of this approach are numerous:</p><ol>  <li>Storage Efficiency: The columnar storage and compression techniques employed by Lance result in significantly reduced storage requirements, making it an ideal choice for handling large-scale image datasets.</li>  <li>Fast Data Loading: The optimized data layout and indexing capabilities of Lance enable lightning-fast data loading times, improving the overall performance and responsiveness of your machine learning pipelines.</li>  <li>Random Access: The ability to selectively load specific data subsets from the Lance dataset allows for efficient data augmentation techniques and custom data loading strategies tailored to your unique requirements.</li>  <li>Unified Data Format: Lance can store diverse data types, such as images, text, and numerical data, in a single, streamlined format. This flexibility is invaluable in machine learning, where different modalities of data often need to be processed together.</li></ol><p>By adopting the Lance format, we can literally elevate our machine learning workflow to new heights, unlocking unprecedented levels of efficiency, performance, and flexibility. Take the first step by running the provided script and converting your image datasets to the Lance format – the future of machine learning data management is awaiting for you, who knows if you find your second love with lance format.</p>",
            "url": "http://localhost:4000/2024/04/09/convert-any-image-dataset-to-lance",
            
            
            
            "tags": ["LanceDB","Dataset"],
            
            "date_published": "2024-04-09T00:00:00+05:30",
            "date_modified": "2024-04-09T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/03/29/effortlessly-loading-and-processing-images-with-lance-a-code-walkthrough",
            "title": "Effortlessly Loading and Processing Images with Lance",
            "summary": "How you can use the lance format to work with big sized data",
            "content_text": "Working with large image datasets in machine learning can be challenging, often requiring significant computational resources and efficient data-handling techniques. While widely used for image storage, traditional file formats like JPEG or PNG are not optimized for efficient data loading and processing in Machine learning workflows. This is where the Lance format shines, offering a modern, columnar data storage solution designed specifically for machine learning applications.The Lance format stores data in a compressed columnar format, enabling efficient storage, fast data loading, and fast random access to data subsets. Additionally, the Lance format is maintained on disk, which provides a couple of advantages: It will persist through a system failure and doesn’t rely on keeping everything in memory, which can run out. This also lends itself to enhanced data privacy and security, as the data doesn’t need to be transferred over a network.One of the other key advantages of the Lance format is its ability to store diverse data types, such as images, text, and numerical data, in a unified format. Imagine having a data lake where each kind of data can be stored seamlessly without separating underlying data types. This flexibility is particularly valuable in machine learning pipelines, where different data types often need to be processed together. This unparalleled flexibility is a game-changer in machine learning pipelines, where different modalities of data often need to be processed together for tasks like multimodal learning, audio-visual analysis, or natural language processing with visual inputs.With Lance, you can effortlessly consider all kinds of data, from images to videos and audio files to text data and numerical values, all within the same columnar storage format. This means you can have a single, streamlined data pipeline that can handle any combination of data types without the need for complex data transformations or conversions. Lance easily handles it without worrying about compatibility issues or dealing with separate storage formats for different data types. And the best part? You can store and retrieve all these diverse data types within the same column.In contrast, while efficient for tabular data, traditional formats like Parquet may need to handle diverse data types better. By converting all data into a single, unified format using Lance, you can retrieve and process any type of data without dealing with multiple formats or complex data structures.In this article, I’ll walk through a Python code example that demonstrates how to convert a dataset of GTA5 images into the Lance format and subsequently load them into a Pandas DataFrame for further processing.import osimport pandas as pdimport pyarrow as paimport lanceimport timefrom tqdm import tqdmWe start by importing the necessary libraries, including os for directory handling, pandas for data manipulation, pyarrow for working with Arrow data formats, lance for interacting with the Lance format, and tqdm for displaying progress bars.def process_images():    current_dir = os.getcwd()    images_folder = os.path.join(current_dir, \"./image\")    # Define schema for RecordBatch    schema = pa.schema([('image', pa.binary())])    image_files = [filename for filename in os.listdir(images_folder)                if filename.endswith((\".png\", \".jpg\", \".jpeg\"))]    # Iterate over all images in the folder with tqdm    for filename in tqdm(image_files, desc=\"Processing Images\"):        # Construct the full path to the image        image_path = os.path.join(images_folder, filename)        # Read and convert the image to a binary format        with open(image_path, 'rb') as f:            binary_data = f.read()        image_array = pa.array([binary_data], type=pa.binary())        # Yield RecordBatch for each image        yield pa.RecordBatch.from_arrays([image_array], schema=schema)The process_images function is responsible for iterating over all image files in a specified directory and converting them into PyArrow RecordBatch objects. It first defines the schema for the RecordBatch, specifying that each batch will contain a single binary column named ‘image’.It then iterates over all image files in the directory, reads each image’s binary data, and yields a RecordBatch containing that image’s binary data.def write_to_lance():    schema = pa.schema([        pa.field(\"image\", pa.binary())    ])    reader = pa.RecordBatchReader.from_batches(schema, process_images())    lance.write_dataset(        reader,        \"image_dataset.lance\",        schema,    )The write_to_lance function creates a RecordBatchReader from the process_images generator and writes the resulting data to a Lance dataset named “image_dataset.lance”. This step converts the image data into the efficient, columnar Lance format, optimizing it for fast data loading and random access.def loading_into_pandas():    uri = \"image_dataset.lance\"    ds = lance.dataset(uri)    # Accumulate data from batches into a list    data = []    for batch in ds.to_batches(columns=[\"image\"], batch_size=10):        tbl = batch.to_pandas()        data.append(tbl)    # Concatenate all DataFrames into a single DataFrame    df = pd.concat(data, ignore_index=True)    print(\"Pandas DataFrame is ready\")    print(\"Total Rows: \", df.shape[0])The loading_into_pandas function demonstrates how to load the image data from the Lance dataset into a Pandas DataFrame. It first creates a Lance dataset object from the “image_dataset.lance” file. Then, it iterates over batches of data, converting each batch into a Pandas DataFrame and appending it to a list. Finally, it concatenates all the DataFrames in the list into a single DataFrame, making the image data accessible for further processing or analysis.if __name__ == \"__main__\":    start = time.time()    write_to_lance()    loading_into_pandas()    end = time.time()    print(f\"Time(sec): {end - start}\")The central part of the script calls the write_to_lance and loading_into_pandas functions, measuring the total execution time for the entire process.By leveraging the Lance format, this code demonstrates how to efficiently store and load large image datasets for machine learning applications. The columnar storage and compression techniques Lance uses result in reduced storage requirements and faster data loading times, making it an ideal choice for working with large-scale image data.Moreover, the random access capabilities of Lance allow for selective loading of specific data subsets, enabling efficient data augmentation techniques and custom data loading strategies tailored to your machine learning workflow.TLDR: Lance format provides a powerful and efficient solution for handling multimodal data in machine learning pipelines, streamlining data storage, loading, and processing tasks. By adopting Lance, we can improve our machine learning projects’ overall performance and resource efficiency while also benefiting from the ability to store diverse data types in a unified format and maintain data locality and privacy. Here is the whole script for your reference.import osimport pandas as pdimport pyarrow as paimport lanceimport timefrom tqdm import tqdmdef process_images():    current_dir = os.getcwd()    images_folder = os.path.join(current_dir, \"./image\")    # Define schema for RecordBatch    schema = pa.schema([('image', pa.binary())])    image_files = [filename for filename in os.listdir(images_folder)                if filename.endswith((\".png\", \".jpg\", \".jpeg\"))]    # Iterate over all images in the folder with tqdm    for filename in tqdm(image_files, desc=\"Processing Images\"):        # Construct the full path to the image        image_path = os.path.join(images_folder, filename)        # Read and convert the image to a binary format        with open(image_path, 'rb') as f:            binary_data = f.read()        image_array = pa.array([binary_data], type=pa.binary())        # Yield RecordBatch for each image        yield pa.RecordBatch.from_arrays([image_array], schema=schema)def write_to_lance():        schema = pa.schema([        pa.field(\"image\", pa.binary())    ])    reader = pa.RecordBatchReader.from_batches(schema, process_images())    lance.write_dataset(        reader,        \"image_dataset.lance\",        schema,    )def loading_into_pandas():    uri = \"image_dataset.lance\"    ds = lance.dataset(uri)    # Accumulate data from batches into a list    data = []    for batch in ds.to_batches(columns=[\"image\"], batch_size=10):        tbl = batch.to_pandas()        data.append(tbl)    # Concatenate all DataFrames into a single DataFrame    df = pd.concat(data, ignore_index=True)    print(\"Pandas DataFrame is ready\")    print(\"Total Rows: \", df.shape[0])if __name__ == \"__main__\":    start = time.time()    write_to_lance()    loading_into_pandas()    end = time.time()    print(f\"Time(sec): {end - start}\")Imagine using Lance-formatted image data to make machine learning and deep learning projects faster. Something big is coming up, stay tuned.",
            "content_html": "<p>Working with large image datasets in machine learning can be challenging, often requiring significant computational resources and efficient data-handling techniques. While widely used for image storage, traditional file formats like JPEG or PNG are not optimized for efficient data loading and processing in Machine learning workflows. This is where the Lance format shines, offering a modern, columnar data storage solution designed specifically for machine learning applications.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/loading_and_processing_image_with_lance/image.png?raw=true\" alt=\"meme_for_ml_workloads\" /></p><p>The Lance format stores data in a compressed columnar format, enabling efficient storage, fast data loading, and fast random access to data subsets. Additionally, the Lance format is maintained on disk, which provides a couple of advantages: It will persist through a system failure and doesn’t rely on keeping everything in memory, which can run out. This also lends itself to enhanced data privacy and security, as the data doesn’t need to be transferred over a network.</p><p>One of the other key advantages of the Lance format is its ability to store diverse data types, such as images, text, and numerical data, in a unified format. Imagine having a data lake where each kind of data can be stored seamlessly without separating underlying data types. This flexibility is particularly valuable in machine learning pipelines, where different data types often need to be processed together. This unparalleled flexibility is a game-changer in machine learning pipelines, where different modalities of data often need to be processed together for tasks like multimodal learning, audio-visual analysis, or natural language processing with visual inputs.</p><p>With Lance, you can effortlessly consider all kinds of data, from images to videos and audio files to text data and numerical values, all within the same columnar storage format. This means you can have a single, streamlined data pipeline that can handle any combination of data types without the need for complex data transformations or conversions. Lance easily handles it without worrying about compatibility issues or dealing with separate storage formats for different data types. And the best part? You can store and retrieve all these diverse data types within the same column.</p><p>In contrast, while efficient for tabular data, traditional formats like Parquet may need to handle diverse data types better. By converting all data into a single, unified format using Lance, you can retrieve and process any type of data without dealing with multiple formats or complex data structures.In this article, I’ll walk through a Python code example that demonstrates how to convert a dataset of GTA5 images into the Lance format and subsequently load them into a Pandas DataFrame for further processing.</p><pre><code class=\"language-python\">import osimport pandas as pdimport pyarrow as paimport lanceimport timefrom tqdm import tqdm</code></pre><p>We start by importing the necessary libraries, including os for directory handling, pandas for data manipulation, pyarrow for working with Arrow data formats, lance for interacting with the Lance format, and tqdm for displaying progress bars.</p><pre><code class=\"language-python\">def process_images():    current_dir = os.getcwd()    images_folder = os.path.join(current_dir, \"./image\")    # Define schema for RecordBatch    schema = pa.schema([('image', pa.binary())])    image_files = [filename for filename in os.listdir(images_folder)                if filename.endswith((\".png\", \".jpg\", \".jpeg\"))]    # Iterate over all images in the folder with tqdm    for filename in tqdm(image_files, desc=\"Processing Images\"):        # Construct the full path to the image        image_path = os.path.join(images_folder, filename)        # Read and convert the image to a binary format        with open(image_path, 'rb') as f:            binary_data = f.read()        image_array = pa.array([binary_data], type=pa.binary())        # Yield RecordBatch for each image        yield pa.RecordBatch.from_arrays([image_array], schema=schema)</code></pre><p>The process_images function is responsible for iterating over all image files in a specified directory and converting them into PyArrow RecordBatch objects. It first defines the schema for the RecordBatch, specifying that each batch will contain a single binary column named ‘image’.</p><p>It then iterates over all image files in the directory, reads each image’s binary data, and yields a RecordBatch containing that image’s binary data.</p><pre><code class=\"language-python\">def write_to_lance():    schema = pa.schema([        pa.field(\"image\", pa.binary())    ])    reader = pa.RecordBatchReader.from_batches(schema, process_images())    lance.write_dataset(        reader,        \"image_dataset.lance\",        schema,    )</code></pre><p>The write_to_lance function creates a RecordBatchReader from the process_images generator and writes the resulting data to a Lance dataset named “image_dataset.lance”. This step converts the image data into the efficient, columnar Lance format, optimizing it for fast data loading and random access.</p><pre><code class=\"language-python\">def loading_into_pandas():    uri = \"image_dataset.lance\"    ds = lance.dataset(uri)    # Accumulate data from batches into a list    data = []    for batch in ds.to_batches(columns=[\"image\"], batch_size=10):        tbl = batch.to_pandas()        data.append(tbl)    # Concatenate all DataFrames into a single DataFrame    df = pd.concat(data, ignore_index=True)    print(\"Pandas DataFrame is ready\")    print(\"Total Rows: \", df.shape[0])</code></pre><p>The loading_into_pandas function demonstrates how to load the image data from the Lance dataset into a Pandas DataFrame. It first creates a Lance dataset object from the “image_dataset.lance” file. Then, it iterates over batches of data, converting each batch into a Pandas DataFrame and appending it to a list. Finally, it concatenates all the DataFrames in the list into a single DataFrame, making the image data accessible for further processing or analysis.</p><pre><code class=\"language-python\">if __name__ == \"__main__\":    start = time.time()    write_to_lance()    loading_into_pandas()    end = time.time()    print(f\"Time(sec): {end - start}\")</code></pre><p>The central part of the script calls the write_to_lance and loading_into_pandas functions, measuring the total execution time for the entire process.By leveraging the Lance format, this code demonstrates how to efficiently store and load large image datasets for machine learning applications. The columnar storage and compression techniques Lance uses result in reduced storage requirements and faster data loading times, making it an ideal choice for working with large-scale image data.</p><p>Moreover, the random access capabilities of Lance allow for selective loading of specific data subsets, enabling efficient data augmentation techniques and custom data loading strategies tailored to your machine learning workflow.</p><p>TLDR: Lance format provides a powerful and efficient solution for handling multimodal data in machine learning pipelines, streamlining data storage, loading, and processing tasks. By adopting Lance, we can improve our machine learning projects’ overall performance and resource efficiency while also benefiting from the ability to store diverse data types in a unified format and maintain data locality and privacy. Here is the whole script for your reference.</p><pre><code class=\"language-python\">import osimport pandas as pdimport pyarrow as paimport lanceimport timefrom tqdm import tqdmdef process_images():    current_dir = os.getcwd()    images_folder = os.path.join(current_dir, \"./image\")    # Define schema for RecordBatch    schema = pa.schema([('image', pa.binary())])    image_files = [filename for filename in os.listdir(images_folder)                if filename.endswith((\".png\", \".jpg\", \".jpeg\"))]    # Iterate over all images in the folder with tqdm    for filename in tqdm(image_files, desc=\"Processing Images\"):        # Construct the full path to the image        image_path = os.path.join(images_folder, filename)        # Read and convert the image to a binary format        with open(image_path, 'rb') as f:            binary_data = f.read()        image_array = pa.array([binary_data], type=pa.binary())        # Yield RecordBatch for each image        yield pa.RecordBatch.from_arrays([image_array], schema=schema)def write_to_lance():        schema = pa.schema([        pa.field(\"image\", pa.binary())    ])    reader = pa.RecordBatchReader.from_batches(schema, process_images())    lance.write_dataset(        reader,        \"image_dataset.lance\",        schema,    )def loading_into_pandas():    uri = \"image_dataset.lance\"    ds = lance.dataset(uri)    # Accumulate data from batches into a list    data = []    for batch in ds.to_batches(columns=[\"image\"], batch_size=10):        tbl = batch.to_pandas()        data.append(tbl)    # Concatenate all DataFrames into a single DataFrame    df = pd.concat(data, ignore_index=True)    print(\"Pandas DataFrame is ready\")    print(\"Total Rows: \", df.shape[0])if __name__ == \"__main__\":    start = time.time()    write_to_lance()    loading_into_pandas()    end = time.time()    print(f\"Time(sec): {end - start}\")</code></pre><p>Imagine using Lance-formatted image data to make machine learning and deep learning projects faster. Something big is coming up, stay tuned.</p>",
            "url": "http://localhost:4000/2024/03/29/effortlessly-loading-and-processing-images-with-lance-a-code-walkthrough",
            
            
            
            
            
            "date_published": "2024-03-29T00:00:00+05:30",
            "date_modified": "2024-03-29T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/03/15/embedded-databases",
            "title": "Databases for ML Worflows",
            "summary": "How LanceDB is beating ass of every other Embedded Database",
            "content_text": "In today’s world, when everyone’s curious about trying out generative AI tools and how they work, you’ve probably heard about embedded databases. Most of us tend to think about client-server based setups when databases come to mind. And honestly, that’s somewhat accurate.However, client-server architectures aren’t really built to handle heavy analytical and ML workloads. Essentially, the processing tasks fall into two main categories: OLTP (online transactional processing) and OLAP (online analytical processing). So, when you’re changing your Instagram profile picture or uploading a photo on Facebook, you’re essentially involved in OLTP tasks, which focus on quick and easy processing. On the flip side, when we deal with OLAP, it’s all about handling complex computations such as retrieving queries from extensive datasets, combining tables, and aggregating data for big data purposes. Now, we need something that can handle our large ML workloads effectively and perform optimally across datasets ranging from small to large scales.Columnar Oriented DatastoresHard drives store data in terms of blocks, so whenever an operation is performed, the entire block containing the data is loaded into memory for reading by the OS. Now, Row-oriented databases aim to store whole rows of the database in the same block, whereas columnar databases store column entries in the same block.This implies that when you need to perform column-oriented operations like updating columns, aggregations, or selecting a column entry, column-oriented databases outperform row-oriented ones in terms of speed. However, if you need to add a new data point entry with multiple columns, then row-oriented databases perform better.Now, the point is, there’s something called Apache Arrow, which is a language-agnostic columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs.This is too technical. Let me break it down for you. Machine learning is all about feeding huge amounts of data into complex mathematical models to find patterns and make predictions, right? Now, Apache Arrow turbocharges this process by providing a standardized way to store and work with data that’s optimized for modern hardware like powerful GPUs. So Instead of dealing with clunky row-based formats, Arrow’s columnar layout lets you focus on the specific data features you need, drastically cutting down processing time. And since Arrow keeps data in memory, AKA RAM, rather than on sluggish disk storage like hard drives or SSDs, your models can crunch through datasets at blistering speeds. The end result? You can iterate faster, train better models, and stay ahead of the competition.Still confused, right? I was too. Well, let’s take an example. If you’re building a model to predict housing prices based on factors like square footage, number of bedrooms, location, etc., Arrow’s columnar format would allow you to easily isolate and work with just the columns containing those specific features, ignoring any other irrelevant data columns. This focused, selective data access is more efficient than dealing with row-based formats where you’d have to sort through all the data indiscriminately.Now that’s where the power of Arrow based columnar databases comes into play.Lance Data FormatBuilding on the advantages of Apache Arrow’s columnar, in-memory approach for machine learning, there’s another game-changing data format that takes things to a whole new level – the Lance Data Format.Designed from the ground up with modern ML workflows in mind, Lance is an absolute speed demon when it comes to querying and prepping data for training models. But it’s not just about raw speed – Lance has some seriously impressive versatility under the hood.Unlike formats limited to tabular data, Lance can handle all kinds of data types like images, videos, 3D point clouds, audio, and more. It’s like a Swiss Army knife of data formats for ML. Btw, Don’t just take my word for it because I love LanceDB, instead – benchmarks have shown that Lance can provide random data access involving read and write operation a mind-boggling approximately 1000 times faster than Parquet, another popular columnar format. This blistering speed comes from unique storage memory layout used by Lance.The other important thing LanceDB provides is the usage of Zero-copy versioning, essentially it means that when you create a new version of your data, LanceDB doesn’t have to make an entire copy – it just stores the changes efficiently. This saves a ton of time and storage space compared to traditional versioning methods. And optimized vector operations allow Lance to process data in bulk, taking full advantage of modern hardware like GPUs and vectorized CPUs. It’s all part of Lance’s cloud-native design.In-processBefore understanding what Embedded Systems really do, First, we need to understand what a database management system (DBMS) is in Layman. Now in simple terms a DBMS is a software system that allows you to create, manage, and interact with databases (obviously duhh). I mean It provides a way to store, retrieve, and manipulate data in an organized and more efficient manner.Now, an embedded database is a type of DBMS that is tightly integrated with the application layer. This means that the database that you are working with is not a separate process or service running on its own; instead, it runs within the same process as the application itself.This tight integration makes it easier for the application to interact with the database, as there’s no need for inter-process communication or network communication unlike traditional client-server database systems where the database is a  separate process or a serviceThis type of thing is called “in-process”. Remember this for the rest of your life. It might be the most important thing to remember when judging the other embedded databases out there.In simple terms, the application can directly access and manipulate the database without going through additional layers or protocols. CUTTING THE BS.On-disk storageRemember I said earlier that Embedded systems have this ability to store the data in-memory and work at blistering speeds. While embedded databases can store data in memory, they also have this capability to store larger datasets on disk. This allows them to scale to large amounts of data (terabytes or billion dataset points, think of embeddings of billion tokens) while still providing relatively low query latencies and response times. So it’s like the best of the both worlds.ServerlessOk new term! Sometimes the terms “Embedded” and “Serverless” are sometimes used interchangeably in the database community, but they actually refer to different concepts. “Embedded” refers to the database being tightly integrated with the application as we seen earlier in case of Embedded databases while Serverless refers to the separation of storage and compute resources, and it’s often used in the context of microservices architectures.To make it more concise, think of the serverless database as it’s composed of two different containers, the storage layer (where the data is stored) and the compute layer (where the data is processed). Now this separation allows the compute resources to be dynamically allocated and scaled up or down based on the workload.And btw, when we are talking about the Serverless model, it is often associated with the cloud based services, where you don’t have to manage the underlying infra..ScalabilityOk so you decided to work on the simple RAG system and wanted to test it out or just maybe play with it and came up with the various vector databases, you did your experiment,  you are happy and done. Now when you got serious and came up with something and wanted to scale your RAG system for let’s say 1 billion embeddings, you open up your earlier setup, ingested more data, created more embeddings and when the time came, your traditional embedding database gave you nightmares in terms of the latency as well as stability.Now, Think of an open-source embedding database designed to seamlessly handle a variety of setups and effortlessly scales up to billions of vectors locally., scales up to billions of embeddings, fetch the relevant embeddings with amazing searching capabilities and data never leaves your local machine, feels too good to be true right?! Well there is LanceDB again. I mean from the moment you dirty your hands for your next RAG system all upto the time you put something as big as production, LanceDB scales amazingly well…MultimodalityThe current embedded databases should not only support textual data but also be compatible with various file formats. It’s no longer just about text ingestion. Some time ago, I developed a GTA-5 multimodal RAG application that displays the GTA5 landscape upon entering a query. I highly recommend giving it a read to understand why the Multimodal RAG system is the new hotshot and why companies are eager to integrate it into their core. Honestly, I haven’t come across any embedded vector database other than LanceDB that can effortlessly ingest any kind of file format.By the way, multimodality does make sense because LanceDB is built on top of the Lance format. As mentioned earlier, it supports diverse data types, including images, audio, and text, making it incredibly flexible for applications with various data formats.Searching and IntegrationsOk, so we stored the Embeddings, we scaled our RAG too, now for a given query, we want to find the relevant embeddings, and that’s where LanceDB shines. Now Searching in LanceDB is as easy as it could be, you can just query your data in a number of ways - via SQL, full-text search , and vector search. As it supports the Hybrid search too which is one of the biggest favorable for the amazing search capabilities in LanceDB.But it’s not just about searching guys, it integrates well enough with native Python, JavaScript/TypeScript, Pandas, Pydantic, that means you can easily integrate it with your favorite programming languages, in addition to that, it has direct integrations with cloud storage providers like AWS S3 and Azure Blob Storage. This means that we can directly query data stored on the cloud, without any added ETL steps.Woah, I do love LanceDBMore or less, we looked at these things right :  Columnar Oriented Databases  Lance Format  In-process  On-disk storage  Serverless  Embedded systems.  Scalability  Multimodality  Searching and IntegrationsWell all of them are bundled together with no cost upfront, ready to serve in one installation click and voila baby, you have your new best friend, maybe more than that, who knows? So what are you waiting for? Here is the reference, see you soon.",
            "content_html": "<p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/embedded_databases/production_fucked.png?raw=true\" alt=\"production_fucked\" /></p><p>In today’s world, when everyone’s curious about trying out generative AI tools and how they work, you’ve probably heard about embedded databases. Most of us tend to think about client-server based setups when databases come to mind. And honestly, that’s somewhat accurate.</p><p>However, client-server architectures aren’t really built to handle heavy analytical and ML workloads. Essentially, the processing tasks fall into two main categories: OLTP (online transactional processing) and OLAP (online analytical processing). So, when you’re changing your Instagram profile picture or uploading a photo on Facebook, you’re essentially involved in OLTP tasks, which focus on quick and easy processing. On the flip side, when we deal with OLAP, it’s all about handling complex computations such as retrieving queries from extensive datasets, combining tables, and aggregating data for big data purposes. Now, we need something that can handle our large ML workloads effectively and perform optimally across datasets ranging from small to large scales.</p><h3 id=\"columnar-oriented-datastores\">Columnar Oriented Datastores</h3><p>Hard drives store data in terms of blocks, so whenever an operation is performed, the entire block containing the data is loaded into memory for reading by the OS. Now, Row-oriented databases aim to store whole rows of the database in the same block, whereas columnar databases store column entries in the same block.</p><p>This implies that when you need to perform column-oriented operations like updating columns, aggregations, or selecting a column entry, column-oriented databases outperform row-oriented ones in terms of speed. However, if you need to add a new data point entry with multiple columns, then row-oriented databases perform better.</p><p>Now, the point is, there’s something called Apache Arrow, which is a language-agnostic columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs.</p><p>This is too technical. Let me break it down for you. Machine learning is all about feeding huge amounts of data into complex mathematical models to find patterns and make predictions, right? Now, Apache Arrow turbocharges this process by providing a standardized way to store and work with data that’s <a href=\"https://developer.nvidia.com/blog/accelerating-apache-spark-3-0-with-gpus-and-rapids/\">optimized</a> for modern hardware like powerful GPUs. So Instead of dealing with clunky row-based formats, Arrow’s columnar layout lets you focus on the specific data features you need, drastically cutting down processing time. And since Arrow keeps data in memory, AKA RAM, rather than on sluggish disk storage like hard drives or SSDs, your models can crunch through datasets at blistering speeds. The end result? You can iterate faster, train better models, and stay ahead of the competition.</p><p>Still confused, right? I was too. Well, let’s take an example. If you’re building a model to predict housing prices based on factors like square footage, number of bedrooms, location, etc., Arrow’s columnar format would allow you to easily isolate and work with just the columns containing those specific features, ignoring any other irrelevant data columns. This focused, selective data access is more efficient than dealing with row-based formats where you’d have to sort through all the data indiscriminately.</p><p>Now that’s where the power of Arrow based columnar databases comes into play.</p><h3 id=\"lance-data-format\">Lance Data Format</h3><p>Building on the advantages of Apache Arrow’s columnar, in-memory approach for machine learning, there’s another game-changing data format that takes things to a whole new level – the Lance Data Format.</p><p>Designed from the ground up with modern ML workflows in mind, Lance is an absolute speed demon when it comes to querying and prepping data for training models. But it’s not just about raw speed – Lance has some seriously impressive versatility under the hood.</p><p>Unlike formats limited to tabular data, Lance can handle all kinds of data types like images, videos, 3D point clouds, audio, and more. It’s like a Swiss Army knife of data formats for ML. Btw, Don’t just take my word for it because I love LanceDB, instead – <a href=\"https://blog.lancedb.com/announcing-lancedb-5cb0deaa46ee-2/\">benchmarks</a> have shown that Lance can provide random data access involving read and write operation a mind-boggling approximately 1000 times faster than Parquet, another popular columnar format. This blistering speed comes from unique storage memory layout used by Lance.</p><p>The other important thing LanceDB provides is the usage of Zero-copy versioning, essentially it means that when you create a new version of your data, LanceDB doesn’t have to make an entire copy – it just stores the changes efficiently. This saves a ton of time and storage space compared to traditional versioning methods. And optimized vector operations allow Lance to process data in bulk, taking full advantage of modern hardware like GPUs and vectorized CPUs. It’s all part of Lance’s cloud-native design.</p><h3 id=\"in-process\">In-process</h3><p>Before understanding what Embedded Systems really do, First, we need to understand what a database management system (DBMS) is in Layman. Now in simple terms a DBMS is a software system that allows you to create, manage, and interact with databases (obviously duhh). I mean It provides a way to store, retrieve, and manipulate data in an organized and more efficient manner.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/embedded_databases/in_process_setting.png?raw=true\" alt=\"inprocess_setting\" /></p><p>Now, an embedded database is a type of DBMS that is tightly integrated with the application layer. This means that the database that you are working with is not a separate process or service running on its own; instead, it runs within the same process as the application itself.</p><p>This tight integration makes it easier for the application to interact with the database, as there’s no need for inter-process communication or network communication unlike traditional client-server database systems where the database is a  separate process or a service</p><p>This type of thing is called “in-process”. Remember this for the rest of your life. It might be the most important thing to remember when judging the other embedded databases out there.</p><p>In simple terms, the application can directly access and manipulate the database without going through additional layers or protocols. CUTTING THE BS.</p><h3 id=\"on-disk-storage\"><strong>On-disk storage</strong></h3><p>Remember I said earlier that Embedded systems have this ability to store the data in-memory and work at blistering speeds. While embedded databases can store data in memory, they also have this capability to store larger datasets on disk. This allows them to scale to large amounts of data (terabytes or billion dataset points, think of embeddings of billion tokens) while still providing relatively low query latencies and response times. So it’s like the best of the both worlds.</p><h3 id=\"serverless\">Serverless</h3><p>Ok new term! Sometimes the terms “Embedded” and “Serverless” are sometimes used interchangeably in the database community, but they actually refer to different concepts. “Embedded” refers to the database being tightly integrated with the application as we seen earlier in case of Embedded databases while Serverless refers to the separation of storage and compute resources, and it’s often used in the context of microservices architectures.</p><p>To make it more concise, think of the serverless database as it’s composed of two different containers, the storage layer (where the data is stored) and the compute layer (where the data is processed). Now this separation allows the compute resources to be dynamically allocated and scaled up or down based on the workload.</p><p>And btw, when we are talking about the Serverless model, it is often associated with the cloud based services, where you don’t have to manage the underlying infra..</p><h3 id=\"scalability\">Scalability</h3><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/embedded_databases/lancedb_scaled.png?raw=true\" alt=\"lancedb_scaled\" /></p><p>Ok so you decided to work on the simple RAG system and wanted to test it out or just maybe play with it and came up with the various vector databases, you did your experiment,  you are happy and done. Now when you got serious and came up with something and wanted to scale your RAG system for let’s say 1 billion embeddings, you open up your earlier setup, ingested more data, created more embeddings and when the time came, your traditional embedding database gave you nightmares in terms of the latency as well as stability.</p><p>Now, Think of an open-source embedding database designed to seamlessly handle a variety of setups and effortlessly scales up to billions of vectors locally., scales up to billions of embeddings, fetch the relevant embeddings with amazing searching capabilities and data never leaves your local machine, feels too good to be true right?! Well there is LanceDB again. I mean from the moment you dirty your hands for your next RAG system all upto the time you put something as big as production, LanceDB scales amazingly well…</p><h3 id=\"multimodality\">Multimodality</h3><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/embedded_databases/multimodality_in_lancedb.png?raw=true\" alt=\"multimodality\" /></p><p>The current embedded databases should not only support textual data but also be compatible with various file formats. It’s no longer just about text ingestion. Some time ago, I developed a GTA-5 multimodal RAG application that displays the GTA5 landscape upon entering a query. I highly recommend giving it a <a href=\"https://vipul-maheshwari.github.io/2024/03/03/multimodal-rag-application\">read</a> to understand why the Multimodal RAG system is the new hotshot and why companies are eager to integrate it into their core. Honestly, I haven’t come across any embedded vector database other than LanceDB that can effortlessly ingest any kind of file format.</p><p>By the way, multimodality does make sense because LanceDB is built on top of the Lance format. As mentioned earlier, it supports diverse data types, including images, audio, and text, making it incredibly flexible for applications with various data formats.</p><h3 id=\"searching-and-integrations\">Searching and Integrations</h3><p>Ok, so we stored the Embeddings, we scaled our RAG too, now for a given query, we want to find the relevant embeddings, and that’s where LanceDB shines. Now Searching in LanceDB is as easy as it could be, you can just query your data in a number of ways - via SQL, full-text search , and vector search. As it supports the Hybrid search too which is one of the biggest favorable for the amazing search capabilities in LanceDB.</p><p>But it’s not just about searching guys, it integrates well enough with native Python, JavaScript/TypeScript, Pandas, Pydantic, that means you can easily integrate it with your favorite programming languages, in addition to that, it has direct integrations with cloud storage providers like AWS S3 and Azure Blob Storage. This means that we can directly query data stored on the cloud, without any added ETL steps.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/embedded_databases/grandma_knows.png?raw=true\" alt=\"grandma_knows\" /></p><p>Woah, I do love LanceDB</p><h3 id=\"more-or-less-we-looked-at-these-things-right-\">More or less, we looked at these things right :</h3><ol>  <li>Columnar Oriented Databases</li>  <li>Lance Format</li>  <li>In-process</li>  <li>On-disk storage</li>  <li>Serverless</li>  <li>Embedded systems.</li>  <li>Scalability</li>  <li>Multimodality</li>  <li>Searching and Integrations</li></ol><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/embedded_databases/gen_ai_applications.png?raw=true\" alt=\"gen_ai_application\" /></p><p>Well all of them are bundled together with no cost upfront, ready to serve in one installation click and voila baby, you have your new best friend, maybe more than that, who knows? So what are you waiting for? Here is the <a href=\"https://lancedb.com/\">reference</a>, see you soon.</p>",
            "url": "http://localhost:4000/2024/03/15/embedded-databases",
            
            
            
            "tags": ["LLM","Embedded Databases","LanceDB"],
            
            "date_published": "2024-03-15T00:00:00+05:30",
            "date_modified": "2024-03-15T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/03/03/multimodal-rag-application",
            "title": "Create a Multimodal RAG application",
            "summary": "Multimodal RAG applications using lanceDB",
            "content_text": "Artificial Intelligence (AI) has been actively working with text for quite some time, but the world isn’t solely centered around words. If you take a moment to look around, you’ll find a mix of text, images, videos, audios, and their combinations.Today we are going to work on Multimodality which is basically a concept that essentially empowers AI models with the capacity to perceive, listen, and comprehend data in diverse formats together with the text. Pretty much like how we do!In an ideal situation, we should be able to mix different types of data together and show them to a generative AI model at the same time and iterate on it. It could be as simple as telling the AI model, “Hey, a few days ago, I sent you a picture of a brown, short dog. Can you find that picture for me?” and the model should then give us the details of that picture. Basically, we want the AI to understand things more like how we humans do,  becoming really good at handling and responding to all kinds of information.But the challenge here is to make a computer understand one data format with its related reference, and that could be a mix of text, audio, thermal imagery, and videos. Now to make this happen, we use something called Embeddings. It’s really a numeric vector which contains a bunch of numbers written together that might not mean much to us but are understood by machines very well.Cat is equal to CatLet’s think of the text components for now, so we are currently aiming that our model should learn that words like “Dog” and “Cat” are closely linked to the word “Pet.” Now this understanding is easily achievable by using an embedding model which will convert these text words into their respective embeddings first and then the model is trained to follow a straightforward logic: if words are related, they are close together in the vector space, if not, they would be separated by the adequate distance.But to help a model recognize that an image of a “Cat” and the word “Cat” are similar, we rely on Multimodal Embeddings. To simplify things a bit, imagine there is a magic box which is capable of handling various inputs – images, audios, text, and more.Now, when we feed the box with an image of a “Cat” with the text “Cat,” it performs its magic and produces two numeric vectors. When these two vectors were given to a machine, it made machines think, “Hmm, based on these numeric values, it seems like both are connected to “Cat”. So that’s exactly what we were aiming for! Our goal was to help machines to recognize the close connection between an image of a “Cat” and the text “Cat”. However, to validate this concept, when we plot those two numeric vectors in a vector space, it turns out they are very close to each other. This outcome exactly mirrors what we observed earlier with the proximity of the two text words “Cat” and “Dog” in the vector space.Ladies and gentlemen, that’s the essence of Multimodality. 👏So we made our model to comprehend the association between “Cat” images and the word “Cat.” Well this is it, I mean if you are able to do this, you would have ingested the audio, images, videos as well as the word “Cat” and the model will understand how the cat is being portrayed across all kinds of file format..RAG is here..Well if you don’t know what RAG means, I would highly advise you to read this article here which I wrote some days back and loved by tons of people, not exaggerating it but yeah, it’s good to get the balls rolling..So there are impressive models like DALLE-2 that provide text-to-image functionality. Essentially, you input text, and the model generates relevant images for you. But can we create a system similar to Multimodal RAG, where the model produces output images based on our own data? Alright, so the goal for today is to create an AI model that when asked something like, “How many girls were there in my party?” 💀 not only provides textual information but also includes a relevant image related to it. Think of it as an extension of a simple RAG system, but now incorporating images.Before we dive in, remember that Multimodality isn’t limited to just text-to-image or image-to-text as it encompasses the freedom to input and output any type of data. However, for now, let’s concentrate on the interaction from image to text exclusively.Contrastive learningNow the question is, What exactly was that box doing? The magic it performs is known as Contrastive Learning. While the term might sound complex, it’s not that tricky. To simplify, consider a dataset with images, along with a caption describing what the image represents.Alright, now what happens is: we give our text-image model with these Positive and Negative samples, where each sample consists of an image and a descriptive text. Positive samples are those where the image and text are correctly aligned – for instance, a picture of a cat matched with the text “this is an image of a cat.” Conversely, negative samples involve a mismatch, like presenting an image of a dog alongside the text “this is an image of a cat.”Now we train our text-image model to recognize that positive samples offer accurate interpretations, while negative samples are misleading and should be disregarded during training. In formal terms this technique is called CLIP (Contrastive Language-Image Pre-training) introduced by OpenAI where authors trained an image-text model on something around 400 million image caption pairs taken from the internet and everytime model makes a mistake, the contrastive loss function increases and penalize it to make sure the model trains well. The same kind of principles are applied to the other modality combinations as well, so the voice of cat with the word cat is a positive sample for speech-text model, a video of cat with the descriptive text “this is a cat” is a positive sample for video-text model.Show timeWell you don’t have to build that box from scratch because folks have already done it for us. There’s a Multimodal embedding model, like the “ViT-L/14” from OpenAI. This model can handle various data types, including text, images, videos, audios, and even thermal and gyroscope data. Now, onto the next question: how do we store those embeddings?For that we’ll need a vector database that can efficiently fetch, query, and retrieve relevant embeddings for us,  ideally one that supports multimodal data and doesn’t burn a hole in our wallets. That’s where LanceDB comes into play.Vector databaseWhen we talk about the vector database, there are ton of options available in the current market, but there is something about the LanceDB which makes it stands out as an optimal choice for a vector database, As far as I have used it, it address the limitations of traditional embedded databases in handling AI/ML workloads. When I say traditional, it typically means those database management tools which are not aligned with the usage of heavy computation that comes with the ML infra.TLDR; LanceDB operates on a serverless architecture, meaning storage and compute are separated into two distinct units. This design makes it exceptionally fast for RAG use cases, ensuring fast fetching and retrieval. Additionally, it has some notable advantages – being open source, utilizing its Lance columnar data format built on top of Apache Arrow for high efficiency, persistent storage capabilities, and incorporating its own Disk Approximate Nearest Neighbor search. All these factors collectively make LanceDB an ideal solution for accessing and working with multimodal data. I love you LanceDB ❤️.Data timeTo add some excitement, I’ve crafted a GTA-V Image Captioning dataset, featuring thousands of images, each paired with a descriptive text illustrating the image’s content. Now, when we train our magic box, the expectation is clear – if I ask that box to provide me an image of “road with a stop sign,” it should deliver a GTA-V image of a road with a stop sign on it. Otherwise, what’s the point, right?FAQ  We will be using “ViT-L/14” to convert our multimodal data into its respective embeddings.  LanceDB as our vector database to store the relevant embeddings.  GTA-V Image Captioning dataset for our magic box.Environment SetupI am using a MacBook Air M1, and it’s important to note that some kinds of dependencies and configurations may vary depending on the type of system that you are running, so it’s important to take that into account.Here are the steps to install the relevant dependencies# Create a virtual environmentpython3 -m venv env# Activate the virtual environmentsource env/bin/activate# Upgrade pip in the virtual environmentpip install --upgrade pip# Install required dependenciespip3 install lancedb clip torch datasets pillow pip3 install git+https://github.com/openai/CLIP.gitAnd don’t forget to get your access token from the hugging face to download the data.Downloading the DataDataset can easily be fetched using the datasets library.import clipimport torchimport osfrom datasets import load_datasetds = load_dataset(\"vipulmaheshwari/GTA-Image-Captioning-Dataset\")device = torch.device(\"mps\")model, preprocess = clip.load(\"ViT-L-14\", device=device)Downloading the dataset may require some time, so please take a moment to relax while this process completes. Once the download is finished, you can visualize some sample points like this:from textwrap import wrapimport matplotlib.pyplot as pltimport numpy as npdef plot_images(images, captions):    plt.figure(figsize=(15, 7))    for i in range(len(images)):        ax = plt.subplot(1, len(images), i + 1)        caption = captions[i]        caption = \"\\n\".join(wrap(caption, 12))        plt.title(caption)        plt.imshow(images[i])        plt.axis(\"off\")# Assuming ds is a dictionary with \"train\" key containing a list of samplessample_dataset = ds[\"train\"]random_indices = np.random.choice(len(sample_dataset), size=2, replace=False)random_indices = [index.item() for index in random_indices]# Get the random images and their captionsrandom_images = [np.array(sample_dataset[index][\"image\"]) for index in random_indices]random_captions = [sample_dataset[index][\"text\"] for index in random_indices]# Plot the random images with their captionsplot_images(random_images, random_captions)# Show the plotplt.show()Storing the EmbeddingsThe dataset consists of two key features: the image and its corresponding descriptive text. Initially, our task is to create a LanceDB table to store the embeddings. This process is straightforward – all you need to do is define the relevant schema. In our case, the columns include “vector” for storing the multimodal embeddings, a “text” column for the descriptive text, and a “label” column for the corresponding IDs.import pyarrow as paimport lancedbimport tqdmdb = lancedb.connect('./data/tables')schema = pa.schema(  [      pa.field(\"vector\", pa.list_(pa.float32(), 512)),      pa.field(\"text\", pa.string()),      pa.field(\"id\", pa.int32())  ])tbl = db.create_table(\"gta_data\", schema=schema, mode=\"overwrite\")Executing this will generate a table with the specified schema, and it’s ready to store the embeddings along with the relevant columns. It’s as straightforward as that – almost too easy!Encode the ImagesNow, we’ll simply take the images from the dataset, feed them into an encoding function that leverages our Multimodal Embedding model, and generate the corresponding embeddings. These embeddings will then be stored in the database.def embed_image(img):    processed_image = preprocess(img)    unsqueezed_image = processed_image.unsqueeze(0).to(device)    embeddings = model.encode_image(unsqueezed_image)        # Detach, move to CPU, convert to numpy array, and extract the first element as a list    result = embeddings.detach().cpu().numpy()[0].tolist()    return resultSo our embed_image function takes an input image, prepocesses it through our CLIP model preprocessor, encode the preprocessed image and returns a list representing the embeddings of that image. This returned embedding serves as a concise numerical representation, capturing all the key features and patterns within the image for downstream tasks or analysis. The next thing is to call this function for all the images and store the relevant embeddings in the database.data = []for i in range(len(ds[\"train\"])):    img = ds[\"train\"][i]['image']    text = ds[\"train\"][i]['text']        # Encode the image    encoded_img = embed_image(img)    data.append({\"vector\": encoded_img, \"text\": text, \"id\" : i})Here, we’re just taking a list, adding the numeric embeddings, reference text and the current index id to it. All that’s left is to include this list in our LanceDB table. And voila, our datalake for the embeddings is set up and good to go!tbl.add(data)tbl.to_pandas()Up until now, we’ve efficiently converted the images into their respective multimodal embeddings and stored them in the LanceDB table. Now the LanceDB tables offer a convenient feature: if there’s a need to add or remove images, it’s remarkably straightforward. Just encode the new image and add it, following to the same steps we followed for the previous images.Query searchOur next move is to embed our text query using the same multimodal embedding model we used for our images. Remember that “box” I mentioned earlier? Essentially, we want this box to create embeddings for both our images and our texts which ensures that the representation of different types of data happens in the same way. Following this, we just need to initiate a search to find the nearest image embeddings that matches our text query.def embed_txt(txt):    tokenized_text = clip.tokenize([txt]).to(device)    embeddings = model.encode_text(tokenized_text)        # Detach, move to CPU, convert to numpy array, and extract the first element as a list    result = embeddings.detach().cpu().numpy()[0].tolist()    return resultres = tbl.search(embed_txt(\"a road with a stop\")).limit(3).to_pandas()res0 | [0.064575195, .. ] | there is a stop sign...| 569 |\t131.9957281 | [-0.07989502, .. ] | there is a bus that... | 423 | 135.0478522 | [0.06756592, .. ]  | amazing view of a ...\t| 30  | 135.309937Let’s slow down a bit and understand what just happened. Putting simply, the code snippet executes a search algorithm in its core to pinpoint the most relevant image embedding that aligns with our text query. The resulting output, as showcased above, gives us the embeddings which closely resembles our text query.  In the result, the second column presents the embedding vector, while the third column contains the description of the image that closely matches our text query. Essentially, we’ve determined which image closely corresponds to our text query by examining the embeddings of both our text query and the image.It’s similar to saying, If these numbers represent the word “Cat”, I spot an image with a similar set of numbers, so most likely it’s a match for an image of a “Cat”. 😺If you are looking for the explanation of how the search happens, I will write a detailed explanation in the coming write ups because it’s so exciting to look under the hood and see how the searching happens. Essentially there is something called Approximate Nearest Neighbors (ANN) which is a technique used to efficiently find the closest points in high-dimensional spaces. ANN is extensively used in data mining, machine learning, computer vision and NLP use cases. So when we passed our embedded text query to the searching algorithm and asked it to give us the closest sample point in the vector space, it used a type of ANN algorithm to get it for us. Specifically LanceDB utilizes DANN (Deep Approximate Nearest Neighbor) for searching the relevant embeddings within its ecosystem..In our results, we have five columns. The first is the index number, the second is the embedding vector, the third is the description of the image matching our text query, and the fourth is the label of the image. However, let’s focus on the last column – Distance. When I mentioned the ANN algorithm, it simply draws a line between the current data point (in our case, the embedding of our text query) and identifies which data point (image embedding) is closest to it. If you observe that the other data points in the results have a greater distance compared to the top one, it indicates they are a bit further away or more unrelated to our query. Just to make it clear, the calculation of distance is a part of the algorithm itself.D-DAYNow that we have all the necessary information, displaying the most relevant image for our query is straightforward. Simply take the relevant label of the top-matched embedding vector and showcase the corresponding image.data_id = int(res['id'][0])display(ds[\"train\"][data_id]['image'])print(ds[\"train\"][data_id]['text'])there is a truck driving down a street with a stop signWhat’s next?To make things more interesting, I’m currently working on creating an extensive GTA-V captioning dataset. This dataset will include a larger number of images paired with their respective reference text, providing us with a richer set of queries to explore and experiment with.. Nevertheless, there’s always room for refining the model. We can explore creating a customized CLIP model, adjusting various parameters. Increasing the number of training epochs may afford the model more time to grasp the relevance between embeddings. Additionally, there’s an impressive multimodal embedding model developed by the Meta known as ImageBind. We can consider trying ImageBind as an alternative to our current multimodal embedding model and compare the outcomes. With numerous options available, the fundamental concept behind the Multimodal RAG workflow remains largely consistent.Here’s how everything comes together in one frame and this is the Collab for your reference",
            "content_html": "<p><em>Artificial Intelligence (AI) has been actively working with text for quite some time, but the world isn’t solely centered around words. If you take a moment to look around, you’ll find a mix of text, images, videos, audios, and their combinations.</em></p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/Renevant%20Cheetah-66.jpg?raw=true\" alt=\"boomer_ai\" /></p><p>Today we are going to work on Multimodality which is basically a concept that essentially empowers AI models with the capacity to perceive, listen, and comprehend data in diverse formats together with the text. Pretty much like how we do!</p><p>In an ideal situation, we should be able to mix different types of data together and show them to a generative AI model at the same time and iterate on it. It could be as simple as telling the AI model, “Hey, a few days ago, I sent you a picture of a brown, short dog. Can you find that picture for me?” and the model should then give us the details of that picture. Basically, we want the AI to understand things more like how we humans do,  becoming really good at handling and responding to all kinds of information.</p><p>But the challenge here is to make a computer understand one data format with its related reference, and that could be a mix of text, audio, thermal imagery, and videos. Now to make this happen, we use something called Embeddings. It’s really a numeric vector which contains a bunch of numbers written together that might not mean much to us but are understood by machines very well.</p><h3 id=\"cat-is-equal-to-cat\">Cat is equal to Cat</h3><p>Let’s think of the text components for now, so we are currently aiming that our model should learn that words like “Dog” and “Cat” are closely linked to the word “Pet.” Now this understanding is easily achievable by using an embedding model which will convert these text words into their respective embeddings first and then the model is trained to follow a straightforward logic: if words are related, they are close together in the vector space, if not, they would be separated by the adequate distance.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/embeddings.png?raw=true\" alt=\"embeddings\" /></p><p>But to help a model recognize that an image of a “Cat” and the word “Cat” are similar, we rely on Multimodal Embeddings. To simplify things a bit, imagine there is a magic box which is capable of handling various inputs – images, audios, text, and more.</p><p>Now, when we feed the box with an image of a “Cat” with the text “Cat,” it performs its magic and produces two numeric vectors. When these two vectors were given to a machine, it made machines think, “Hmm, based on these numeric values, it seems like both are connected to “Cat”. So that’s exactly what we were aiming for! Our goal was to help machines to recognize the close connection between an image of a “Cat” and the text “Cat”. However, to validate this concept, when we plot those two numeric vectors in a vector space, it turns out they are very close to each other. This outcome exactly mirrors what we observed earlier with the proximity of the two text words “Cat” and “Dog” in the vector space.</p><h2 id=\"ladies-and-gentlemen-thats-the-essence-of-multimodality-\">Ladies and gentlemen, that’s the essence of Multimodality. 👏</h2><p>So we made our model to comprehend the association between “Cat” images and the word “Cat.” Well this is it, I mean if you are able to do this, you would have ingested the audio, images, videos as well as the word “Cat” and the model will understand how the cat is being portrayed across all kinds of file format..</p><h3 id=\"rag-is-here\">RAG is here..</h3><p>Well if you don’t know what RAG means, I would highly advise you to read this article <a href=\"https://vipul-maheshwari.github.io/2024/02/14/rag-application-with-langchain\">here</a> which I wrote some days back and loved by tons of people, not exaggerating it but yeah, it’s good to get the balls rolling..</p><p>So there are impressive models like DALLE-2 that provide text-to-image functionality. Essentially, you input text, and the model generates relevant images for you. But can we create a system similar to Multimodal RAG, where the model produces output images based on our own data? Alright, so the goal for today is to create an AI model that when asked something like, “How many girls were there in my party?” 💀 not only provides textual information but also includes a relevant image related to it. Think of it as an extension of a simple RAG system, but now incorporating images.</p><p>Before we dive in, remember that Multimodality isn’t limited to just text-to-image or image-to-text as it encompasses the freedom to input and output any type of data. However, for now, let’s concentrate on the interaction from image to text exclusively.</p><h3 id=\"contrastive-learning\">Contrastive learning</h3><p>Now the question is, What exactly was that box doing? The magic it performs is known as Contrastive Learning. While the term might sound complex, it’s not that tricky. To simplify, consider a dataset with images, along with a caption describing what the image represents.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/clipmodel.png?raw=true\" alt=\"clipmodel\" /></p><p>Alright, now what happens is: we give our text-image model with these Positive and Negative samples, where each sample consists of an image and a descriptive text. Positive samples are those where the image and text are correctly aligned – for instance, a picture of a cat matched with the text “this is an image of a cat.” Conversely, negative samples involve a mismatch, like presenting an image of a dog alongside the text “this is an image of a cat.”</p><p>Now we train our text-image model to recognize that positive samples offer accurate interpretations, while negative samples are misleading and should be disregarded during training. In formal terms this technique is called <a href=\"https://openai.com/research/clip\">CLIP</a> (Contrastive Language-Image Pre-training) introduced by OpenAI where authors trained an image-text model on something around 400 million image caption pairs taken from the internet and everytime model makes a mistake, the contrastive loss function increases and penalize it to make sure the model trains well. The same kind of principles are applied to the other modality combinations as well, so the voice of cat with the word cat is a positive sample for speech-text model, a video of cat with the descriptive text “this is a cat” is a positive sample for video-text model.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/easy.png?raw=true\" alt=\"easy\" /></p><h3 id=\"show-time\">Show time</h3><p>Well you don’t have to build that box from scratch because folks have already done it for us. There’s a Multimodal embedding model, like the “ViT-L/14” from OpenAI. This model can handle various data types, including text, images, videos, audios, and even thermal and gyroscope data. Now, onto the next question: how do we store those embeddings?</p><p>For that we’ll need a vector database that can efficiently fetch, query, and retrieve relevant embeddings for us,  ideally one that supports multimodal data and doesn’t burn a hole in our wallets. That’s where LanceDB comes into play.</p><h3 id=\"vector-database\">Vector database</h3><p>When we talk about the vector database, there are ton of options available in the current market, but there is something about the LanceDB which makes it stands out as an optimal choice for a vector database, As far as I have used it, it address the limitations of traditional embedded databases in handling AI/ML workloads. When I say traditional, it typically means those database management tools which are not aligned with the usage of heavy computation that comes with the ML infra.</p><p>TLDR; LanceDB operates on a serverless architecture, meaning storage and compute are separated into two distinct units. This design makes it exceptionally fast for RAG use cases, ensuring fast fetching and retrieval. Additionally, it has some notable advantages – being open source, utilizing its Lance columnar data format built on top of Apache Arrow for high efficiency, persistent storage capabilities, and incorporating its own Disk Approximate Nearest Neighbor search. All these factors collectively make LanceDB an ideal solution for accessing and working with multimodal data. I love you LanceDB ❤️.</p><h3 id=\"data-time\">Data time</h3><p>To add some excitement, I’ve crafted a GTA-V Image Captioning dataset, featuring thousands of images, each paired with a descriptive text illustrating the image’s content. Now, when we train our magic box, the expectation is clear – if I ask that box to provide me an image of “road with a stop sign,” it should deliver a GTA-V image of a road with a stop sign on it. Otherwise, what’s the point, right?</p><h3 id=\"faq\">FAQ</h3><ol>  <li>We will be using “ViT-L/14” to convert our multimodal data into its respective embeddings.</li>  <li>LanceDB as our vector database to store the relevant embeddings.</li>  <li>GTA-V Image Captioning dataset for our magic box.</li></ol><h3 id=\"environment-setup\">Environment Setup</h3><p>I am using a MacBook Air M1, and it’s important to note that some kinds of dependencies and configurations may vary depending on the type of system that you are running, so it’s important to take that into account.</p><p>Here are the steps to install the relevant dependencies</p><pre><code class=\"language-python\"># Create a virtual environmentpython3 -m venv env# Activate the virtual environmentsource env/bin/activate# Upgrade pip in the virtual environmentpip install --upgrade pip# Install required dependenciespip3 install lancedb clip torch datasets pillow pip3 install git+https://github.com/openai/CLIP.git</code></pre><p>And don’t forget to get your access token from the hugging face to download the data.</p><h3 id=\"downloading-the-data\">Downloading the Data</h3><p>Dataset can easily be fetched using the datasets library.</p><pre><code class=\"language-python\">import clipimport torchimport osfrom datasets import load_datasetds = load_dataset(\"vipulmaheshwari/GTA-Image-Captioning-Dataset\")device = torch.device(\"mps\")model, preprocess = clip.load(\"ViT-L-14\", device=device)</code></pre><p>Downloading the dataset may require some time, so please take a moment to relax while this process completes. Once the download is finished, you can visualize some sample points like this:</p><pre><code class=\"language-python\">from textwrap import wrapimport matplotlib.pyplot as pltimport numpy as npdef plot_images(images, captions):    plt.figure(figsize=(15, 7))    for i in range(len(images)):        ax = plt.subplot(1, len(images), i + 1)        caption = captions[i]        caption = \"\\n\".join(wrap(caption, 12))        plt.title(caption)        plt.imshow(images[i])        plt.axis(\"off\")# Assuming ds is a dictionary with \"train\" key containing a list of samplessample_dataset = ds[\"train\"]random_indices = np.random.choice(len(sample_dataset), size=2, replace=False)random_indices = [index.item() for index in random_indices]# Get the random images and their captionsrandom_images = [np.array(sample_dataset[index][\"image\"]) for index in random_indices]random_captions = [sample_dataset[index][\"text\"] for index in random_indices]# Plot the random images with their captionsplot_images(random_images, random_captions)# Show the plotplt.show()</code></pre><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/output3.png?raw=true\" alt=\"output3\" /></p><h3 id=\"storing-the-embeddings\">Storing the Embeddings</h3><p>The dataset consists of two key features: the image and its corresponding descriptive text. Initially, our task is to create a LanceDB table to store the embeddings. This process is straightforward – all you need to do is define the relevant schema. In our case, the columns include “vector” for storing the multimodal embeddings, a “text” column for the descriptive text, and a “label” column for the corresponding IDs.</p><pre><code class=\"language-python\">import pyarrow as paimport lancedbimport tqdmdb = lancedb.connect('./data/tables')schema = pa.schema(  [      pa.field(\"vector\", pa.list_(pa.float32(), 512)),      pa.field(\"text\", pa.string()),      pa.field(\"id\", pa.int32())  ])tbl = db.create_table(\"gta_data\", schema=schema, mode=\"overwrite\")</code></pre><p>Executing this will generate a table with the specified schema, and it’s ready to store the embeddings along with the relevant columns. It’s as straightforward as that – almost too easy!</p><h3 id=\"encode-the-images\">Encode the Images</h3><p>Now, we’ll simply take the images from the dataset, feed them into an encoding function that leverages our Multimodal Embedding model, and generate the corresponding embeddings. These embeddings will then be stored in the database.</p><pre><code class=\"language-python\">def embed_image(img):    processed_image = preprocess(img)    unsqueezed_image = processed_image.unsqueeze(0).to(device)    embeddings = model.encode_image(unsqueezed_image)        # Detach, move to CPU, convert to numpy array, and extract the first element as a list    result = embeddings.detach().cpu().numpy()[0].tolist()    return result</code></pre><p>So our <code>embed_image</code> function takes an input image, prepocesses it through our CLIP model preprocessor, encode the preprocessed image and returns a list representing the embeddings of that image. This returned embedding serves as a concise numerical representation, capturing all the key features and patterns within the image for downstream tasks or analysis. The next thing is to call this function for all the images and store the relevant embeddings in the database.</p><pre><code class=\"language-python\">data = []for i in range(len(ds[\"train\"])):    img = ds[\"train\"][i]['image']    text = ds[\"train\"][i]['text']        # Encode the image    encoded_img = embed_image(img)    data.append({\"vector\": encoded_img, \"text\": text, \"id\" : i})</code></pre><p>Here, we’re just taking a list, adding the numeric embeddings, reference text and the current index id to it. All that’s left is to include this list in our LanceDB table. And voila, our datalake for the embeddings is set up and good to go!</p><pre><code class=\"language-python\">tbl.add(data)tbl.to_pandas()</code></pre><p>Up until now, we’ve efficiently converted the images into their respective multimodal embeddings and stored them in the LanceDB table. Now the LanceDB tables offer a convenient feature: if there’s a need to add or remove images, it’s remarkably straightforward. Just encode the new image and add it, following to the same steps we followed for the previous images.</p><h3 id=\"query-search\">Query search</h3><p>Our next move is to embed our text query using the same multimodal embedding model we used for our images. Remember that “box” I mentioned earlier? Essentially, we want this box to create embeddings for both our images and our texts which ensures that the representation of different types of data happens in the same way. Following this, we just need to initiate a search to find the nearest image embeddings that matches our text query.</p><pre><code class=\"language-python\">def embed_txt(txt):    tokenized_text = clip.tokenize([txt]).to(device)    embeddings = model.encode_text(tokenized_text)        # Detach, move to CPU, convert to numpy array, and extract the first element as a list    result = embeddings.detach().cpu().numpy()[0].tolist()    return resultres = tbl.search(embed_txt(\"a road with a stop\")).limit(3).to_pandas()res</code></pre><pre><code class=\"language-txt\">0 | [0.064575195, .. ] | there is a stop sign...| 569 |\t131.9957281 | [-0.07989502, .. ] | there is a bus that... | 423 | 135.0478522 | [0.06756592, .. ]  | amazing view of a ...\t| 30  | 135.309937</code></pre><p>Let’s slow down a bit and understand what just happened. Putting simply, the code snippet executes a search algorithm in its core to pinpoint the most relevant image embedding that aligns with our text query. The resulting output, as showcased above, gives us the embeddings which closely resembles our text query.  In the result, the second column presents the embedding vector, while the third column contains the description of the image that closely matches our text query. Essentially, we’ve determined which image closely corresponds to our text query by examining the embeddings of both our text query and the image.</p><h3 id=\"its-similar-to-saying-if-these-numbers-represent-the-word-cat-i-spot-an-image-with-a-similar-set-of-numbers-so-most-likely-its-a-match-for-an-image-of-a-cat-\">It’s similar to saying, If these numbers represent the word “Cat”, I spot an image with a similar set of numbers, so most likely it’s a match for an image of a “Cat”. 😺</h3><p>If you are looking for the explanation of how the search happens, I will write a detailed explanation in the coming write ups because it’s so exciting to look under the hood and see how the searching happens. Essentially there is something called Approximate Nearest Neighbors (ANN) which is a technique used to efficiently find the closest points in high-dimensional spaces. ANN is extensively used in data mining, machine learning, computer vision and NLP use cases. So when we passed our embedded text query to the searching algorithm and asked it to give us the closest sample point in the vector space, it used a type of ANN algorithm to get it for us. Specifically LanceDB utilizes DANN (Deep Approximate Nearest Neighbor) for searching the relevant embeddings within its ecosystem..</p><p>In our results, we have five columns. The first is the index number, the second is the embedding vector, the third is the description of the image matching our text query, and the fourth is the label of the image. However, let’s focus on the last column – Distance. When I mentioned the ANN algorithm, it simply draws a line between the current data point (in our case, the embedding of our text query) and identifies which data point (image embedding) is closest to it. If you observe that the other data points in the results have a greater distance compared to the top one, it indicates they are a bit further away or more unrelated to our query. Just to make it clear, the calculation of distance is a part of the algorithm itself.</p><h2 id=\"d-day\">D-DAY</h2><p>Now that we have all the necessary information, displaying the most relevant image for our query is straightforward. Simply take the relevant label of the top-matched embedding vector and showcase the corresponding image.</p><pre><code class=\"language-python\">data_id = int(res['id'][0])display(ds[\"train\"][data_id]['image'])print(ds[\"train\"][data_id]['text'])</code></pre><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/output_final.png?raw=true\" alt=\"output_final\" /></p><pre><code class=\"language-python\">there is a truck driving down a street with a stop sign</code></pre><h3 id=\"whats-next\">What’s next?</h3><p>To make things more interesting, I’m currently working on creating an extensive GTA-V captioning dataset. This dataset will include a larger number of images paired with their respective reference text, providing us with a richer set of queries to explore and experiment with.. Nevertheless, there’s always room for refining the model. We can explore creating a customized CLIP model, adjusting various parameters. Increasing the number of training epochs may afford the model more time to grasp the relevance between embeddings. Additionally, there’s an impressive multimodal embedding model developed by the Meta known as <a href=\"https://imagebind.metademolab.com/\">ImageBind</a>. We can consider trying ImageBind as an alternative to our current multimodal embedding model and compare the outcomes. With numerous options available, the fundamental concept behind the Multimodal RAG workflow remains largely consistent.</p><p>Here’s how everything comes together in one frame and this is the <a href=\"https://colab.research.google.com/drive/1LM-WrDSBXpiMZ94CtaMCaGHlkxqGR6WK?usp=sharing\">Collab</a> for your reference</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/multimodalrag.png?raw=true\" alt=\"multimodal_rag\" /></p>",
            "url": "http://localhost:4000/2024/03/03/multimodal-rag-application",
            
            
            
            "tags": ["LLM","Deep Learning"],
            
            "date_published": "2024-03-03T00:00:00+05:30",
            "date_modified": "2024-03-03T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/02/14/rag-application-with-langchain",
            "title": "Create LLM apps using RAG",
            "summary": "RAG and Langcahin for creating the personalized bots",
            "content_text": "If you’re considering making a personalized bot for your documents or your website that responds to you, you’re in the right spot. I’m here to help you create a bot using Langchain and RAG strategies for this purpose.Understanding the Limitations of ChatGPT and LLMsChatGPTs and other Large Language Models (LLMs) are extensively trained on text corpora to comprehend language semantics and coherence. Despite their impressive capabilities, these models have limitations that require careful consideration for particular use cases. One significant challenge is the potential for hallucinations, where the model might generate inaccurate or contextually irrelevant information.Imagine requesting the model to enhance your company policies; in such scenarios, ChatGPTs and other Large Language Models might struggle to provide factual responses because they lack training on your company’s data. Instead, they may generate nonsensical or irrelevant responses, which can be unhelpful. So, how can we ensure that an LLM comprehends our specific data and generates responses accordingly? This is where techniques like Retrieval Augmentation Generation (RAG) come to the rescue.What is RAG?RAG or Retrieval Augmented Generation uses three main workflows to generate and give the better response      Information Retrieval: When a user asks a question, the AI system retrieves the relevant data from a well-maintained knowledge library or external sources like databases, articles, APIs, or document repositories. This is achieved by converting the query into a numerical format or vector that can be understood by machines.        LLM:The retrieved data is then presented to the LLM or Large Language Model, along with the user’s query. The LLM uses this new knowledge and its training data to generate the response.        Response: Finally, the LLM generates a response that is more accurate and relevant since it has been augmented with the retrieved information. I mean we gave LLM some additional information from our Knowledge library which allows LLMs to provide more contextually relevant and factual responses, solving the problem of models when they are just hallucinating or providing irrelevant answers.  Let’s take the example of company policies again. Suppose you have an HR bot that handles queries related to your Company policies. Now if someone asks anything specific to the policies, The bot can pull the most recent policy documents from the knowledge library, pass the relevant context to a well crafted prompt which is then passed further to the LLM for generating the response.To make it easier, Imagine a LLM as your knowledgeable friend who seems to know everything, from Geography to Computer Science, from Politics to Philosophy. Now, picture yourself asking this friend a few questions:  “Who handles my laundry on weekends?”  “Who lives next door to me?”  “What brand of peanut butter do I prefer?”Chances are, your friend wouldn’t be able to answer these questions. Most of the time, no. But let’s say this distant friend becomes closer to you over time, he comes to your place regularly, knows your parents very well, you both hangout pretty often, you go on outings, blah blah blah.. You got the point.I mean he is gaining access to personal and insider information about you.  Now, when you pose the same questions, he can somehow answer those questions with more relevance now because he is better suited with your personal insights.Similarly, a LLM, when provided with additional information or access to your data, won’t guess or hallucinate. Instead, it can leverage that access data to provide more relevant and accurate answers.To break it down, here are the exact steps to create any RAG application…  Extract the relevant information from your data sources.  Break the information into the small chunks.  Store the chunks as their embedddings into a vector database.  Create a prompt template which will be fed to the LLM with the query and the context.  Convert the query to it’s relevant embedding using same embedding model.  Fetch k number of relevant documents related to the query from the vector database.  Pass the relevant documents to the LLM and get the response.FAQs      We will be using Langchain for this task, Basically it’s like a wrapper which lets you talk and manage your LLM operations better. Note that the Langchain is updating very fast and some functions and other classes might moved to the different modules. So if something doesn’t work, just check if you are importing the libraries from the right sources!        Along with it we will be using Hugging Face, an open-source library for building, training, and deploying state-of-the-art machine learning models, especially about NLP. To use the HuggingFace we need the access token, Get your access token here        For our models, we’ll need two key components: a LLM (Large Language Model) and an embedding model. While paid sources like OpenAI offer these, we’ll be utilizing open-source models to ensure accessibility for everyone.        Now we need a Vector Database to store our embeddings, For that task, we’ve got LanceDB – it’s like a super-smart data lake for handling lots of information. It’s a top-notch vector database, making it the go-to choice for dealing with complex data like vector embeddings.. And the best part? It won’t burn a dent in your pocket because it’s open source and free to use!!        To keep things simple, our data ingestion process will involve using a URL and some PDFs. While you can incorporate additional data sources if needed, we’ll concentrate solely on these two for now.  With Langchain for the interface, Hugging Face for fetching the models, along with open-source components, we’re all set to go! This way, we will save some bucks while still having everything we need. Let’s move to the next stepsEnvironment SetupI am using a MacBook Air M1, and it’s important to note that certain dependencies and configurations may vary depending on the type of system you are using. Now open your favorite editor, create a python environment and install the relevant dependencies# Create a virtual environmentpython3 -m venv env# Activate the virtual environmentsource env/bin/activate# Upgrade pip in the virtual environmentpip install --upgrade pip# Install required dependenciespip3 install lancedb langchain langchain_community prettytable sentence-transformers huggingface-hub bs4 pypdf pandas# This is optional, I did it for removing a warningpip3 uninstall urllib3pip3 install 'urllib3&lt;2.0'Now create a .env file in the same directory to place your Hugging Face api credentials like thisHUGGINGFACEHUB_API_TOKEN = hf_KKNWfBqgwCUOHdHFrBwQ.....Ensure the name “HUGGINGFACEHUB_API_TOKEN” remains unchanged, as it is crucial for authentication purposes.If you prefer a straightforward approach without relying on external packages or file loading, you can directly configure the environment variable within your code like this..HF_TOKEN = \"hf_KKNWfBqgwCUOHdHFrBwQ.....\"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKENFinally create a data folder in the project’s root directory, designated as the central repository for storing PDF documents. You can add some sample PDFs for testing purposes; for instance, I am using the Yolo V7 and Transformers paper for demonstration. It’s important to note that this designated folder will function as our primary source for data ingestion.It seems like everything is in order, and we’re all set!Step 1 : Extracting the relevant informationTo get your RAG application running, the first thing we need to do is to extract the relevant information from the various data sources. It can be a website page, a PDF file, a notion link, a google doc whatever it is, it needs to be extracted from it’s original source first.import osfrom langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, DirectoryLoader# Put the token values inside the double quotesHF_TOKEN = \"hf_*******\"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN# Loading the web url and data url_loader = WebBaseLoader(\"https://gameofthrones.fandom.com/wiki/Jon_Snow\")documents_loader = DirectoryLoader('data', glob=\"./*.pdf\", loader_cls=PyPDFLoader)# Creating the instancesurl_docs = url_loader.load()data_docs = documents_loader.load()# Combining all the data that we ingesteddocs = url_docs + data_docsThis will ingest all the data from the URL link and the PDFs.Step 2 : Breaking the information into smaller chunksWe’ve all the necessary data for developing our RAG application. Now, it’s time to break down this information into smaller chunks. Later, we’ll utilize an embedding model to convert these chunks into their respective embeddings. But why is it important?Think of it like this: If you’re tasked with digesting a 100-page book all at once and then asked a specific question about it, it would be challenging to retrieve the necessary information from the entire book to provide an answer. However, if you’re permitted to break the book into smaller, manageable chunks—let’s say 10 pages each—and each chunk is labeled with an index from 0 to 9, the process becomes much simpler. When the same question is posed after this breakdown, you can easily locate the relevant chunk based on its index and then extract the information needed to answer the question accurately.Picture the book as your extracted information, with each 10-page segment representing a small chunk of data, and the index pages as the embedding. Essentially, we’ll apply an embedding model to these chunks to transform the information into their respective embeddings. While as humans, we may not directly comprehend or relate to these embeddings, they serve as numeric representations of the chunks to our application.  This is how you can do this in Pythonfrom langchain.text_splitter import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)chunks = text_splitter.split_documents(docs)Now the chunk_size parameter specifies the maximum number of characters that a chunk can contain, while the chunk_overlap parameter specifies the number of characters that should overlap between two adjacent chunks. With the chunk_overlap set to 50, the last 50 characters of the adjacent chunks will be shared between each other.This approach helps to prevent important information from being split across two chunks, ensuring that each chunk contains sufficient contextual information of their neighbor chunks for the subsequent processing or analysis.Shared information at the boundary of neighboring chunks enables a more seamless transition and understanding of the text’s content. The best strategy for choosing the chunk_size and chunk_overlap parameters largely depends on the nature of the documents and the purpose of the application.Step 3 : Creating the embeddings and store them into a vectordatabaseThere are two primary methods to generate embeddings for our text chunks. The first involves downloading a model, managing preprocessing, and conducting computations independently. Alternatively, we can leverage Hugging Face’s model hub, which offers a variety of pre-trained models for various NLP tasks, including embedding generation.Opting for the latter approach allows us to utilize one of Hugging Face’s embedding models. With this method, we simply provide our text chunks to the chosen model, saving us from the resource-intensive computations on our local machines. 💀Hugging Face’s model hub provides numerous options for embedding models, and you can explore the leaderboard to select the most suitable one for your requirements. For now, we’ll proceed with “sentence-transformers/all-MiniLM-L6-v2.” This model is pretty fast and highly efficient in our task!!from langchain_community.embeddings import HuggingFaceEmbeddingsembedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': 'cpu'})Here’s a way to see the number of embeddings for each chunkquery = \"Hello I want to see the length of the embeddings for this document.\"len(embeddings.embed_documents([query])[0])# 384We have the embeddings for our chunks, now we need a vector database to store them.When it comes to vector databases, there are plenty of options out there suiting various needs. Databases like Pinecone offer adequate performance and advanced features but come with a hefty price tag. On the other hand, open-source alternatives like FAISS or Chroma may lack some extras but are more than sufficient for those not who don’t require extensive scalability.But wait, I am dropping a bomb here, I’ve recently come across LanceDB, So it’s an open-source vector database similar to FAISS and Chroma. What makes LanceDB stand out is not just its open-source nature but its unparalleled scalability. In fact, after a closer look, I realized that I haven’t done justice in highlighting the true value propositions of LanceDB earlier!!Surprisingly, LanceDB is the most scalable vector database available, outperforming even the likes of Pinecone, Chroma, Qdrant, and others. Scaling up to a billion vectors locally on your laptop is a feat only achievable with LanceDB. I mean this capability is a game-changer, especially when you compare it to other vector databases struggling even with a hundred million vectors. What’s more mind blowing is that LanceDB manages to offer this unprecedented scalability at a fraction of cost, I mean they are offering the utilities and database tools at much cheaper rates than its closest counterparts.So now, We’ll create an instance of LanceDB vector database by calling lancedb.connect(\"lance_database\"). This line essentially sets up a connection to the LanceDB database named “lance_database.” Next, we create a table within the database named “rag_sample” using the create_table function. Now we initialzed this table with a single data entry which includes a numeric vector generated by the embed_query function. So text “Hello World” is first converted to it’s numeric representation (fancy name of embeddings) and then it’s mapped to id number 1. Like a key-value pair. Lastly, the mode=”overwrite” parameter ensures that if the table “rag_sample” already exists, it will be overwritten with the new data.This happens with all the text chunks and it’s quite straightforward. This is how it looks in Python..import lancedbfrom langchain_community.vectorstores import LanceDBdb = lancedb.connect(\"lance_database\")table = db.create_table(    \"rag_sample\",    data=[        {            \"vector\": embeddings.embed_query(\"Hello World\"),            \"text\": \"Hello World\",            \"id\": \"1\",        }    ],    mode=\"overwrite\",)docsearch = LanceDB.from_documents(chunks, embeddings, connection=table)NO ROCKET SCIENCE HA!Step 4 : Create a prompt template which will be fed to the LLMOk now comes the prompt template. So when you write a question to the ChatGPT and it answers that question, you are basically providing a prompt to the model so that it can understand what the question is. When companies train the models, they decide what kind of prompt they are going to use for invoking the model and ask the question. For example, if you are working with “Mistral 7B instruct” and you want the optimal results it’s recommended to use the following chat template:&lt;s&gt;[INST] Instruction [/INST] Model answer&lt;/s&gt;[INST] Follow-up instruction [/INST]Note that &lt;s&gt; and &lt;/s&gt; are special tokens to represent beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings. It’s just that the Mistral 7B instruct is made in such a way that the model looks for those special tokens to understand the question better. Different types of LLMs have different kinds of instructed prompts.Now for our case we are going to use huggingfaceh4/zephyr-7b-alpha which is a text generation model. Just to make it clear, Zephyr-7B-α has not been aligned or formated to human preferences with techniques like RLHF (Reinforcement Learning with Human Feedback) or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so).Instead of writing a Prompt of our own, I will use ChatPromptTemplate class which creates a prompt template for the chat models. In layman terms, instead of writing a specified prompt I am letting ChatPromptTemplate to do it for me. Here is an example prompt template that is being generated from the manual messsages.from langchain_core.prompts import ChatPromptTemplatechat_template = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),        (\"human\", \"Hello, how are you doing?\"),        (\"ai\", \"I'm doing well, thanks!\"),        (\"human\", \"{user_input}\"),    ])messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")If you don’t want to write the manual instructions, you can just use the from_template function to generate a more generic prompt template which I used for this project. Here it is..from langchain_core.prompts import ChatPromptTemplatetemplate = \"\"\"{query}\"\"\"prompt = ChatPromptTemplate.from_template(template)Our prompt is set! We’ve crafted a single message, assuming it’s from a human xD . If you’re not using the from_messages function, the ChatPromptTemplate will ensure your prompt works seamlessly with the language model by reserving some additional system messages. There’s always room for improvement with more generic prompts to achieve better results. For now this setup should work..Step 5 : Convert the query to it’s relevant embedding using same embedding model.Now, let’s talk about the query or question we want to ask our RAG application. We can’t just pass the query to our model and expect information in return. Instead, we need to pass the query through the same embedding model used for the chunks earlier. Why is this important? Well, by embedding queries, we allow models to compare them efficiently with previously processed chunks of text. This enables tasks like finding similar documents or generating relevant responses.To understand it better, Imagine you and your friend speak different languages, like English and Hindi, and you need to understand each other’s writings. If your friend hands you a page in Hindi, you won’t understand it directly. So, your friend translates it first, turning the Hindi into English for you. So now if your friend asks you a question in Hindi, you can easily translate that question into English first and look up for the relevant answers in that translated English Text..Similarly, we initially transformed textual information into their corresponding embeddings. Now,  when you pose a query, it undergoes a similar kind of conversion into the numeric form using the same embedding model applied previously to process our textual chunks. This consistent approach allows for efficient retrieval of relevant responses.Step 6 : Fetch K number of documents.Now, let’s talk about the retriever. Its job is to dive into the vector database and perform a search to find relevant documents. It returns a set number, let’s call it “k”, of these documents, which are ranked based on their contextual relevance to the query or question you asked. You can set “k” as a parameter, indicating how many relevant documents you want - whether it’s 2, 5, or 10. Generally, if you have a smaller amount of data, it’s best to stick with a lower “k”, around 2. For longer documents or larger datasets, a “k” between 10 and 20 is recommended.Different search techniques can be employed to fetch relevant documents more effectively and quickly from a vector database. The choice depends on various factors such as your specific use case, the amount of data you have, what kind of vector database you are using and the context of your problem.retriever = docsearch.as_retriever(search_kwargs={\"k\": 3})docs = retriever.get_relevant_documents(\"what did you know about Yolo V7?\")print(docs)When you run this code, the retriever will fetch 3 most most relevant documents from the vector database. All these documents will be the contexts for our LLM model to generate the response for our query.Step 7 : Pass the relevant documents to the LLM and get the response.So far, we’ve asked our retriever to fetch a set number of relevant documents from the database. Now, we need a language model (LLM) to generate a relevant response based on that context. To ensure robustness, let’s remember that at the beginning of this blog, I mentioned that LLMs like ChatGPT can sometimes generate irrelevant responses, especially when asked about specific use cases or contexts. However, this time, we’re providing the context from our own data to the LLM as a reference. So, it will consider this reference along with its general capabilities to answer the question. That’s the whole idea behind using RAG!Now, let’s dive into implementing the language model (LLM) aspect of our RAG setup. We’ll be using zephyr model architecture from the Hugging Face Hub. Here’s how we do it in Python:from langchain_community.llms import HuggingFaceHub# Model architecturellm_repo_id = \"huggingfaceh4/zephyr-7b-alpha\"model_kwargs = {\"temperature\": 0.5, \"max_length\": 4096, \"max_new_tokens\": 2048}model = HuggingFaceHub(repo_id=llm_repo_id, model_kwargs=model_kwargs)In this code snippet, we’re instantiating our language model using the Hugging Face Hub. Specifically, we’re selecting the zephyr 7 billion model which is placed in this repository ID “huggingfaceh4/zephyr-7b-alpha”. Choosing this model isn’t arbitrary; as I said before, it’s based on the model’s suitability for our specific task and requirements. As we are already implementing only Open Source components, Zephyr 7 billion works good enough to generate a useful response with minimal overhead and low latency.This model comes with some additional parameters to fine-tune its behavior. We’ve set the temperature to 0.5, which controls the randomness of the generated text. As a lower temperature tends to result in more conservative and predictable outputs and when the temperature is set to max which is 1, the model tries to be as creative as it could, so based on what type of output you want for your use case, you can tweak this parameter. For the sake of the simplicity and demonstration purposes, I set it to 0.5 to make sure we get decent results. Next is max_length parameter which defines the maximum length of the generated text and it includes the size of your prompt as well as the response.max_new_tokens sets the threshold on the maximum number of new tokens that can be generated. As a general rule of thumb, the max_new_tokens should always be less than or equal to the max_length parameter. Why? Think about it..Step 8 : Create a chain for invoking the LLM.We have everything we want for our RAG application. The last thing we need to do is to create a chain for invoking the LLM on our query to generate the response. There are different types of chains for the different types of use cases, if you like your LLM to remember the context of the chat over the time like the ChatGPT , you would need a memory instance which can be shared among multiple conversation pieces, for such cases, there are conversational chains available.For now we just need a chain which can combine our retrieved contexts and pass it with the query to the LLM to generate the response.from langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughrag_chain = (    {\"context\": retriever,  \"query\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())response = rag_chain.invoke(\"Who killed Jon Snow?\")We have our Prompt, model, context and the query! All of them are combined into a single chain. It’s pretty much what all the chains does! Now before running the final code, I want to give a quick check on these two helper functions: RunnablePassthrough() and StrOutputParser().The RunnablePassthrough class in LangChain serves to pass inputs unchanged or with additional keys. In our chain, a prompt expects input in the form of a map with keys “context” and “question.” However, user input only includes the “question.” or the “query”.  Here, RunnablePassthrough is utilized to pass the user’s question under the “question” key while retrieving the context using a retriever. It just ensures that the input to the prompt conforms to the expected format.Secondally, StrOutputParser is typically employed in RAG chains to parse the output of the model into a human-readable string. In the layman terms, It is responsible for transforming the model’s output into a more coherent and grammatically correct sentence, which is generally better readable by Humans! That’s it!D-DayTo make sure we get the entire idea even if the response gets cut off, I’ve implemented a function called get_complete_sentence(). Basically this function helps extract the last complete sentence from the text. So, even if the response hits the maximum token limit that we set upon and it gets truncated midway, we will still get a coherent understanding of the message.For practical testing, I suggest storing some low sized PDFs in the data folder of your project. You can choose PDFs related to various topics or domains that you want the chatbot to interact with. Additionally, providing a URL as a reference for the chatbot can be helpful for testing. For example, you could use a Wikipedia page, a research paper, or any other online document relevant to your testing goals. During my testing, I used a URL containing information about Jon Snow from Game of Thrones,  and PDFs of Transformers paper, and the YOLO V7 paper to evaluate the bot’s performance. Let’s see how our bot performs in varied content.import osimport timeimport lancedbfrom langchain_community.vectorstores import LanceDBfrom langchain_community.llms import HuggingFaceHubfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain_community.vectorstores import LanceDBfrom langchain_community.embeddings import HuggingFaceEmbeddingsfrom langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, DirectoryLoaderfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom prettytable import PrettyTableHF_TOKEN = \"hf*********\"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN# Loading the web URL and breaking down the information into chunksstart_time = time.time()loader = WebBaseLoader(\"https://gameofthrones.fandom.com/wiki/Jon_Snow\")documents_loader = DirectoryLoader('data', glob=\"./*.pdf\", loader_cls=PyPDFLoader)# URL loaderurl_docs = loader.load()# Document loaderdata_docs = documents_loader.load()# Combining all the information into a single variabledocs = url_docs + data_docs# Specify chunk size and overlapchunk_size = 256chunk_overlap = 20text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)chunks = text_splitter.split_documents(docs)# Specify Embedding Modelembedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': 'cpu'})# Specify Vector Databasevectorstore_start_time = time.time()database_name = \"LanceDB\"db = lancedb.connect(\"src/lance_database\")table = db.create_table(    \"rag_sample\",    data=[        {            \"vector\": embeddings.embed_query(\"Hello World\"),            \"text\": \"Hello World\",            \"id\": \"1\",        }    ],    mode=\"overwrite\",)docsearch = LanceDB.from_documents(chunks, embeddings, connection=table)vectorstore_end_time = time.time()# Specify Retrieval Informationsearch_kwargs = {\"k\": 3}retriever = docsearch.as_retriever(search_kwargs = {\"k\": 3})# Specify Model Architecturellm_repo_id = \"huggingfaceh4/zephyr-7b-alpha\"model_kwargs = {\"temperature\": 0.5, \"max_length\": 4096, \"max_new_tokens\": 2048}model = HuggingFaceHub(repo_id=llm_repo_id, model_kwargs=model_kwargs)template = \"\"\"{query}\"\"\"prompt = ChatPromptTemplate.from_template(template)rag_chain_start_time = time.time()rag_chain = (    {\"context\": retriever, \"query\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())rag_chain_end_time = time.time()def get_complete_sentence(response):    last_period_index = response.rfind('.')    if last_period_index != -1:        return response[:last_period_index + 1]    else:        return response# Invoke the RAG chain and retrieve the responserag_invoke_start_time = time.time()response = rag_chain.invoke(\"Who killed Jon Snow?\")rag_invoke_end_time = time.time()# Get the complete sentencecomplete_sentence_start_time = time.time()complete_sentence = get_complete_sentence(response)complete_sentence_end_time = time.time()# Create a tabletable = PrettyTable()table.field_names = [\"Task\", \"Time Taken (Seconds)\"]# Add rows to the tabletable.add_row([\"Vectorstore Creation\", round(vectorstore_end_time - vectorstore_start_time, 2)])table.add_row([\"RAG Chain Setup\", round(rag_chain_end_time - rag_chain_start_time, 2)])table.add_row([\"RAG Chain Invocation\", round(rag_invoke_end_time - rag_invoke_start_time, 2)])table.add_row([\"Complete Sentence Extraction\", round(complete_sentence_end_time - complete_sentence_start_time, 2)])# Additional information in the tabletable.add_row([\"Embedding Model\", embedding_model_name])table.add_row([\"LLM (Language Model) Repo ID\", llm_repo_id])table.add_row([\"Vector Database\", database_name])table.add_row([\"Temperature\", model_kwargs[\"temperature\"]])table.add_row([\"Max Length Tokens\", model_kwargs[\"max_length\"]])table.add_row([\"Max New Tokens\", model_kwargs[\"max_new_tokens\"]])table.add_row([\"Chunk Size\", chunk_size])table.add_row([\"Chunk Overlap\", chunk_overlap])table.add_row([\"Number of Documents\", len(docs)])print(\"\\nComplete Sentence:\")print(complete_sentence)# Print the tableprint(\"\\nExecution Timings:\")print(table)To enhance readability and present the execution information in a structured tabular format, I have used PrettyTable library. You can add it to your virtual environment by using the command pip3 install prettytable.So this is the response I received in less than &lt; 1 minute, which is quite considerable for the starters. The time it takes can vary depending on your system’s configuration, but I believe you’ll get decent results in just a few minutes. So, please be patient if it’s taking a bit longer.Human: Question : Who killed Jon Snow?Answer: In the TV series Game of Thrones, Jon Snow was stabbed by his fellow Night's Watch members in season 5, episode 9, \"The Dance of Dragons.\" However, he was later resurrected by Melisandre in season 6, episode 3, \"Oathbreaker.\" So, technically, no one killed Jon Snow in the show.Execution Timings:+------------------------------+----------------------------------------+|             Task             |          Time Taken (Seconds)          |+------------------------------+----------------------------------------+|     Vectorstore Creation     |                 16.21                  ||       RAG Chain Setup        |                  0.03                  ||     RAG Chain Invocation     |                  2.06                  || Complete Sentence Extraction |                  0.0                   ||       Embedding Model        | sentence-transformers/all-MiniLM-L6-v2 || LLM (Language Model) Repo ID |     huggingfaceh4/zephyr-7b-alpha      ||       Vector Database        |                LanceDB                 ||         Temperature          |                  0.5                   ||      Max Length Tokens       |                  4096                  ||        Max New Tokens        |                  2048                  ||          Chunk Size          |                  256                   ||        Chunk Overlap         |                   20                   ||     Number of Documents      |                   39                   |+------------------------------+----------------------------------------+Have fun experimenting with various data sources! You can try changing the website addresses, adding new PDF files or maybe change the template a bit. LLMs are fun, you never know what you get!What’s next?There are plenty of things we can adjust here. We could switch to a more effective embedding model for better indexing, try different searching techniques for the retriever, add a reranker to improve the ranking of documents, or use a more advanced LLM with a larger context window and faster response times. Essentially, every RAG application is just an enhanced version based on these factors. However, the fundamental concept of how a RAG application works always remains the same.Here is the collab link for the reference..",
            "content_html": "<p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/make_your_application_with_rag/cat.png?raw=true\" alt=\"cat\" /></p><p><em>If you’re considering making a personalized bot for your documents or your website that responds to you, you’re in the right spot. I’m here to help you create a bot using Langchain and RAG strategies for this purpose.</em></p><h3 id=\"understanding-the-limitations-of-chatgpt-and-llms\">Understanding the Limitations of ChatGPT and LLMs</h3><p>ChatGPTs and other Large Language Models (LLMs) are extensively trained on text corpora to comprehend language semantics and coherence. Despite their impressive capabilities, these models have limitations that require careful consideration for particular use cases. One significant challenge is the potential for hallucinations, where the model might generate inaccurate or contextually irrelevant information.</p><p>Imagine requesting the model to enhance your company policies; in such scenarios, ChatGPTs and other Large Language Models might struggle to provide factual responses because they lack training on your company’s data. Instead, they may generate nonsensical or irrelevant responses, which can be unhelpful. So, how can we ensure that an LLM comprehends our specific data and generates responses accordingly? This is where techniques like Retrieval Augmentation Generation (RAG) come to the rescue.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/make_your_application_with_rag/LLM_without_RAG.png?raw=true\" alt=\"RAG\" /></p><h3 id=\"what-is-rag\">What is RAG?</h3><p>RAG or Retrieval Augmented Generation uses three main workflows to generate and give the better response</p><ul>  <li>    <p>Information Retrieval: When a user asks a question, the AI system retrieves the relevant data from a well-maintained knowledge library or external sources like databases, articles, APIs, or document repositories. This is achieved by converting the query into a numerical format or vector that can be understood by machines.</p>  </li>  <li>    <p>LLM:The retrieved data is then presented to the LLM or Large Language Model, along with the user’s query. The LLM uses this new knowledge and its training data to generate the response.</p>  </li>  <li>    <p>Response: Finally, the LLM generates a response that is more accurate and relevant since it has been augmented with the retrieved information. I mean we gave LLM some additional information from our Knowledge library which allows LLMs to provide more contextually relevant and factual responses, solving the problem of models when they are just hallucinating or providing irrelevant answers.</p>  </li></ul><p>Let’s take the example of company policies again. Suppose you have an HR bot that handles queries related to your Company policies. Now if someone asks anything specific to the policies, The bot can pull the most recent policy documents from the knowledge library, pass the relevant context to a well crafted prompt which is then passed further to the LLM for generating the response.</p><p>To make it easier, Imagine a LLM as your knowledgeable friend who seems to know everything, from Geography to Computer Science, from Politics to Philosophy. Now, picture yourself asking this friend a few questions:</p><ul>  <li>“Who handles my laundry on weekends?”</li>  <li>“Who lives next door to me?”</li>  <li>“What brand of peanut butter do I prefer?”</li></ul><p>Chances are, your friend wouldn’t be able to answer these questions. Most of the time, no. But let’s say this distant friend becomes closer to you over time, he comes to your place regularly, knows your parents very well, you both hangout pretty often, you go on outings, blah blah blah.. You got the point.</p><p>I mean he is gaining access to personal and insider information about you.  Now, when you pose the same questions, he can somehow answer those questions with more relevance now because he is better suited with your personal insights.</p><p>Similarly, a LLM, when provided with additional information or access to your data, won’t guess or hallucinate. Instead, it can leverage that access data to provide more relevant and accurate answers.</p><h3 id=\"to-break-it-down-here-are-the-exact-steps-to-create-any-rag-application\">To break it down, here are the exact steps to create any RAG application…</h3><ol>  <li>Extract the relevant information from your data sources.</li>  <li>Break the information into the small chunks.</li>  <li>Store the chunks as their embedddings into a vector database.</li>  <li>Create a prompt template which will be fed to the LLM with the query and the context.</li>  <li>Convert the query to it’s relevant embedding using same embedding model.</li>  <li>Fetch k number of relevant documents related to the query from the vector database.</li>  <li>Pass the relevant documents to the LLM and get the response.</li></ol><h3 id=\"faqs\">FAQs</h3><ol>  <li>    <p>We will be using <a href=\"https://python.langchain.com/docs/get_started/introduction\">Langchain</a> for this task, Basically it’s like a wrapper which lets you talk and manage your LLM operations better. Note that the Langchain is updating very fast and some functions and other classes might moved to the different modules. So if something doesn’t work, just check if you are importing the libraries from the right sources!</p>  </li>  <li>    <p>Along with it we will be using <a href=\"https://huggingface.co/\">Hugging Face</a>, an open-source library for building, training, and deploying state-of-the-art machine learning models, especially about NLP. To use the HuggingFace we need the access token, Get your access token <a href=\"https://huggingface.co/docs/hub/security-tokens\">here</a></p>  </li>  <li>    <p>For our models, we’ll need two key components: a LLM (Large Language Model) and an embedding model. While paid sources like OpenAI offer these, we’ll be utilizing open-source models to ensure accessibility for everyone.</p>  </li>  <li>    <p>Now we need a Vector Database to store our embeddings, For that task, we’ve got <a href=\"https://lancedb.com/\">LanceDB</a> – it’s like a super-smart data lake for handling lots of information. It’s a top-notch vector database, making it the go-to choice for dealing with complex data like vector embeddings.. And the best part? It won’t burn a dent in your pocket because it’s open source and free to use!!</p>  </li>  <li>    <p>To keep things simple, our data ingestion process will involve using a URL and some PDFs. While you can incorporate additional data sources if needed, we’ll concentrate solely on these two for now.</p>  </li></ol><p>With Langchain for the interface, Hugging Face for fetching the models, along with open-source components, we’re all set to go! This way, we will save some bucks while still having everything we need. Let’s move to the next steps</p><h3 id=\"environment-setup\">Environment Setup</h3><p>I am using a MacBook Air M1, and it’s important to note that certain dependencies and configurations may vary depending on the type of system you are using. Now open your favorite editor, create a python environment and install the relevant dependencies</p><pre><code class=\"language-python\"># Create a virtual environmentpython3 -m venv env# Activate the virtual environmentsource env/bin/activate# Upgrade pip in the virtual environmentpip install --upgrade pip# Install required dependenciespip3 install lancedb langchain langchain_community prettytable sentence-transformers huggingface-hub bs4 pypdf pandas# This is optional, I did it for removing a warningpip3 uninstall urllib3pip3 install 'urllib3&lt;2.0'</code></pre><p>Now create a .env file in the same directory to place your Hugging Face api credentials like this</p><pre><code class=\"language-python\">HUGGINGFACEHUB_API_TOKEN = hf_KKNWfBqgwCUOHdHFrBwQ.....</code></pre><p>Ensure the name “HUGGINGFACEHUB_API_TOKEN” remains unchanged, as it is crucial for authentication purposes.</p><p>If you prefer a straightforward approach without relying on external packages or file loading, you can directly configure the environment variable within your code like this..</p><pre><code class=\"language-python\">HF_TOKEN = \"hf_KKNWfBqgwCUOHdHFrBwQ.....\"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN</code></pre><p>Finally create a data folder in the project’s root directory, designated as the central repository for storing PDF documents. You can add some sample PDFs for testing purposes; for instance, I am using the <a href=\"https://arxiv.org/pdf/2207.02696.pdf\">Yolo V7</a> and <a href=\"https://arxiv.org/abs/1706.03762\">Transformers</a> paper for demonstration. It’s important to note that this designated folder will function as our primary source for data ingestion.</p><p>It seems like everything is in order, and we’re all set!</p><h3 id=\"step-1--extracting-the-relevant-information\">Step 1 : Extracting the relevant information</h3><p>To get your RAG application running, the first thing we need to do is to extract the relevant information from the various data sources. It can be a website page, a PDF file, a notion link, a google doc whatever it is, it needs to be extracted from it’s original source first.</p><pre><code class=\"language-python\">import osfrom langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, DirectoryLoader# Put the token values inside the double quotesHF_TOKEN = \"hf_*******\"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN# Loading the web url and data url_loader = WebBaseLoader(\"https://gameofthrones.fandom.com/wiki/Jon_Snow\")documents_loader = DirectoryLoader('data', glob=\"./*.pdf\", loader_cls=PyPDFLoader)# Creating the instancesurl_docs = url_loader.load()data_docs = documents_loader.load()# Combining all the data that we ingesteddocs = url_docs + data_docs</code></pre><p>This will ingest all the data from the URL link and the PDFs.</p><h3 id=\"step-2--breaking-the-information-into-smaller-chunks\">Step 2 : Breaking the information into smaller chunks</h3><p>We’ve all the necessary data for developing our RAG application. Now, it’s time to break down this information into smaller chunks. Later, we’ll utilize an embedding model to convert these chunks into their respective embeddings. But why is it important?</p><p>Think of it like this: If you’re tasked with digesting a 100-page book all at once and then asked a specific question about it, it would be challenging to retrieve the necessary information from the entire book to provide an answer. However, if you’re permitted to break the book into smaller, manageable chunks—let’s say 10 pages each—and each chunk is labeled with an index from 0 to 9, the process becomes much simpler. When the same question is posed after this breakdown, you can easily locate the relevant chunk based on its index and then extract the information needed to answer the question accurately.</p><p>Picture the book as your extracted information, with each 10-page segment representing a small chunk of data, and the index pages as the embedding. Essentially, we’ll apply an embedding model to these chunks to transform the information into their respective embeddings. While as humans, we may not directly comprehend or relate to these embeddings, they serve as numeric representations of the chunks to our application.  This is how you can do this in Python</p><pre><code class=\"language-python\">from langchain.text_splitter import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)chunks = text_splitter.split_documents(docs)</code></pre><p>Now the chunk_size parameter specifies the maximum number of characters that a chunk can contain, while the chunk_overlap parameter specifies the number of characters that should overlap between two adjacent chunks. With the chunk_overlap set to 50, the last 50 characters of the adjacent chunks will be shared between each other.</p><p>This approach helps to prevent important information from being split across two chunks, ensuring that each chunk contains sufficient contextual information of their neighbor chunks for the subsequent processing or analysis.</p><p>Shared information at the boundary of neighboring chunks enables a more seamless transition and understanding of the text’s content. The best strategy for choosing the chunk_size and chunk_overlap parameters largely depends on the nature of the documents and the purpose of the application.</p><h3 id=\"step-3--creating-the-embeddings-and-store-them-into-a-vectordatabase\">Step 3 : Creating the embeddings and store them into a vectordatabase</h3><p>There are two primary methods to generate embeddings for our text chunks. The first involves downloading a model, managing preprocessing, and conducting computations independently. Alternatively, we can leverage Hugging Face’s model hub, which offers a variety of pre-trained models for various NLP tasks, including embedding generation.</p><p>Opting for the latter approach allows us to utilize one of Hugging Face’s embedding models. With this method, we simply provide our text chunks to the chosen model, saving us from the resource-intensive computations on our local machines. 💀</p><p>Hugging Face’s model hub provides numerous options for embedding models, and you can explore the <a href=\"https://huggingface.co/spaces/mteb/leaderboard\">leaderboard</a> to select the most suitable one for your requirements. For now, we’ll proceed with “sentence-transformers/all-MiniLM-L6-v2.” This model is pretty fast and highly efficient in our task!!</p><pre><code class=\"language-python\">from langchain_community.embeddings import HuggingFaceEmbeddingsembedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': 'cpu'})</code></pre><p>Here’s a way to see the number of embeddings for each chunk</p><pre><code class=\"language-python\">query = \"Hello I want to see the length of the embeddings for this document.\"len(embeddings.embed_documents([query])[0])# 384</code></pre><p>We have the embeddings for our chunks, now we need a vector database to store them.</p><p>When it comes to vector databases, there are plenty of options out there suiting various needs. Databases like Pinecone offer adequate performance and advanced features but come with a hefty price tag. On the other hand, open-source alternatives like FAISS or Chroma may lack some extras but are more than sufficient for those not who don’t require extensive scalability.</p><p>But wait, I am dropping a bomb here, I’ve recently come across LanceDB, So it’s an open-source vector database similar to FAISS and Chroma. What makes LanceDB stand out is not just its open-source nature but its unparalleled scalability. In fact, after a closer look, I realized that I haven’t done justice in highlighting the true value propositions of LanceDB earlier!!</p><p>Surprisingly, LanceDB is the most scalable vector database available, outperforming even the likes of Pinecone, Chroma, Qdrant, and others. Scaling up to a billion vectors locally on your laptop is a feat only achievable with LanceDB. I mean this capability is a game-changer, especially when you compare it to other vector databases struggling even with a hundred million vectors. What’s more mind blowing is that LanceDB manages to offer this unprecedented scalability at a fraction of cost, I mean they are offering the utilities and database tools at much cheaper rates than its closest counterparts.</p><p>So now, We’ll create an instance of LanceDB vector database by calling <code>lancedb.connect(\"lance_database\")</code>. This line essentially sets up a connection to the LanceDB database named “lance_database.” Next, we create a table within the database named “rag_sample” using the create_table function. Now we initialzed this table with a single data entry which includes a numeric vector generated by the embed_query function. So text “Hello World” is first converted to it’s numeric representation (fancy name of embeddings) and then it’s mapped to <code>id</code> number 1. Like a key-value pair. Lastly, the mode=”overwrite” parameter ensures that if the table “rag_sample” already exists, it will be overwritten with the new data.</p><p>This happens with all the text chunks and it’s quite straightforward. This is how it looks in Python..</p><pre><code class=\"language-python\">import lancedbfrom langchain_community.vectorstores import LanceDBdb = lancedb.connect(\"lance_database\")table = db.create_table(    \"rag_sample\",    data=[        {            \"vector\": embeddings.embed_query(\"Hello World\"),            \"text\": \"Hello World\",            \"id\": \"1\",        }    ],    mode=\"overwrite\",)docsearch = LanceDB.from_documents(chunks, embeddings, connection=table)</code></pre><p>NO ROCKET SCIENCE HA!</p><h3 id=\"step-4--create-a-prompt-template-which-will-be-fed-to-the-llm\">Step 4 : Create a prompt template which will be fed to the LLM</h3><p>Ok now comes the prompt template. So when you write a question to the ChatGPT and it answers that question, you are basically providing a prompt to the model so that it can understand what the question is. When companies train the models, they decide what kind of prompt they are going to use for invoking the model and ask the question. For example, if you are working with “Mistral 7B instruct” and you want the optimal results it’s recommended to use the following chat template:</p><pre><code class=\"language-python\">&lt;s&gt;[INST] Instruction [/INST] Model answer&lt;/s&gt;[INST] Follow-up instruction [/INST]</code></pre><p>Note that &lt;s&gt; and &lt;/s&gt; are special tokens to represent beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings. It’s just that the Mistral 7B instruct is made in such a way that the model looks for those special tokens to understand the question better. Different types of LLMs have different kinds of instructed prompts.</p><p>Now for our case we are going to use <a href=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\">huggingfaceh4/zephyr-7b-alpha</a> which is a text generation model. Just to make it clear, Zephyr-7B-α has not been aligned or formated to human preferences with techniques like RLHF (Reinforcement Learning with Human Feedback) or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so).</p><p>Instead of writing a Prompt of our own, I will use ChatPromptTemplate class which creates a prompt template for the chat models. In layman terms, instead of writing a specified prompt I am letting ChatPromptTemplate to do it for me. Here is an example prompt template that is being generated from the manual messsages.</p><pre><code class=\"language-python\">from langchain_core.prompts import ChatPromptTemplatechat_template = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),        (\"human\", \"Hello, how are you doing?\"),        (\"ai\", \"I'm doing well, thanks!\"),        (\"human\", \"{user_input}\"),    ])messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")</code></pre><p>If you don’t want to write the manual instructions, you can just use the <em>from_template</em> function to generate a more generic prompt template which I used for this project. Here it is..</p><pre><code class=\"language-python\">from langchain_core.prompts import ChatPromptTemplatetemplate = \"\"\"{query}\"\"\"prompt = ChatPromptTemplate.from_template(template)</code></pre><p>Our prompt is set! We’ve crafted a single message, assuming it’s from a human xD . If you’re not using the from_messages function, the ChatPromptTemplate will ensure your prompt works seamlessly with the language model by reserving some additional system messages. There’s always room for improvement with more generic prompts to achieve better results. For now this setup should work..</p><h3 id=\"step-5--convert-the-query-to-its-relevant-embedding-using-same-embedding-model\">Step 5 : Convert the query to it’s relevant embedding using same embedding model.</h3><p>Now, let’s talk about the query or question we want to ask our RAG application. We can’t just pass the query to our model and expect information in return. Instead, we need to pass the query through the same embedding model used for the chunks earlier. Why is this important? Well, by embedding queries, we allow models to compare them efficiently with previously processed chunks of text. This enables tasks like finding similar documents or generating relevant responses.</p><p>To understand it better, Imagine you and your friend speak different languages, like English and Hindi, and you need to understand each other’s writings. If your friend hands you a page in Hindi, you won’t understand it directly. So, your friend translates it first, turning the Hindi into English for you. So now if your friend asks you a question in Hindi, you can easily translate that question into English first and look up for the relevant answers in that translated English Text..</p><p>Similarly, we initially transformed textual information into their corresponding embeddings. Now,  when you pose a query, it undergoes a similar kind of conversion into the numeric form using the same embedding model applied previously to process our textual chunks. This consistent approach allows for efficient retrieval of relevant responses.</p><h3 id=\"step-6--fetch-k-number-of-documents\">Step 6 : Fetch K number of documents.</h3><p>Now, let’s talk about the retriever. Its job is to dive into the vector database and perform a search to find relevant documents. It returns a set number, let’s call it “k”, of these documents, which are ranked based on their contextual relevance to the query or question you asked. You can set “k” as a parameter, indicating how many relevant documents you want - whether it’s 2, 5, or 10. Generally, if you have a smaller amount of data, it’s best to stick with a lower “k”, around 2. For longer documents or larger datasets, a “k” between 10 and 20 is recommended.</p><p>Different <a href=\"https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore\">search techniques</a> can be employed to fetch relevant documents more effectively and quickly from a vector database. The choice depends on various factors such as your specific use case, the amount of data you have, what kind of vector database you are using and the context of your problem.</p><pre><code class=\"language-python\">retriever = docsearch.as_retriever(search_kwargs={\"k\": 3})docs = retriever.get_relevant_documents(\"what did you know about Yolo V7?\")print(docs)</code></pre><p>When you run this code, the retriever will fetch 3 most most relevant documents from the vector database. All these documents will be the contexts for our LLM model to generate the response for our query.</p><h3 id=\"step-7--pass-the-relevant-documents-to-the-llm-and-get-the-response\">Step 7 : Pass the relevant documents to the LLM and get the response.</h3><p>So far, we’ve asked our retriever to fetch a set number of relevant documents from the database. Now, we need a language model (LLM) to generate a relevant response based on that context. To ensure robustness, let’s remember that at the beginning of this blog, I mentioned that LLMs like ChatGPT can sometimes generate irrelevant responses, especially when asked about specific use cases or contexts. However, this time, we’re providing the context from our own data to the LLM as a reference. So, it will consider this reference along with its general capabilities to answer the question. That’s the whole idea behind using RAG!</p><p>Now, let’s dive into implementing the language model (LLM) aspect of our RAG setup. We’ll be using zephyr model architecture from the Hugging Face Hub. Here’s how we do it in Python:</p><pre><code class=\"language-python\">from langchain_community.llms import HuggingFaceHub# Model architecturellm_repo_id = \"huggingfaceh4/zephyr-7b-alpha\"model_kwargs = {\"temperature\": 0.5, \"max_length\": 4096, \"max_new_tokens\": 2048}model = HuggingFaceHub(repo_id=llm_repo_id, model_kwargs=model_kwargs)</code></pre><p>In this code snippet, we’re instantiating our language model using the Hugging Face Hub. Specifically, we’re selecting the zephyr 7 billion model which is placed in this repository ID <a href=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\">“huggingfaceh4/zephyr-7b-alpha”</a>. Choosing this model isn’t arbitrary; as I said before, it’s based on the model’s suitability for our specific task and requirements. As we are already implementing only Open Source components, Zephyr 7 billion works good enough to generate a useful response with minimal overhead and low latency.</p><p>This model comes with some additional parameters to fine-tune its behavior. We’ve set the temperature to 0.5, which controls the randomness of the generated text. As a lower temperature tends to result in more conservative and predictable outputs and when the temperature is set to max which is 1, the model tries to be as creative as it could, so based on what type of output you want for your use case, you can tweak this parameter. For the sake of the simplicity and demonstration purposes, I set it to 0.5 to make sure we get decent results. Next is max_length parameter which defines the maximum length of the generated text and it includes the size of your prompt as well as the response.</p><p>max_new_tokens sets the threshold on the maximum number of new tokens that can be generated. As a general rule of thumb, the max_new_tokens should always be less than or equal to the max_length parameter. Why? Think about it..</p><h3 id=\"step-8--create-a-chain-for-invoking-the-llm\">Step 8 : Create a chain for invoking the LLM.</h3><p>We have everything we want for our RAG application. The last thing we need to do is to create a chain for invoking the LLM on our query to generate the response. There are different types of chains for the different types of use cases, if you like your LLM to remember the context of the chat over the time like the ChatGPT , you would need a memory instance which can be shared among multiple conversation pieces, for such cases, there are conversational chains available.</p><p>For now we just need a chain which can combine our retrieved contexts and pass it with the query to the LLM to generate the response.</p><pre><code class=\"language-python\">from langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughrag_chain = (    {\"context\": retriever,  \"query\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())response = rag_chain.invoke(\"Who killed Jon Snow?\")</code></pre><p>We have our Prompt, model, context and the query! All of them are combined into a single chain. It’s pretty much what all the chains does! Now before running the final code, I want to give a quick check on these two helper functions: <code>RunnablePassthrough()</code> and <code>StrOutputParser()</code>.</p><p>The <code>RunnablePassthrough</code> class in <code>LangChain</code> serves to pass inputs unchanged or with additional keys. In our chain, a prompt expects input in the form of a map with keys “context” and “question.” However, user input only includes the “question.” or the “query”.  Here, <code>RunnablePassthrough</code> is utilized to pass the user’s question under the “question” key while retrieving the context using a retriever. It just ensures that the input to the prompt conforms to the expected format.</p><p>Secondally, <code>StrOutputParser</code> is typically employed in RAG chains to parse the output of the model into a human-readable string. In the layman terms, It is responsible for transforming the model’s output into a more coherent and grammatically correct sentence, which is generally better readable by Humans! That’s it!</p><h3 id=\"d-day\">D-Day</h3><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/make_your_application_with_rag/LLM_with_RAG.png?raw=true\" alt=\"With_RAG\" /></p><p>To make sure we get the entire idea even if the response gets cut off, I’ve implemented a function called <code>get_complete_sentence()</code>. Basically this function helps extract the last complete sentence from the text. So, even if the response hits the maximum token limit that we set upon and it gets truncated midway, we will still get a coherent understanding of the message.</p><p>For practical testing, I suggest storing some low sized PDFs in the data folder of your project. You can choose PDFs related to various topics or domains that you want the chatbot to interact with. Additionally, providing a URL as a reference for the chatbot can be helpful for testing. For example, you could use a Wikipedia page, a research paper, or any other online document relevant to your testing goals. During my testing, I used a URL containing information about Jon Snow from Game of Thrones,  and PDFs of Transformers paper, and the YOLO V7 paper to evaluate the bot’s performance. Let’s see how our bot performs in varied content.</p><pre><code class=\"language-python\">import osimport timeimport lancedbfrom langchain_community.vectorstores import LanceDBfrom langchain_community.llms import HuggingFaceHubfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain_community.vectorstores import LanceDBfrom langchain_community.embeddings import HuggingFaceEmbeddingsfrom langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, DirectoryLoaderfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom prettytable import PrettyTableHF_TOKEN = \"hf*********\"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN# Loading the web URL and breaking down the information into chunksstart_time = time.time()loader = WebBaseLoader(\"https://gameofthrones.fandom.com/wiki/Jon_Snow\")documents_loader = DirectoryLoader('data', glob=\"./*.pdf\", loader_cls=PyPDFLoader)# URL loaderurl_docs = loader.load()# Document loaderdata_docs = documents_loader.load()# Combining all the information into a single variabledocs = url_docs + data_docs# Specify chunk size and overlapchunk_size = 256chunk_overlap = 20text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)chunks = text_splitter.split_documents(docs)# Specify Embedding Modelembedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': 'cpu'})# Specify Vector Databasevectorstore_start_time = time.time()database_name = \"LanceDB\"db = lancedb.connect(\"src/lance_database\")table = db.create_table(    \"rag_sample\",    data=[        {            \"vector\": embeddings.embed_query(\"Hello World\"),            \"text\": \"Hello World\",            \"id\": \"1\",        }    ],    mode=\"overwrite\",)docsearch = LanceDB.from_documents(chunks, embeddings, connection=table)vectorstore_end_time = time.time()# Specify Retrieval Informationsearch_kwargs = {\"k\": 3}retriever = docsearch.as_retriever(search_kwargs = {\"k\": 3})# Specify Model Architecturellm_repo_id = \"huggingfaceh4/zephyr-7b-alpha\"model_kwargs = {\"temperature\": 0.5, \"max_length\": 4096, \"max_new_tokens\": 2048}model = HuggingFaceHub(repo_id=llm_repo_id, model_kwargs=model_kwargs)template = \"\"\"{query}\"\"\"prompt = ChatPromptTemplate.from_template(template)rag_chain_start_time = time.time()rag_chain = (    {\"context\": retriever, \"query\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())rag_chain_end_time = time.time()def get_complete_sentence(response):    last_period_index = response.rfind('.')    if last_period_index != -1:        return response[:last_period_index + 1]    else:        return response# Invoke the RAG chain and retrieve the responserag_invoke_start_time = time.time()response = rag_chain.invoke(\"Who killed Jon Snow?\")rag_invoke_end_time = time.time()# Get the complete sentencecomplete_sentence_start_time = time.time()complete_sentence = get_complete_sentence(response)complete_sentence_end_time = time.time()# Create a tabletable = PrettyTable()table.field_names = [\"Task\", \"Time Taken (Seconds)\"]# Add rows to the tabletable.add_row([\"Vectorstore Creation\", round(vectorstore_end_time - vectorstore_start_time, 2)])table.add_row([\"RAG Chain Setup\", round(rag_chain_end_time - rag_chain_start_time, 2)])table.add_row([\"RAG Chain Invocation\", round(rag_invoke_end_time - rag_invoke_start_time, 2)])table.add_row([\"Complete Sentence Extraction\", round(complete_sentence_end_time - complete_sentence_start_time, 2)])# Additional information in the tabletable.add_row([\"Embedding Model\", embedding_model_name])table.add_row([\"LLM (Language Model) Repo ID\", llm_repo_id])table.add_row([\"Vector Database\", database_name])table.add_row([\"Temperature\", model_kwargs[\"temperature\"]])table.add_row([\"Max Length Tokens\", model_kwargs[\"max_length\"]])table.add_row([\"Max New Tokens\", model_kwargs[\"max_new_tokens\"]])table.add_row([\"Chunk Size\", chunk_size])table.add_row([\"Chunk Overlap\", chunk_overlap])table.add_row([\"Number of Documents\", len(docs)])print(\"\\nComplete Sentence:\")print(complete_sentence)# Print the tableprint(\"\\nExecution Timings:\")print(table)</code></pre><p>To enhance readability and present the execution information in a structured tabular format, I have used <code>PrettyTable</code> library. You can add it to your virtual environment by using the command <code>pip3 install prettytable</code>.</p><p>So this is the response I received in less than &lt; 1 minute, which is quite considerable for the starters. The time it takes can vary depending on your system’s configuration, but I believe you’ll get decent results in just a few minutes. So, please be patient if it’s taking a bit longer.</p><pre><code class=\"language-python\">Human: Question : Who killed Jon Snow?Answer: In the TV series Game of Thrones, Jon Snow was stabbed by his fellow Night's Watch members in season 5, episode 9, \"The Dance of Dragons.\" However, he was later resurrected by Melisandre in season 6, episode 3, \"Oathbreaker.\" So, technically, no one killed Jon Snow in the show.Execution Timings:+------------------------------+----------------------------------------+|             Task             |          Time Taken (Seconds)          |+------------------------------+----------------------------------------+|     Vectorstore Creation     |                 16.21                  ||       RAG Chain Setup        |                  0.03                  ||     RAG Chain Invocation     |                  2.06                  || Complete Sentence Extraction |                  0.0                   ||       Embedding Model        | sentence-transformers/all-MiniLM-L6-v2 || LLM (Language Model) Repo ID |     huggingfaceh4/zephyr-7b-alpha      ||       Vector Database        |                LanceDB                 ||         Temperature          |                  0.5                   ||      Max Length Tokens       |                  4096                  ||        Max New Tokens        |                  2048                  ||          Chunk Size          |                  256                   ||        Chunk Overlap         |                   20                   ||     Number of Documents      |                   39                   |+------------------------------+----------------------------------------+</code></pre><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/make_your_application_with_rag/cat2.png?raw=true\" alt=\"cat2\" /></p><p>Have fun experimenting with various data sources! You can try changing the website addresses, adding new PDF files or maybe change the template a bit. LLMs are fun, you never know what you get!</p><h3 id=\"whats-next\">What’s next?</h3><p>There are plenty of things we can adjust here. We could switch to a more effective embedding model for better indexing, try different searching techniques for the retriever, add a reranker to improve the ranking of documents, or use a more advanced LLM with a larger context window and faster response times. Essentially, every RAG application is just an enhanced version based on these factors. However, the fundamental concept of how a RAG application works always remains the same.</p><p>Here is the <a href=\"https://colab.research.google.com/drive/1YsOfovVdNPBwCDMWHvLfOaNtqXn4qXTs?usp=sharing\">collab</a> link for the reference..</p>",
            "url": "http://localhost:4000/2024/02/14/rag-application-with-langchain",
            
            
            
            "tags": ["LLM","Deep Learning"],
            
            "date_published": "2024-02-14T00:00:00+05:30",
            "date_modified": "2024-02-14T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        }
    
    ]
}