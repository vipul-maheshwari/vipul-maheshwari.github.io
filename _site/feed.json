{
    "version": "https://jsonfeed.org/version/1",
    "title": "Blixxi Labs",
    "home_page_url": "http://localhost:4000/",
    "feed_url": "http://localhost:4000/feed.json",
    "description": "Everything related to AI",
    "icon": "http://localhost:4000/apple-touch-icon.png",
    "favicon": "http://localhost:4000/favicon.ico",
    "expired": false,
    
    "author": "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}",
    
"items": [
    
        {
            "id": "http://localhost:4000/2024/01/23/rag-application-with-langchain",
            "title": "Create LLM apps using RAG",
            "summary": "RAG and Langcahin for creating the personalized bots",
            "content_text": "If you’ve ever thought about creating a custom bot for your documents or website that interacts based on specific data, you’re in the right place. I’m here to assist you in developing a bot that leverages Langchain and RAG strategies for this purpose.Understanding the Limitations of ChatGPT and LLMsChatGPTs and other Large Language Models (LLMs) are extensively trained on text corpora to comprehend language semantics and coherence. Despite their impressive capabilities, these models have limitations that require careful consideration for particular use cases. One significant challenge is the potential for hallucinations, where the model might generate inaccurate or contextually irrelevant information.Imagine requesting the model to enhance your company policies; in such scenarios, ChatGPTs and other Large Language Models might struggle to provide factual responses because they lack training on your company’s specific policies. Instead, they may generate nonsensical or irrelevant responses, which can be unhelpful. So, how can we ensure that an LLM comprehends our specific data and generates responses accordingly? This is where techniques like Retrieval Augmentation Generation (RAG) come to the rescue.What is RAG?RAG or Retrieval Augmented Generation uses three main workflows to generate and give the better response      Information Retrieval: When a user asks a question, the AI system retrieves the relevant data from a well-maintained knowledge library or external sources like databases, articles, APIs, or document repositories. This is achieved by converting the query into a numerical format or vector that can be understood by machines.        LLM: The retrieved data is then presented to the LLM or Large Language Model, along with the user’s query. The LLM uses this new knowledge and its training data to generate the response.        Response: Finally, the LLM generates a response that is more accurate and relevant since it has been augmented with the retrieved information. I mean we gave LLM some additional information from our Knowledge library which allows LLMs to provide more contextually relevant and factual responses, solving the problem of models when they are just hallucinating or providing irrelevant answer.  Let’s take an example of company policies again. Suppose you have an HR bot that handles queries related to your Company policies. Now if someones asks anything specific to the policies, The bot can pull the most recent policy documents from the knowledge libray, pass the relevant context to a well crafted prompt which is then passed further to the LLM for generating the response.To make it more easy, Imagine a LLM as your knowledgeable friend who seems to know everything, from Geography to Computer Science from Politics to Philosophy. Now, picture yourself asking this friend a few questions:  “Who handles my laundry on weekends?”  “Who lives next door to me?”  “What brand of peanut butter do I prefer?”Chances are, your friend wouldn’t be able to answer these questions, right? Most of the time, no.But let’s say this distant friend becomes closer to you over time, he comes at your place regularly, know your parents very well, you both hangout pretty often, you go on outings, blah blah blah.. You got the point right? I mean he is gaining access to personal and insider information about you.  Now, when you pose the same questions, they can somehow answer those question with more relevance now because because he is better suited with your personal insights.Similarly, an LLM, when provided with additional information or access to your use case data, won’t guess or hallucinate. Instead, it will leverage that data to provide relevant and accurate answers.To break it down, here are the exact steps to create any RAG application…  Extract the relevant information from your data sources.  Break the information into the small chunks.  Store the chunks as their embedddings into a vector database.  Create a prompt template which will be fed to the LLM with the query and the context.  Convert the query to it’s relevant embedding using same embedding model.  Fetch k number of relevant documents related to the query from the vector database.  Pass the relevant documents to the LLM and get the response.FAQs      We will be using Langchain for this task, Basically it’s like a wrapper which lets you talk and manage to your LLM operations better.        Along with it we will be using Hugging Face,  it is like an open-source library for building, training, and deploying state-of-the-art machine learning models, especially about NLP. To use the HuggingFace we need the access token, Get your access token here        For our models, we’ll need two key components: a LLM (Large Language Model) and an embedding model. While paid sources like OpenAI offer these, we’ll be utilizing open-source models to ensure accessibility for everyone.        To keep things simple, our data ingestion process will involve using a URL and some PDFs. While you can incorporate additional data sources if needed, we’ll concentrate solely on these two for now.  With Langchain for the interface, Hugging Face for fetching the models, along with open-source components, we’re all set to go! This way, we save some bucks while still having everything we need. Let’s move to the next stepsEnvironment SetupOpen your favorite editor, create a python environment and install the relevant dependenciespython3 -m venv envsource env/bin/activatepip3 install dotenv langchain langchain_community Now create a .env file in the same directory to place your Hugging face api credentials like thisHUGGINGFACEHUB_API_TOKEN = hf_KKNWfBqgwCUOHdHFrBwQ.....Ensure the name “HUGGINGFACEHUB_API_TOKEN” remains unchanged, as it is crucial for authentication purposes.Finally create a data folder in the project’s root directory, designated as the central repository for storing PDF documents. You can add some sample PDFs for testing purposes; for instance, I am using the Yolo V7 and Transformers paper for demonstration. It’s important to note that this designated folder will function as our primary source for data ingestion.It seems like everything is in order, and we’re all set!Step 1 : Extracting the relevant informationTo get your RAG application running, the first thing we need to do is to extract the relevant information from the various data sources. It can be a website page, a PDF file, a notion link, a google doc whatever it is, it needs to be extracted from it’s original source first.from langchain.document_loaders import PyPDFLoaderfrom langchain_community.document_loaders import WebBaseLoader# Loading the environment variablesload_dotenv()HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")# Loading the web url and data url_loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")documents_loader = DirectoryLoader('../data/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)# Creating the instancesurl_docs = url_loader.load()data_docs = documents_loader.load()# Combining all the data that we ingesteddocs = url_docs + data_docsThis will ingest all the data from the URL link and the PDFs.Step 2 : Breaking the information into smaller chunksWe’ve all the necessary data for developing our RAG application. Now, it’s time to break down this information into smaller chunks. Later, we’ll utilize an embedding model to convert these chunks into their respective embeddings. But why it’s important?Think of it like this: If you’re tasked with digesting a 100-page book all at once and then asked a specific question about it, it would be challenging to retrieve the necessary information from the entire book to provide an answer. However, if you’re permitted to break the book into smaller, manageable chunks—let’s say 10 pages each—and each chunk is labeled with an index or numbering from 0 to 9, the process becomes much simpler. When the same question is posed after this breakdown, you can easily locate the relevant chunk based on its index and then extract the information needed to answer the question accurately.Picture the book as your extracted information, with each 10-page segment representing a small chunk of data, and the index as the embedding. Essentially, we’ll apply an embedding model to these chunks to transform the information into their respective embeddings. While as humans, we may not directly comprehend or relate to these embeddings, they serve as numeric representations of the chunks to our application.  This is how you can do this in Pythonfrom langchain.text_splitter import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)documents = text_splitter.split_documents(docs)Now the chunk_size parameter specifies the maximum number of characters that a chunk can contain, while the chunk_overlap parameter specifies the number of characters that should overlap between two adjacent chunks. With the chunk_overlap set to 50, the last 50 characters of the adjacent chunks will be shared between each other. This approach helps to prevent important information from being split across two chunks, ensuring that each chunk contains sufficient contextual information for the subsequent processing or analysis. As the shared information at the boundary of neighboring chunks enables a more seamless transition and understanding of the text’s content. The best strategy for choosing the chunk_size and chunk_overlap parameters largely depends on the nature of the documents and the purpose of the application.Step 3 : Creating the embeddings and store them into a vectordatabaseWe have two ways to get embeddings from these chunks. First, we can download a model, handle preprocessing, and do computations on our own. Or, we can use Hugging Face’s model hub. They’ve got lots of pre-trained models for different NLP tasks, including embeddings. With this approach, we’ll use one of their embedding models. We’ll just give our chunks to this model, and Hugging Face’s servers will do the hard work like preprocessing and computing. This saves us from doing it all on our own machines.We have a bunch of options for embedding models, and you can check out the leaderboard here. But for now, we’ll go with “bge-base-en-v1.5”. It’s great for making embeddings for English text, plus it’s smaller, so it loads quickly and gets the job done fast.from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings# Creating the embeddings objectembeddings = HuggingFaceInferenceAPIEmbeddings(    api_key=HF_TOKEN, model_name=\"BAAI/bge-base-en-v1.5\")Here’s a way to see the number of embeddings for each chunkquery = \"Hello I want to see the length of the embeddings for this document.\"embeddings.embed_documents([query])[0]# 768When it comes to vector databases, there are plenty of options out there. Some, like Pinecone, are paid but offer fast performance and extra features compared to open-source alternatives like FAISS or Chroma. However, if you don’t need to scale your database to handle hundreds of users fetching and storing data every minute, open-source options are more than sufficient. They work really well. So, what we’ll do is create an instance of FAISS vector database and store our embeddings in it. It’s straightforward and doesn’t require any rocket science.from langchain_community.vectorstores import FAISS# Creating a vectorstore objectvectorstore = FAISS.from_documents(documents, embeddings)Step 4 : Create a prompt template which will be fed to the LLMOk now comes the prompt template. So when you write a question to the ChatGPT and it answers that question, you are basically providing a prompt to the model so that it can understand what’s the question is. When companies train the models, they decide what kind of prompt they are going to use for invoking the model and ask the question. So for example,if you are working with “Mistral 7B instruct” and you want the optimal results it’s recommended to use the following chat template:&lt;s&gt;[INST] Instruction [/INST] Model answer&lt;/s&gt;[INST] Follow-up instruction [/INST]Note that  and  are special tokens for beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings. It’s just that the Mistral 7B instruct is made in such a way that the model look for those special tokens to understand the question better. Different types of LLMs have different kinds of instructed prompts.Now for our case we are going to use “huggingfaceh4/zephyr-7b-alpha” which is a text generation model. Just to make it clear, Zephyr-7B-α has not been aligned or formated to human preferences with techniques like RLHF (Reinforcement Learning with Human Feedback) or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so). Instead of writing a Prompt on our own, I will use ChatPromptTemplate class which creates a prompt template for the chat models. Basically, instead of writing a specified prompt I am letting ChatPromptTemplate to do it for me. Here is an example prompt template that is being generated from the manual messsages.from langchain_core.prompts import ChatPromptTemplatetemplate = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),    (\"human\", \"Hello, how are you doing?\"),    (\"ai\", \"I'm doing well, thanks!\"),    (\"human\", \"{user_input}\"),])messages = template.format_messages(    name=\"Bob\",    user_input=\"What is your name?\")If you don’t want to write the manual instructions, you can just use the from_template function to generate a more generic prompt template which I used for this project. Here it is..from langchain_core.prompts import ChatPromptTemplatetemplate = \"\"\"{query}\"\"\"prompt = ChatPromptTemplate.from_template(template)Our prompt is set! We’ve crafted a single message, assuming it’s from a human or you xD . If you’re not using the from_messages function, the ChatPromptTemplate will ensure your prompt works seamlessly with the language model by reserving some additional system messages. While there’s always room for improvement with more generic prompts to achieve better results, this setup should work for now!Step 5 : Convert the query to it’s relevant embedding using same embedding model.Now, let’s talk about the query or question we want to ask our RAG application. We can’t just pass the query to our model and expect information in return. Instead, we need to pass the query through the same embedding model used for the chunks earlier. Why is this important? Well, by embedding queries, we allow models to compare them efficiently with previously processed chunks of text. This enables tasks like finding similar documents or generating relevant responses. It’s like translating language into a language computers understand.It’s like translating language into a language computers understand. Imagine you’re an English speaker and your friend speaks Hindi. Neither of you understands each other’s language. You hand your English-speaking friend a one-page document written in Hindi. Your friend has to translate that document into English before they can understand its content. Now, if you ask a question in Hindi related to that document, your friend has to translate the question into English first to understand it and find a relevant response from the information you shared earlier. In this scenario, your friend acts like an embedding model. They converted your previous texts into embeddings. Now, when you ask a query or question, it will be translated into the relevant embeddings using the same embedding model used for the chunks. Then, a search operation will be performed to find the relevant response to your query. I hope this clarifies why we converted our query into embeddings first.That being said, any query or a question that you want to ask will first be collectilevy used for creating a generic prompt then the whole piece of text will be embedded using the same embedding model that you used earlier for the chunks. As the embedding is done, we can process furtherStep 6 : Fetch K number of documents.Now, let’s talk about the retriever. Its job is to dive into the vector database and perform a search to find relevant documents. It returns a set number, let’s call it “k”, of these documents, which are ranked based on their contextual relevance to the query or question you asked. You can set “k” as a parameter, indicating how many relevant documents you want - whether it’s 2, 5, or 10. Generally, if you have a smaller amount of data, it’s best to stick with a lower “k”, around 2. For longer documents or larger datasets, a “k” between 10 and 20 is recommended.Different search techniques can be employed to fetch relevant documents more effectively and quickly from a vector database. The choice depends on various factors such as your specific use case, the amount of data you have, what kind of vector database you are using and the context of your problem.retriever = vectorstore.as_retriever(search_type = \"mmr\", search_kwargs={\"k\": 3})docs = retriever.get_relevant_documents(\"what did you know about Yolo V7?\")print(docs)When you run this code, the retriever will fetch 3 most most relevant documents from the vector database based on the mmr search criteria. All these documents will be the contexts for our LLM model to generate the response for our query.Step 7 : Pass the relevant documents to the LLM and get the response.So far, we’ve asked our retriever to fetch a set number of relevant documents from the database. Now, we need a language model (LLM) to generate a relevant response based on that context. To ensure robustness, let’s remember that at the beginning of this blog, I mentioned that LLMs like ChatGPT can sometimes generate irrelevant responses, especially when asked about specific use cases or contexts. However, this time, we’re providing the context from our own data to the LLM as a reference. So, it will consider this reference along with its general capabilities to answer the question. That’s the whole idea behind using RAG!Now, let’s dive into implementing the language model (LLM) aspect of our RAG setup. We’ll be using a powerful model architecture from the Hugging Face Hub. Here’s how we do it in Python:from langchain.llms import HuggingFaceHub# Model architecturemodel =  HuggingFaceHub(    repo_id=\"huggingfaceh4/zephyr-7b-alpha\",    model_kwargs={\"temperature\": 0.5, \"max_length\": 4096 ,\"max_new_tokens\": 2048 })In this code snippet, we’re instantiating our language model using the Hugging Face Hub. Specifically, we’re selecting the Zephyr 7 billion model which is placed in this repository ID “huggingfaceh4/zephyr-7b-alpha”. Choice of choosing this model isn’t arbitrary; it’s based on the model’s suitability for our specific task and requirements. As we are already implementing only Open Source components, Zephyr 7 billion works good enough to generate the useful response with minimal overhead and low latency.This model comes with some additional parameters to fine-tune its behavior. We’ve set the temperature to 0.5, which controls the randomness of the generated text. As a lower temperature tends to result in more conservative and predictable outputs and    when the temperature is set to max which is 1, the model tries to be as much creative as it could, so based on what type of output you want for your use case, you can tweak this parameter. For the sake of the simplicity and demonstration purposes, I set it to 0.5 to make sure we get decent results. Next is max_length parameter, it defines the maximum length of the generated text which includes the size of your prompt as well as the response, and max_new_tokens which sets the maximum number of new tokens that can be generated. As a general rule of thumb, the max_new_tokens should always be less than or equal to the max_length parameter. Why? Think about it..Step 8 : Create a chain for invoking the LLM.We have everything we want for our RAG application, last thing we need to do is to create a chain for invoking the LLM on our query to generate the response. There are different types of chains for the different types of use cases, if you like your LLM to remember the context of the chat over the time like the ChatGPT , you would need a memory instance which can be shared among multiple conversation pieces, for such cases, there are conversational chains available.For now we just need a chain which can combine our retrieved contexts and pass it with the query to the LLM to generate the response.rag_chain = (    {\"context\": retriever,  \"query\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())We have our Prompt, model, context and the query! All of them are combined into a single chain. It’s pretty much what all the chains does! Now before running the final code, I want to give a quick check on these two helper functions: RunnablePassthrough() and StrOutputParser().The RunnablePassthrough class in LangChain serves to pass inputs unchanged or with additional keys. In our chain, a prompt expects input in the form of a map with keys “context” and “question.” However, user input only includes the “question.” or the “query”.  Here, RunnablePassthrough is utilized to pass the user’s question under the “question” key while retrieving the context using a retriever. It just ensures that the input to the prompt conforms to the expected format.Secondally, StrOutputParser is typically employed in RAG chains to parse the output of the model into a human-readable string. In the layman terms, It is responsible for transforming the model’s output into a more coherent and grammatically correct sentence, which is generally better readable by Humans! That’s it!D-DayTo make sure we get the entire idea even if the response gets cut off, I’ve implemented a function called get_complete_sentence(). Basically this function helps extract the last complete sentence from the text. So, even if the response hits the maximum token limit that we set upon and it gets truncated midway, we will still get a coherent understanding of the message.For practical testing, I suggest storing some low sized PDFs in the data folder of your project. You can choose PDFs related to various topics or domains that you want the chatbot to interact with. Additionally, providing a URL as a reference for the chatbot can be helpful for testing. For example, you could use a Wikipedia page, a research paper, or any other online document relevant to your testing goals. During my testing, I used a URL containing information about Jon Snow from Game of Thrones,  and PDFs of Transformers paper, and the YOLO V7 paper to evaluate the bot’s performance. Let’s see how our bot performs in varied content.import osfrom dotenv import load_dotenvfrom langchain.llms import HuggingFaceHubfrom langchain.embeddings import HuggingFaceInferenceAPIEmbeddingsfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain_community.vectorstores import FAISSfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain.document_loaders import PyPDFLoaderfrom langchain.document_loaders import DirectoryLoader# Loading the environment variablesload_dotenv()HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")# Loading the web url and breaking down the information into chunksloader = WebBaseLoader(\"https://gameofthrones.fandom.com/wiki/Jon_Snow\")documents_loader = DirectoryLoader('../data/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)# URL loaderurl_docs = loader.load()# Document loaderdata_docs = documents_loader.load()# Combining all the information into single variabledocs = url_docs + data_docstext_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)documents = text_splitter.split_documents(docs)# Creating the embeddings objectembeddings = HuggingFaceInferenceAPIEmbeddings(    api_key=HF_TOKEN, model_name=\"BAAI/bge-base-en-v1.5\")# Creating a vectorstore objectvectorstore = FAISS.from_documents(documents, embeddings)# Defining a retriever retriever = vectorstore.as_retriever(search_type = \"mmr\", search_kwargs={\"k\": 3})# Model architecturemodel =  HuggingFaceHub(    repo_id=\"huggingfaceh4/zephyr-7b-alpha\",    model_kwargs={\"temperature\": 0.5, \"max_length\": 4096 ,\"max_new_tokens\": 2048 })template = \"\"\"Question : {query}\"\"\"prompt = ChatPromptTemplate.from_template(template)rag_chain = (    {\"context\": retriever,  \"query\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())def get_complete_sentence(response):    \"\"\"    Extracts the last complete sentence from a given text.        Args: response (str): The input text from which the last complete     sentence will be extracted.        Returns: str: The last complete sentence found in the input text.     If no complete sentence is found, returns the entire input text.    \"\"\"    # Find the last occurrence of a period (.)    last_period_index = response.rfind('.')     # If a period is found, return the text up to that point    if last_period_index != -1: return response[:last_period_index + 1]          # If no period is found, return the entire response    else: return response          # Invoke the RAG chain and retrieve the responseresponse = rag_chain.invoke(\"Who killed Jon Snow?\")# Get the complete sentencecomplete_sentence = get_complete_sentence(response)print(complete_sentence)This is the response I received after 10 seconds, which is quite good. The time it takes can vary depending on your system’s configuration, but I believe you’ll get decent results in just a few minutes. So, please be patient if it’s taking a bit longer.Human: Question : Who killed Jon Snow?Answer: In the TV series Game of Thrones, Jon Snow was stabbed by his fellow Night's Watch members in season 5, episode 9, \"The Dance of Dragons.\" However, he was later resurrected by Melisandre in season 6, episode 3, \"Oathbreaker.\" So, technically, no one killed Jon Snow in the show.Have fun experimenting with various data sources! You can try changing the website addresses, adding new PDF files or maybe change the template a bit. LLMs are fun, you never know what you get!What’s next?There are plenty of things we can adjust here. We could switch to a more effective embedding model for better indexing, try different searching techniques for the retriever, add a reranker to improve the ranking of documents, or use a more advanced LLM with a larger context window and faster response times. Essentially, every RAG application is just an enhanced version based on these factors. However, the fundamental concept of how RAG applications function always stays the same.",
            "content_html": "<p><em>If you’ve ever thought about creating a custom bot for your documents or website that interacts based on specific data, you’re in the right place. I’m here to assist you in developing a bot that leverages Langchain and RAG strategies for this purpose.</em></p><h3 id=\"understanding-the-limitations-of-chatgpt-and-llms\">Understanding the Limitations of ChatGPT and LLMs</h3><p>ChatGPTs and other Large Language Models (LLMs) are extensively trained on text corpora to comprehend language semantics and coherence. Despite their impressive capabilities, these models have limitations that require careful consideration for particular use cases. One significant challenge is the potential for hallucinations, where the model might generate inaccurate or contextually irrelevant information.</p><p>Imagine requesting the model to enhance your company policies; in such scenarios, ChatGPTs and other Large Language Models might struggle to provide factual responses because they lack training on your company’s specific policies. Instead, they may generate nonsensical or irrelevant responses, which can be unhelpful. So, how can we ensure that an LLM comprehends our specific data and generates responses accordingly? This is where techniques like Retrieval Augmentation Generation (RAG) come to the rescue.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/_images/2024-01-23-rag-application-with-langchain/rag1.png?raw=true\" alt=\"RAG\" /></p><h3 id=\"what-is-rag\">What is RAG?</h3><p>RAG or Retrieval Augmented Generation uses three main workflows to generate and give the better response</p><ul>  <li>    <p>Information Retrieval: When a user asks a question, the AI system retrieves the relevant data from a well-maintained knowledge library or external sources like databases, articles, APIs, or document repositories. This is achieved by converting the query into a numerical format or vector that can be understood by machines.</p>  </li>  <li>    <p>LLM: The retrieved data is then presented to the LLM or Large Language Model, along with the user’s query. The LLM uses this new knowledge and its training data to generate the response.</p>  </li>  <li>    <p>Response: Finally, the LLM generates a response that is more accurate and relevant since it has been augmented with the retrieved information. I mean we gave LLM some additional information from our Knowledge library which allows LLMs to provide more contextually relevant and factual responses, solving the problem of models when they are just hallucinating or providing irrelevant answer.</p>  </li></ul><p>Let’s take an example of company policies again. Suppose you have an HR bot that handles queries related to your Company policies. Now if someones asks anything specific to the policies, The bot can pull the most recent policy documents from the knowledge libray, pass the relevant context to a well crafted prompt which is then passed further to the LLM for generating the response.</p><p>To make it more easy, Imagine a LLM as your knowledgeable friend who seems to know everything, from Geography to Computer Science from Politics to Philosophy. Now, picture yourself asking this friend a few questions:</p><ul>  <li>“Who handles my laundry on weekends?”</li>  <li>“Who lives next door to me?”</li>  <li>“What brand of peanut butter do I prefer?”</li></ul><p>Chances are, your friend wouldn’t be able to answer these questions, right? Most of the time, no.But let’s say this distant friend becomes closer to you over time, he comes at your place regularly, know your parents very well, you both hangout pretty often, you go on outings, blah blah blah.. You got the point right? I mean he is gaining access to personal and insider information about you.  Now, when you pose the same questions, they can somehow answer those question with more relevance now because because he is better suited with your personal insights.</p><p>Similarly, an LLM, when provided with additional information or access to your use case data, won’t guess or hallucinate. Instead, it will leverage that data to provide relevant and accurate answers.</p><h3 id=\"to-break-it-down-here-are-the-exact-steps-to-create-any-rag-application\">To break it down, here are the exact steps to create any RAG application…</h3><ol>  <li>Extract the relevant information from your data sources.</li>  <li>Break the information into the small chunks.</li>  <li>Store the chunks as their embedddings into a vector database.</li>  <li>Create a prompt template which will be fed to the LLM with the query and the context.</li>  <li>Convert the query to it’s relevant embedding using same embedding model.</li>  <li>Fetch k number of relevant documents related to the query from the vector database.</li>  <li>Pass the relevant documents to the LLM and get the response.</li></ol><h3 id=\"faqs\">FAQs</h3><ol>  <li>    <p>We will be using <a href=\"https://python.langchain.com/docs/get_started/introduction\">Langchain</a> for this task, Basically it’s like a wrapper which lets you talk and manage to your LLM operations better.</p>  </li>  <li>    <p>Along with it we will be using <a href=\"https://huggingface.co/\">Hugging Face</a>,  it is like an open-source library for building, training, and deploying state-of-the-art machine learning models, especially about NLP. To use the HuggingFace we need the access token, Get your access token <a href=\"https://huggingface.co/docs/hub/security-tokens\">here</a></p>  </li>  <li>    <p>For our models, we’ll need two key components: a LLM (Large Language Model) and an embedding model. While paid sources like OpenAI offer these, we’ll be utilizing open-source models to ensure accessibility for everyone.</p>  </li>  <li>    <p>To keep things simple, our data ingestion process will involve using a URL and some PDFs. While you can incorporate additional data sources if needed, we’ll concentrate solely on these two for now.</p>  </li></ol><p>With Langchain for the interface, Hugging Face for fetching the models, along with open-source components, we’re all set to go! This way, we save some bucks while still having everything we need. Let’s move to the next steps</p><h3 id=\"environment-setup\">Environment Setup</h3><p>Open your favorite editor, create a python environment and install the relevant dependencies</p><pre><code class=\"language-python\">python3 -m venv envsource env/bin/activatepip3 install dotenv langchain langchain_community </code></pre><p>Now create a .env file in the same directory to place your Hugging face api credentials like this</p><pre><code class=\"language-python\">HUGGINGFACEHUB_API_TOKEN = hf_KKNWfBqgwCUOHdHFrBwQ.....</code></pre><p>Ensure the name “HUGGINGFACEHUB_API_TOKEN” remains unchanged, as it is crucial for authentication purposes.</p><p>Finally create a data folder in the project’s root directory, designated as the central repository for storing PDF documents. You can add some sample PDFs for testing purposes; for instance, I am using the <a href=\"https://arxiv.org/pdf/2207.02696.pdf\">Yolo V7</a> and <a href=\"https://arxiv.org/abs/1706.03762\">Transformers</a> paper for demonstration. It’s important to note that this designated folder will function as our primary source for data ingestion.</p><p>It seems like everything is in order, and we’re all set!</p><h3 id=\"step-1--extracting-the-relevant-information\">Step 1 : Extracting the relevant information</h3><p>To get your RAG application running, the first thing we need to do is to extract the relevant information from the various data sources. It can be a website page, a PDF file, a notion link, a google doc whatever it is, it needs to be extracted from it’s original source first.</p><pre><code class=\"language-python\">from langchain.document_loaders import PyPDFLoaderfrom langchain_community.document_loaders import WebBaseLoader# Loading the environment variablesload_dotenv()HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")# Loading the web url and data url_loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")documents_loader = DirectoryLoader('../data/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)# Creating the instancesurl_docs = url_loader.load()data_docs = documents_loader.load()# Combining all the data that we ingesteddocs = url_docs + data_docs</code></pre><p>This will ingest all the data from the URL link and the PDFs.</p><h3 id=\"step-2--breaking-the-information-into-smaller-chunks\">Step 2 : Breaking the information into smaller chunks</h3><p>We’ve all the necessary data for developing our RAG application. Now, it’s time to break down this information into smaller chunks. Later, we’ll utilize an embedding model to convert these chunks into their respective embeddings. But why it’s important?</p><p>Think of it like this: If you’re tasked with digesting a 100-page book all at once and then asked a specific question about it, it would be challenging to retrieve the necessary information from the entire book to provide an answer. However, if you’re permitted to break the book into smaller, manageable chunks—let’s say 10 pages each—and each chunk is labeled with an index or numbering from 0 to 9, the process becomes much simpler. When the same question is posed after this breakdown, you can easily locate the relevant chunk based on its index and then extract the information needed to answer the question accurately.</p><p>Picture the book as your extracted information, with each 10-page segment representing a small chunk of data, and the index as the embedding. Essentially, we’ll apply an embedding model to these chunks to transform the information into their respective embeddings. While as humans, we may not directly comprehend or relate to these embeddings, they serve as numeric representations of the chunks to our application.  This is how you can do this in Python</p><pre><code class=\"language-python\">from langchain.text_splitter import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)documents = text_splitter.split_documents(docs)</code></pre><p>Now the chunk_size parameter specifies the maximum number of characters that a chunk can contain, while the chunk_overlap parameter specifies the number of characters that should overlap between two adjacent chunks. With the chunk_overlap set to 50, the last 50 characters of the adjacent chunks will be shared between each other. This approach helps to prevent important information from being split across two chunks, ensuring that each chunk contains sufficient contextual information for the subsequent processing or analysis. As the shared information at the boundary of neighboring chunks enables a more seamless transition and understanding of the text’s content. The best strategy for choosing the chunk_size and chunk_overlap parameters largely depends on the nature of the documents and the purpose of the application.</p><h3 id=\"step-3--creating-the-embeddings-and-store-them-into-a-vectordatabase\">Step 3 : Creating the embeddings and store them into a vectordatabase</h3><p>We have two ways to get embeddings from these chunks. First, we can download a model, handle preprocessing, and do computations on our own. Or, we can use Hugging Face’s model hub. They’ve got lots of pre-trained models for different NLP tasks, including embeddings. With this approach, we’ll use one of their embedding models. We’ll just give our chunks to this model, and Hugging Face’s servers will do the hard work like preprocessing and computing. This saves us from doing it all on our own machines.</p><p>We have a bunch of options for embedding models, and you can check out the leaderboard <a href=\"https://huggingface.co/spaces/mteb/leaderboard\">here</a>. But for now, we’ll go with “bge-base-en-v1.5”. It’s great for making embeddings for English text, plus it’s smaller, so it loads quickly and gets the job done fast.</p><pre><code class=\"language-python\">from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings# Creating the embeddings objectembeddings = HuggingFaceInferenceAPIEmbeddings(    api_key=HF_TOKEN, model_name=\"BAAI/bge-base-en-v1.5\")</code></pre><p>Here’s a way to see the number of embeddings for each chunk</p><pre><code class=\"language-python\">query = \"Hello I want to see the length of the embeddings for this document.\"embeddings.embed_documents([query])[0]# 768</code></pre><p>When it comes to vector databases, there are plenty of options out there. Some, like Pinecone, are paid but offer fast performance and extra features compared to open-source alternatives like FAISS or Chroma. However, if you don’t need to scale your database to handle hundreds of users fetching and storing data every minute, open-source options are more than sufficient. They work really well. So, what we’ll do is create an instance of FAISS vector database and store our embeddings in it. It’s straightforward and doesn’t require any rocket science.</p><pre><code class=\"language-python\">from langchain_community.vectorstores import FAISS# Creating a vectorstore objectvectorstore = FAISS.from_documents(documents, embeddings)</code></pre><h3 id=\"step-4--create-a-prompt-template-which-will-be-fed-to-the-llm\">Step 4 : Create a prompt template which will be fed to the LLM</h3><p>Ok now comes the prompt template. So when you write a question to the ChatGPT and it answers that question, you are basically providing a prompt to the model so that it can understand what’s the question is. When companies train the models, they decide what kind of prompt they are going to use for invoking the model and ask the question. So for example,if you are working with “Mistral 7B instruct” and you want the optimal results it’s recommended to use the following chat template:</p><pre><code class=\"language-python\">&lt;s&gt;[INST] Instruction [/INST] Model answer&lt;/s&gt;[INST] Follow-up instruction [/INST]</code></pre><p>Note that <s> and </s> are special tokens for beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings. It’s just that the Mistral 7B instruct is made in such a way that the model look for those special tokens to understand the question better. Different types of LLMs have different kinds of instructed prompts.</p><p>Now for our case we are going to use “huggingfaceh4/zephyr-7b-alpha” which is a text generation model. Just to make it clear, Zephyr-7B-α has not been aligned or formated to human preferences with techniques like RLHF (Reinforcement Learning with Human Feedback) or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so). Instead of writing a Prompt on our own, I will use ChatPromptTemplate class which creates a prompt template for the chat models. Basically, instead of writing a specified prompt I am letting ChatPromptTemplate to do it for me. Here is an example prompt template that is being generated from the manual messsages.</p><pre><code class=\"language-python\">from langchain_core.prompts import ChatPromptTemplatetemplate = ChatPromptTemplate.from_messages([    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),    (\"human\", \"Hello, how are you doing?\"),    (\"ai\", \"I'm doing well, thanks!\"),    (\"human\", \"{user_input}\"),])messages = template.format_messages(    name=\"Bob\",    user_input=\"What is your name?\")</code></pre><p>If you don’t want to write the manual instructions, you can just use the <em>from_template</em> function to generate a more generic prompt template which I used for this project. Here it is..</p><pre><code class=\"language-python\">from langchain_core.prompts import ChatPromptTemplatetemplate = \"\"\"{query}\"\"\"prompt = ChatPromptTemplate.from_template(template)</code></pre><p>Our prompt is set! We’ve crafted a single message, assuming it’s from a human or you xD . If you’re not using the from_messages function, the ChatPromptTemplate will ensure your prompt works seamlessly with the language model by reserving some additional system messages. While there’s always room for improvement with more generic prompts to achieve better results, this setup should work for now!</p><h3 id=\"step-5--convert-the-query-to-its-relevant-embedding-using-same-embedding-model\">Step 5 : Convert the query to it’s relevant embedding using same embedding model.</h3><p>Now, let’s talk about the query or question we want to ask our RAG application. We can’t just pass the query to our model and expect information in return. Instead, we need to pass the query through the same embedding model used for the chunks earlier. Why is this important? Well, by embedding queries, we allow models to compare them efficiently with previously processed chunks of text. This enables tasks like finding similar documents or generating relevant responses. It’s like translating language into a language computers understand.</p><p>It’s like translating language into a language computers understand. Imagine you’re an English speaker and your friend speaks Hindi. Neither of you understands each other’s language. You hand your English-speaking friend a one-page document written in Hindi. Your friend has to translate that document into English before they can understand its content. Now, if you ask a question in Hindi related to that document, your friend has to translate the question into English first to understand it and find a relevant response from the information you shared earlier. In this scenario, your friend acts like an embedding model. They converted your previous texts into embeddings. Now, when you ask a query or question, it will be translated into the relevant embeddings using the same embedding model used for the chunks. Then, a search operation will be performed to find the relevant response to your query. I hope this clarifies why we converted our query into embeddings first.</p><p>That being said, any query or a question that you want to ask will first be collectilevy used for creating a generic prompt then the whole piece of text will be embedded using the same embedding model that you used earlier for the chunks. As the embedding is done, we can process further</p><h3 id=\"step-6--fetch-k-number-of-documents\">Step 6 : Fetch K number of documents.</h3><p>Now, let’s talk about the retriever. Its job is to dive into the vector database and perform a search to find relevant documents. It returns a set number, let’s call it “k”, of these documents, which are ranked based on their contextual relevance to the query or question you asked. You can set “k” as a parameter, indicating how many relevant documents you want - whether it’s 2, 5, or 10. Generally, if you have a smaller amount of data, it’s best to stick with a lower “k”, around 2. For longer documents or larger datasets, a “k” between 10 and 20 is recommended.</p><p>Different <a href=\"https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore\">search techniques</a> can be employed to fetch relevant documents more effectively and quickly from a vector database. The choice depends on various factors such as your specific use case, the amount of data you have, what kind of vector database you are using and the context of your problem.</p><pre><code class=\"language-python\">retriever = vectorstore.as_retriever(search_type = \"mmr\", search_kwargs={\"k\": 3})docs = retriever.get_relevant_documents(\"what did you know about Yolo V7?\")print(docs)</code></pre><p>When you run this code, the retriever will fetch 3 most most relevant documents from the vector database based on the mmr search criteria. All these documents will be the contexts for our LLM model to generate the response for our query.</p><h3 id=\"step-7--pass-the-relevant-documents-to-the-llm-and-get-the-response\">Step 7 : Pass the relevant documents to the LLM and get the response.</h3><p>So far, we’ve asked our retriever to fetch a set number of relevant documents from the database. Now, we need a language model (LLM) to generate a relevant response based on that context. To ensure robustness, let’s remember that at the beginning of this blog, I mentioned that LLMs like ChatGPT can sometimes generate irrelevant responses, especially when asked about specific use cases or contexts. However, this time, we’re providing the context from our own data to the LLM as a reference. So, it will consider this reference along with its general capabilities to answer the question. That’s the whole idea behind using RAG!</p><p>Now, let’s dive into implementing the language model (LLM) aspect of our RAG setup. We’ll be using a powerful model architecture from the Hugging Face Hub. Here’s how we do it in Python:</p><pre><code class=\"language-python\">from langchain.llms import HuggingFaceHub# Model architecturemodel =  HuggingFaceHub(    repo_id=\"huggingfaceh4/zephyr-7b-alpha\",    model_kwargs={\"temperature\": 0.5, \"max_length\": 4096 ,\"max_new_tokens\": 2048 })</code></pre><p>In this code snippet, we’re instantiating our language model using the Hugging Face Hub. Specifically, we’re selecting the Zephyr 7 billion model which is placed in this repository ID “huggingfaceh4/zephyr-7b-alpha”. Choice of choosing this model isn’t arbitrary; it’s based on the model’s suitability for our specific task and requirements. As we are already implementing only Open Source components, Zephyr 7 billion works good enough to generate the useful response with minimal overhead and low latency.</p><p>This model comes with some additional parameters to fine-tune its behavior. We’ve set the temperature to 0.5, which controls the randomness of the generated text. As a lower temperature tends to result in more conservative and predictable outputs and    when the temperature is set to max which is 1, the model tries to be as much creative as it could, so based on what type of output you want for your use case, you can tweak this parameter. For the sake of the simplicity and demonstration purposes, I set it to 0.5 to make sure we get decent results. Next is max_length parameter, it defines the maximum length of the generated text which includes the size of your prompt as well as the response, and max_new_tokens which sets the maximum number of new tokens that can be generated. As a general rule of thumb, the max_new_tokens should always be less than or equal to the max_length parameter. Why? Think about it..</p><h3 id=\"step-8--create-a-chain-for-invoking-the-llm\">Step 8 : Create a chain for invoking the LLM.</h3><p>We have everything we want for our RAG application, last thing we need to do is to create a chain for invoking the LLM on our query to generate the response. There are different types of chains for the different types of use cases, if you like your LLM to remember the context of the chat over the time like the ChatGPT , you would need a memory instance which can be shared among multiple conversation pieces, for such cases, there are conversational chains available.</p><p>For now we just need a chain which can combine our retrieved contexts and pass it with the query to the LLM to generate the response.</p><pre><code class=\"language-python\">rag_chain = (    {\"context\": retriever,  \"query\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())</code></pre><p>We have our Prompt, model, context and the query! All of them are combined into a single chain. It’s pretty much what all the chains does! Now before running the final code, I want to give a quick check on these two helper functions: <code>RunnablePassthrough()</code> and <code>StrOutputParser()</code>.</p><p>The <code>RunnablePassthrough</code> class in <code>LangChain</code> serves to pass inputs unchanged or with additional keys. In our chain, a prompt expects input in the form of a map with keys “context” and “question.” However, user input only includes the “question.” or the “query”.  Here, <code>RunnablePassthrough</code> is utilized to pass the user’s question under the “question” key while retrieving the context using a retriever. It just ensures that the input to the prompt conforms to the expected format.</p><p>Secondally, <code>StrOutputParser</code> is typically employed in RAG chains to parse the output of the model into a human-readable string. In the layman terms, It is responsible for transforming the model’s output into a more coherent and grammatically correct sentence, which is generally better readable by Humans! That’s it!</p><h3 id=\"d-day\">D-Day</h3><p>To make sure we get the entire idea even if the response gets cut off, I’ve implemented a function called <code>get_complete_sentence()</code>. Basically this function helps extract the last complete sentence from the text. So, even if the response hits the maximum token limit that we set upon and it gets truncated midway, we will still get a coherent understanding of the message.</p><p>For practical testing, I suggest storing some low sized PDFs in the data folder of your project. You can choose PDFs related to various topics or domains that you want the chatbot to interact with. Additionally, providing a URL as a reference for the chatbot can be helpful for testing. For example, you could use a Wikipedia page, a research paper, or any other online document relevant to your testing goals. During my testing, I used a URL containing information about Jon Snow from Game of Thrones,  and PDFs of Transformers paper, and the YOLO V7 paper to evaluate the bot’s performance. Let’s see how our bot performs in varied content.</p><pre><code class=\"language-python\">import osfrom dotenv import load_dotenvfrom langchain.llms import HuggingFaceHubfrom langchain.embeddings import HuggingFaceInferenceAPIEmbeddingsfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain_community.vectorstores import FAISSfrom langchain_community.document_loaders import WebBaseLoaderfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom langchain.document_loaders import PyPDFLoaderfrom langchain.document_loaders import DirectoryLoader# Loading the environment variablesload_dotenv()HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")# Loading the web url and breaking down the information into chunksloader = WebBaseLoader(\"https://gameofthrones.fandom.com/wiki/Jon_Snow\")documents_loader = DirectoryLoader('../data/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)# URL loaderurl_docs = loader.load()# Document loaderdata_docs = documents_loader.load()# Combining all the information into single variabledocs = url_docs + data_docstext_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)documents = text_splitter.split_documents(docs)# Creating the embeddings objectembeddings = HuggingFaceInferenceAPIEmbeddings(    api_key=HF_TOKEN, model_name=\"BAAI/bge-base-en-v1.5\")# Creating a vectorstore objectvectorstore = FAISS.from_documents(documents, embeddings)# Defining a retriever retriever = vectorstore.as_retriever(search_type = \"mmr\", search_kwargs={\"k\": 3})# Model architecturemodel =  HuggingFaceHub(    repo_id=\"huggingfaceh4/zephyr-7b-alpha\",    model_kwargs={\"temperature\": 0.5, \"max_length\": 4096 ,\"max_new_tokens\": 2048 })template = \"\"\"Question : {query}\"\"\"prompt = ChatPromptTemplate.from_template(template)rag_chain = (    {\"context\": retriever,  \"query\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())def get_complete_sentence(response):    \"\"\"    Extracts the last complete sentence from a given text.        Args: response (str): The input text from which the last complete     sentence will be extracted.        Returns: str: The last complete sentence found in the input text.     If no complete sentence is found, returns the entire input text.    \"\"\"    # Find the last occurrence of a period (.)    last_period_index = response.rfind('.')     # If a period is found, return the text up to that point    if last_period_index != -1: return response[:last_period_index + 1]          # If no period is found, return the entire response    else: return response          # Invoke the RAG chain and retrieve the responseresponse = rag_chain.invoke(\"Who killed Jon Snow?\")# Get the complete sentencecomplete_sentence = get_complete_sentence(response)print(complete_sentence)</code></pre><p>This is the response I received after 10 seconds, which is quite good. The time it takes can vary depending on your system’s configuration, but I believe you’ll get decent results in just a few minutes. So, please be patient if it’s taking a bit longer.</p><pre><code class=\"language-python\">Human: Question : Who killed Jon Snow?Answer: In the TV series Game of Thrones, Jon Snow was stabbed by his fellow Night's Watch members in season 5, episode 9, \"The Dance of Dragons.\" However, he was later resurrected by Melisandre in season 6, episode 3, \"Oathbreaker.\" So, technically, no one killed Jon Snow in the show.</code></pre><p>Have fun experimenting with various data sources! You can try changing the website addresses, adding new PDF files or maybe change the template a bit. LLMs are fun, you never know what you get!</p><h3 id=\"whats-next\">What’s next?</h3><p>There are plenty of things we can adjust here. We could switch to a more effective embedding model for better indexing, try different searching techniques for the retriever, add a reranker to improve the ranking of documents, or use a more advanced LLM with a larger context window and faster response times. Essentially, every RAG application is just an enhanced version based on these factors. However, the fundamental concept of how RAG applications function always stays the same.</p>",
            "url": "http://localhost:4000/2024/01/23/rag-application-with-langchain",
            
            
            
            "tags": ["LLM","Deep Learning"],
            
            "date_published": "2024-01-23T00:00:00+05:30",
            "date_modified": "2024-01-23T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/01/14/ml-fine-print",
            "title": "ML Fine Print",
            "summary": "Take a look at the fonts in this post that can make your text editor or terminal emulator look little bit nicer",
            "content_text": "ML Fine Print is my essential resource for recalling and understanding crucial details in the ML software cycle which often overlooked in the heat of real-world challenges. The documentation is designed for easy reference without a specific reading order.Overfitting and Regularization      Overfitting occurs when our model becomes too complex, fitting too closely to the noise in the training data. While it might perform well during training, it often falters during testing or real-world applications. This happens when our model gets too attuned to the specific variations in the training data, as it starts to memorize the data instead of capturing the relevant patterns in it, resulting in poor generalization to new, unseen data. A model with low bias and high variance is considered overfit and doesn’t perform well with new data.        Regularization steps comes in as a solution, introducing a penalty for excessive model complexity to curb overfitting. Techniques like Early Stopping, L2, and L1 regularization help strike a balance between fitting the training data and keeping the model reasonably simple.        Early Stopping involves halting training when a threshold of convergence is reached or when the validation loss starts to rise, preventing overfitting by avoiding excessive training. On the other hand, L2 regularization ensures that weights of various features don’t overlap excessively, as one feature weights shouldn’t overperform over the others promoting a balance. While the L1 regularization focuses on sparsity, driving some weights to absolute zero, doing so, it emphasize on keeping the informative features only.        Choosing the right lambda value in regularization is crucial. Too high, and your model may underfit; too low, and it may overfit. It’s a delicate tradeoff between simplicity and training data fit. The ideal lambda value is data-dependent, requiring hyperparameter tuning for optimal results. Despite potential increases in training loss, regularization often improves real-time predictions during inference, emphasizing the importance of overall model performance. Overall, regularization acts as a “keep things reasonable” rule for models, preventing extremes and ensuring their ability to make accurate predictions for new, unseen data.  Assumptions for Data SamplingIn data sampling, we often make certain assumptions to ensure the validity and reliability of our results. These assumptions include:      Independent and Identically Distributed (i.i.d.): Examples are drawn independently and identically from the same distribution. This means that the probability of selecting any particular example is the same for all examples, and the selection of one example does not influence the selection of any other example.        Stationarity: The distribution of the data does not change over time or across different parts of the data set. This means that the probability of observing a particular value or outcome is the same regardless of when or where in the data set it is observed        Same Distribution: All examples are drawn from the same distribution. This means that the underlying process that generates the data is the same for all examples.  Violations in Practice:      Non-Independence: In some cases, the i.i.d. assumption may be violated due to dependencies between examples. For example, in a model that chooses ads to display, the choice of ads may be influenced by the user’s previous ad history, creating a temporal relationship between observations.        Non-Stationarity: The stationarity assumption may be violated if the underlying distribution of the data changes over time or across different parts of the data set. For example, in a data set of retail sales information, user purchases may change seasonally, violating stationarity.        Different Distributions: The same distribution assumption may be violated if examples are drawn from different distributions. For example, in a data set of customer reviews, the distribution of reviews may differ between different products or services.  Test and Training Set      In machine learning, splitting the data into test and training sets is a crucial step. Randomization of the data before splitting is essential to ensure that the model does not train on a biased subset of the data. For instance, we would not want our Climate Predictor model to train solely on summer data and then be used for inference on test data consisting exclusively of winter data.        When dealing with a large dataset containing billions of sample points, a small percentage (5-10%) of the data can be sufficient for testing the model during inference. However, if the dataset is relatively small, alternative methods like cross-validation may be necessary for better results.        It is important to note that the test set should never be exposed to the model during training time. Repeated evaluation on the same test set can lead to implicit overfitting, which reduces the model’s ability to generalize to new, unseen data.        Furthermore, it is essential to shuffle the training dataset before creating a validation split. This is because sometimes, no matter how the training set and validation set are split, the loss curves may differ significantly. This issue is often caused by dissimilarities in the data between the training set and the validation set. Most libraries, including Pandas, split the dataset sequentially, which can lead to problems if the dataset points are arranged in a specific order. Therefore, it is recommended to randomize or shuffle the dataset points before splitting, ensuring that both the training and validation sets have an equivalent distribution of dataset points.  Validation Set      To avoid overfitting to a specific test set, it is advisable to use a validation set for tuning the model’s hyperparameters. This allows for more objective evaluation of the model’s performance and reduces the risk of implicit overfitting.        Using the same data set for testing and validation can limit the potential for tuning the hyperparameters or improving the model. It is beneficial to continuously acquire more data to refresh the test and validation sets, ensuring a more comprehensive evaluation of the model’s performance.        It is important to note that the internal model parameters are adjusted during the training process, while the hyperparameters are tuned based on the results obtained from the validation and test sets.  Convergence      Convergence is the state of model training when the loss function exhibits minimal change, indicating that further training will not significantly improve the model’s performance        It is worth noting that the loss function may remain constant for a number of iterations before it begins to decrease. This can lead to a false sense of convergence, and it is important to monitor the loss function over a longer period to ensure true convergence.  Feature EngineeringBinning Trick is generally used when we have the continuous numerical data and we want to convert them into the discrete bins or intervals. This technique is highly useful when we are working with the algorithms that works really well with the categorical data or when we wanted to reduced the impact of the outliers.Let’s say you are working on the housing price prediction problem and encoded the street_name as the numerical number starting from 0 to n, doing so will create the bias as our model would assume you have ordered the streets based on their average house prices, or for that matter, many houses are located at the corner of two streets, and there’s no way to encode that information in the street_name value if it contains a single index. To solve this problem, we can create a binary feature vector where each column represents a street name, with 1 indicating the presence of the house on that street. As if the house is present on two streets, the model will use the weights for both of the streets as the feature vector for that house would have 1 for those two streets.Techniques like One-Hot encoding and the Label encoding helps to create the meaningful representations for the data that we can’t use directly to feed to our model in it’s raw form.If you have a dataset with 1,000,000 different street names for the street_name feature, creating a binary vector with true or false for each name is inefficient in terms of storage and computation. Instead, a common approach is to use a sparse representation, storing only nonzero values.  That is if we have 35 street_name and our house belongs to the street 24 then instead of storing the 35 different bits as the indicators we could store the 24Sometimes, taking the logarithm of a distribution address long tail issues, especially in data analysis and statistics. The long tail typically refers to a distribution where a few values occur frequently (the “head” of the distribution), while many other values occur infrequently (the “tail” of the distribution). By taking the logarithm, we can compress the range of values, which can be particularly helpful when dealing with data that has a wide spread. This transformation is especially useful when the data has a positive skewness, meaning that the tail is on the right side of the distribution.Clipping the outliers beyond a certain set threshold is also a part of the feature engineering, necessarily for a given use case if the threshold is set to 4.0 then Clipping the feature value at 4.0 doesn’t mean that we ignore all values greater than 4.0. Rather, it means that all values that were greater than 4.0 now become 4.0.Dealing with the missing values, duplicates, bad labels and bad feature values is important to make sure we have a robust set of data pipeline which we can use for our models.Note:      We shouldn’t pass a sparse representation as described about as a direct feature input to a model. Instead, we should convert the sparse representation into a one-hot representation before training on it.        There is a minor difference between Sparse Vector and Sparse Representation. First one actually means the vector consisting mostly zeroes and the second one actually means the dense representation of a Sparse Vector.        If you have continuous values like latitudes and longitudes for a housing price prediction problem, it is advised to use the bins instead of floating points numbers for the model predictions, as using them in the floating point values provides almost no predictive powers, instead we can create the bins with the specified boundaries as neighborhoods at latitude 35.4 and 35.8 are in the same bucket, but neighborhoods in latitude 35.4 and 36.2 are in different buckets. This way the model will learn a separate weight for each bucket. For example, the model will learn one weight for all the neighborhoods in the “35” bin, a different weight for neighborhoods in the “36” bin, and so on.  ScalingScaling involves transforming feature values from their original range (e.g., 10000 to 50000) to a more standard range, such as 0 to 1 or -1 to +1. This process is particularly useful when dealing with multiple features in a dataset, as it provides several benefits:      Improved convergence of gradient descent: Scaling the features can accelerate the convergence of gradient descent algorithms, as it ensures that all features are on the same scale and contribute equally to the optimization process.        Prevention of floating-point precision issues: During training, if the weights for a feature column exceed the floating-point precision limit, they may be set to NaN (Not a Number). This can cause a chain reaction, resulting in all the numbers in the model becoming NaN. Scaling the features helps to prevent this issue by keeping the weights within a manageable range.        Reduction of bias: Without feature scaling, a model may be biased towards features with a wider range of values. This is because the model will give more weight to these features during the training process. Scaling the features ensures that all features are treated equally, regardless of their range.        It is possible to apply different scaling thresholds to distinct features. For instance, one feature could be scaled between -2 to +2, while another might range from -4 to +4. However, using an excessively large scale, such as -10,000 to +10,000, could lead to suboptimal results.  Z-Score Normalization      Z-score normalization, also known as standardization or z-score scaling, is a statistical method used to make datasets comparable by transforming them into a standard normal distribution. This technique is commonly applied in statistics and machine learning for analyzing data points on a standardized scale.        To calculate the z-score for a data point (x) in a distribution, we use the formula: z = (x - μ) / σ, where μ is the mean of the dataset, and σ is the standard deviation. The z-score ensures that different features share a common scale, making comparison and analysis more straightforward. This proves especially useful when dealing with variables of diverse units or scales, preventing any single variable from dominating the analysis due to its magnitude.        After z-score normalization, each feature column in the transformed dataset has a mean of 0 and a standard deviation of 1. This normalization results in a standard normal distribution, making it easier to interpret and work with for statistical analyses.    Note: The fit and transform steps for z-score normalization should be applied initially to the complete dataset. Since z-score normalization involves both fitting and transforming methods, it’s essential to use the same normalizer when transforming the test data.  Feature CrossesA feature cross serves as a synthetic feature, injecting nonlinearity into the feature space by multiplying two or more input features. In simpler terms, it enables a linear model to grasp and interpret non-linear relationships.Consider having two features, x1 and x2, insufficient for our linear model to comprehend non-linearity. Enter the feature cross, x3, defined as the product of x1 and x2:x3 = x1*x2This newly introduced feature cross seamlessly integrates into the linear formula:Y = b + w1*x1 + w2*x2 + w3*x3Even though w3 encodes nonlinear information, the linear model adapts naturally during training, requiring no special adjustments.Feature crosses prove highly effective in enhancing and scale linear models when dealing with massive datasets which often have non linearity. Integrating feature crosses into our modeling practices significantly improves architecture and results. They play a crucial role in capturing complex relationships between features that might be not related when considered in isolation. By enabling the model to learn both individual and combined effects of features, feature crosses prove essential in capturing the difficulties of the of real-world data.For a matter of fact, sometimes in real-world applications, feature crosses are used not just for continuous features, but also for one-hot feature vectors. One-hot feature vector crosses act as logical conjunctions, revealing unknown patterns and interactions in the data which might not be known previously.For an instance, think of online shopping, now there might be some one-hot encoding features related to user behavior, shopping type, and product type, now feature crosses allows the model to create features like “Product Segment A AND Shopping Type Mobile.” This logical conjunction helps in capturing difficult patterns, I mean it may give some insights regarding when and how Product Segment A is being choosed when the person is shopping for a Mobile set, potentially improving predictive performance overall. Sometimes when our linear model is not performing upto mark, creating the features crosses might be a good choice.Classification MetricsAccuracy is a commonly used metric to evaluate the performance of a classification model. It is defined as the ratio of correctly predicted instances to the total number of instances. While accuracy is a straightforward and intuitive metric, it can be misleading in certain situations. In cases when we have in-balanced dataset, a classifier can achieve a higher accuracy by just simply predicting the majority class. However, this doesn’t mean the model is performing well. It might be failing to correctly identify instances of the minority class, which could be the more critical class in certain applications.Let’s consider an example of a medical diagnostic test for a rare disease. Suppose we have a dataset with 1000 instances, and only 10 of them belong to the positive class (people with the disease), while the remaining 990 instances belong to the negative class (people without the disease).Now, let’s imagine we have a simple classifier that predicts everyone as negative. Here’s how the confusion matrix would look like:Now, let’s calculate the accuracy:Accuracy = Number of Correct Predictions​ / Total number of instances =&gt; 990 / 1000  =&gt;  99 %The accuracy seems very high (99%), but the model is not performing well in identifying individuals with the disease. It predicts every dataset point as negative, so it misses the cases of the positive class. In a medical context, failing to identify individuals with the disease (false negatives) can have serious consequences.In this example, accuracy is misleading because the dataset is imbalanced, and the classifier achieves high accuracy by simply predicting the majority class. In such situations, accuracy alone doesn’t provide a complete picture of the model’s performance, and it’s important to consider other metrics like sensitivity (recall) or the F1 score, which take into account the ability of the model to correctly identify positive instances.      Precision and Recall are generally used to get the better understanding of the classification process we did. Precision means out of all the cases where your model marked something as positive, how many of them were actually true. So let’s say if marking an email correctly as spam if considered as Positive thing, then out of all the cases where your model marked some emails as Positive, how many of those emails were actually spam. On the other hand, Recall means out of all the positive cases from your dataset, what percentage did our model marked correctly as positive. So if you have 100 emails and 40 of them are spammed and your model predicted 30 of the 40 spammed emails as Spammed then the recall would be 30/40.        Remember that there is always some kind of trade-off between the Precision and Recall. As let’s say our threshold is 0.4 initially, that means all the cases where our classification model gives us the probability equal to or greater than 0.4, it’s considered to be Positive, so if we are more concerned for higher precision, or we want our model to be very sure that if it marks something as positive , it would be positive actually then we can increase the threshold to maybe a higher number of 0.6, so now only those instances when the probability from the classification model is higher than or equal to 0.6 will be considered positive which in turn will decrease the false positive and increase the Precision overall        But if we increase the threshold to 0.6 then it might happen that the instances when the probability for some dataset points are below the 0.6, let’s say 0.5 but our model will still marked them negative as it’s not above or equal to the threshold that we decided already, so now the model misses that, and it will only predict something as positive if it’s beyond that threshold and in that process it might miss the actual ones. So for those cases Recall will decrease        Tuning a threshold for logistic regression is different from tuning hyperparameters such as learning rate. Part of choosing a threshold is assessing how much you’ll suffer for making a mistake. For example, mistakenly labeling a non-spam message as spam is very bad. However, mistakenly labeling a spam message as non-spam is unpleasant, but hardly the end of your job.        In general, a model that outperforms another model on both precision and recall is likely the better model. Obviously, we’ll need to make sure that comparison is being done at a precision / recall point that is useful in practice for this to be meaningful. For example, suppose our spam detection model needs to have at least 90% precision to be useful and avoid unnecessary false alarms. In this case, comparing one model at {20% precision, 99% recall} to another at {15% precision, 98% recall} is not particularly instructive, as neither model meets the 90% precision requirement. But with that caveat in mind, this is a good way to think about comparing models when using precision and recall.        Precision and Recall solely depends on the threshold we choose, so it’s important to come up with an adequate set of threshold values.  ROC and AUCROC (Receiver Operating Characteristic) and AUC (Area Under the Curve) are closely related concepts used to evaluate the performance of classification models, particularly binary classifiers. Here are the key differences between ROC and AUC:  ROC (Receiver Operating Characteristic):                  Definition: The ROC curve is a graphical representation of the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) at various thresholds.                    Components: The ROC curve is typically a plot of true positive rate (y-axis) against the false positive rate (x-axis) for different threshold values.                    Interpretation: A model with a better performance will have an ROC curve that is closer to the top-left corner of the plot, indicating higher true positive rates and lower false positive rates across different threshold values.              AUC (Area Under the Curve):                  Definition: AUC is a scalar value that represents the area under the ROC curve. It provides a single numerical summary of the model’s ability to distinguish between the two classes.                    Range: AUC values range from 0 to 1, where a higher AUC indicates better discrimination performance. A model with an AUC of 0.5 is no better than random, while an AUC of 1 represents a perfect classifier.                    Interpretation: The AUC is a useful metric for comparing and ranking different models. A higher AUC suggests a better overall ability of the model to discriminate between positive and negative instances across all possible threshold values. That being said, a model with higher AUC suggests that our model ranks a random positive example more highly than a random negative example.                    AUC is desirable for the following two reasons , first it is scale-invariant as it measures how well predictions are ranked, rather than their absolute values. Secondly it is classification-threshold-invariant. as it measures the quality of the model’s predictions irrespective of what classification threshold is chosen.              However, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases:        Scale invariance is not always desirable. For example, sometimes we really do need well calibrated probability outputs, and AUC won’t tell us about that.        Classification-threshold invariance is not always desirable. In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives even if that results in a significant increase of false negatives. I mean if you are doing that job, You’d want to avoid marking important emails as spam, even if it means missing some spam emails. AUC isn’t a useful metric for this type of optimization.  Key Differences:      Format: ROC is a curve (plot), while AUC is a single scalar value.        Graphical Representation: ROC visually displays the trade-off between true positive rate and false positive rate, while AUC condenses this information into a single number.        Interpretation: ROC is useful for understanding the model’s behavior across different thresholds, while AUC provides a summary measure of overall model performance.        Note : To understand what Threshold term means, refer to topic of Classification Metrics above this section.  ",
            "content_html": "<p><em>ML Fine Print is my essential resource for recalling and understanding crucial details in the ML software cycle which often overlooked in the heat of real-world challenges. The documentation is designed for easy reference without a specific reading order.</em></p><h3 id=\"overfitting-and-regularization\">Overfitting and Regularization</h3><ul>  <li>    <p>Overfitting occurs when our model becomes too complex, fitting too closely to the noise in the training data. While it might perform well during training, it often falters during testing or real-world applications. This happens when our model gets too attuned to the specific variations in the training data, as it starts to memorize the data instead of capturing the relevant patterns in it, resulting in poor generalization to new, unseen data. A model with low bias and high variance is considered overfit and doesn’t perform well with new data.</p>  </li>  <li>    <p>Regularization steps comes in as a solution, introducing a penalty for excessive model complexity to curb overfitting. Techniques like Early Stopping, L2, and L1 regularization help strike a balance between fitting the training data and keeping the model reasonably simple.</p>  </li>  <li>    <p>Early Stopping involves halting training when a threshold of convergence is reached or when the validation loss starts to rise, preventing overfitting by avoiding excessive training. On the other hand, L2 regularization ensures that weights of various features don’t overlap excessively, as one feature weights shouldn’t overperform over the others promoting a balance. While the L1 regularization focuses on sparsity, driving some weights to absolute zero, doing so, it emphasize on keeping the informative features only.</p>  </li>  <li>    <p>Choosing the right lambda value in regularization is crucial. Too high, and your model may underfit; too low, and it may overfit. It’s a delicate tradeoff between simplicity and training data fit. The ideal lambda value is data-dependent, requiring hyperparameter tuning for optimal results. Despite potential increases in training loss, regularization often improves real-time predictions during inference, emphasizing the importance of overall model performance. Overall, regularization acts as a “keep things reasonable” rule for models, preventing extremes and ensuring their ability to make accurate predictions for new, unseen data.</p>  </li></ul><h3 id=\"assumptions-for-data-sampling\">Assumptions for Data Sampling</h3><p>In data sampling, we often make certain assumptions to ensure the validity and reliability of our results. These assumptions include:</p><ol>  <li>    <p>Independent and Identically Distributed (i.i.d.): Examples are drawn independently and identically from the same distribution. This means that the probability of selecting any particular example is the same for all examples, and the selection of one example does not influence the selection of any other example.</p>  </li>  <li>    <p>Stationarity: The distribution of the data does not change over time or across different parts of the data set. This means that the probability of observing a particular value or outcome is the same regardless of when or where in the data set it is observed</p>  </li>  <li>    <p>Same Distribution: All examples are drawn from the same distribution. This means that the underlying process that generates the data is the same for all examples.</p>  </li></ol><h3 id=\"violations-in-practice\">Violations in Practice:</h3><ul>  <li>    <p>Non-Independence: In some cases, the i.i.d. assumption may be violated due to dependencies between examples. For example, in a model that chooses ads to display, the choice of ads may be influenced by the user’s previous ad history, creating a temporal relationship between observations.</p>  </li>  <li>    <p>Non-Stationarity: The stationarity assumption may be violated if the underlying distribution of the data changes over time or across different parts of the data set. For example, in a data set of retail sales information, user purchases may change seasonally, violating stationarity.</p>  </li>  <li>    <p>Different Distributions: The same distribution assumption may be violated if examples are drawn from different distributions. For example, in a data set of customer reviews, the distribution of reviews may differ between different products or services.</p>  </li></ul><h3 id=\"test-and-training-set\">Test and Training Set</h3><ul>  <li>    <p>In machine learning, splitting the data into test and training sets is a crucial step. Randomization of the data before splitting is essential to ensure that the model does not train on a biased subset of the data. For instance, we would not want our Climate Predictor model to train solely on summer data and then be used for inference on test data consisting exclusively of winter data.</p>  </li>  <li>    <p>When dealing with a large dataset containing billions of sample points, a small percentage (5-10%) of the data can be sufficient for testing the model during inference. However, if the dataset is relatively small, alternative methods like cross-validation may be necessary for better results.</p>  </li>  <li>    <p>It is important to note that the test set should never be exposed to the model during training time. Repeated evaluation on the same test set can lead to implicit overfitting, which reduces the model’s ability to generalize to new, unseen data.</p>  </li>  <li>    <p>Furthermore, it is essential to shuffle the training dataset before creating a validation split. This is because sometimes, no matter how the training set and validation set are split, the loss curves may differ significantly. This issue is often caused by dissimilarities in the data between the training set and the validation set. Most libraries, including Pandas, split the dataset sequentially, which can lead to problems if the dataset points are arranged in a specific order. Therefore, it is recommended to randomize or shuffle the dataset points before splitting, ensuring that both the training and validation sets have an equivalent distribution of dataset points.</p>  </li></ul><h3 id=\"validation-set\">Validation Set</h3><ul>  <li>    <p>To avoid overfitting to a specific test set, it is advisable to use a validation set for tuning the model’s hyperparameters. This allows for more objective evaluation of the model’s performance and reduces the risk of implicit overfitting.</p>  </li>  <li>    <p>Using the same data set for testing and validation can limit the potential for tuning the hyperparameters or improving the model. It is beneficial to continuously acquire more data to refresh the test and validation sets, ensuring a more comprehensive evaluation of the model’s performance.</p>  </li>  <li>    <p>It is important to note that the internal model parameters are adjusted during the training process, while the hyperparameters are tuned based on the results obtained from the validation and test sets.</p>  </li></ul><h3 id=\"convergence\">Convergence</h3><ul>  <li>    <p>Convergence is the state of model training when the loss function exhibits minimal change, indicating that further training will not significantly improve the model’s performance</p>  </li>  <li>    <p>It is worth noting that the loss function may remain constant for a number of iterations before it begins to decrease. This can lead to a false sense of convergence, and it is important to monitor the loss function over a longer period to ensure true convergence.</p>  </li></ul><h3 id=\"feature-engineering\">Feature Engineering</h3><p>Binning Trick is generally used when we have the continuous numerical data and we want to convert them into the discrete bins or intervals. This technique is highly useful when we are working with the algorithms that works really well with the categorical data or when we wanted to reduced the impact of the outliers.</p><p>Let’s say you are working on the housing price prediction problem and encoded the <strong>street_name</strong> as the numerical number starting from 0 to n, doing so will create the bias as our model would assume you have ordered the streets based on their average house prices, or for that matter, many houses are located at the corner of two streets, and there’s no way to encode that information in the <strong>street_name</strong> value if it contains a single index. To solve this problem, we can create a binary feature vector where each column represents a street name, with 1 indicating the presence of the house on that street. As if the house is present on two streets, the model will use the weights for both of the streets as the feature vector for that house would have 1 for those two streets.</p><p>Techniques like One-Hot encoding and the Label encoding helps to create the meaningful representations for the data that we can’t use directly to feed to our model in it’s raw form.</p><p>If you have a dataset with 1,000,000 different street names for the <em>street_name</em> feature, creating a binary vector with true or false for each name is inefficient in terms of storage and computation. Instead, a common approach is to use a sparse representation, storing only nonzero values.  That is if we have 35 <em>street_name</em> and our house belongs to the <em>street</em> 24 then instead of storing the 35 different bits as the indicators we could store the 24</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/assets/images/sparse_representation.png?raw=true\" alt=\"sparse representation\" /></p><p>Sometimes, taking the logarithm of a distribution address long tail issues, especially in data analysis and statistics. The long tail typically refers to a distribution where a few values occur frequently (the “head” of the distribution), while many other values occur infrequently (the “tail” of the distribution). By taking the logarithm, we can compress the range of values, which can be particularly helpful when dealing with data that has a wide spread. This transformation is especially useful when the data has a positive skewness, meaning that the tail is on the right side of the distribution.</p><p>Clipping the outliers beyond a certain set threshold is also a part of the feature engineering, necessarily for a given use case if the threshold is set to 4.0 then Clipping the feature value at 4.0 doesn’t mean that we ignore all values greater than 4.0. Rather, it means that all values that were greater than 4.0 now become 4.0.</p><p>Dealing with the missing values, duplicates, bad labels and bad feature values is important to make sure we have a robust set of data pipeline which we can use for our models.</p><p><strong>Note:</strong></p><ul>  <li>    <p>We shouldn’t pass a sparse representation as described about as a direct feature input to a model. Instead, we should convert the sparse representation into a one-hot representation before training on it.</p>  </li>  <li>    <p>There is a minor difference between Sparse Vector and Sparse Representation. First one actually means the vector consisting mostly zeroes and the second one actually means the <em>dense representation</em> of a Sparse Vector.</p>  </li>  <li>    <p>If you have continuous values like latitudes and longitudes for a housing price prediction problem, it is advised to use the bins instead of floating points numbers for the model predictions, as using them in the floating point values provides almost no predictive powers, instead we can create the bins with the specified boundaries as neighborhoods at latitude 35.4 and 35.8 are in the same bucket, but neighborhoods in latitude 35.4 and 36.2 are in different buckets. This way the model will learn a separate weight for each bucket. For example, the model will learn one weight for all the neighborhoods in the “35” bin, a different weight for neighborhoods in the “36” bin, and so on.</p>  </li></ul><h3 id=\"scaling\">Scaling</h3><p>Scaling involves transforming feature values from their original range (e.g., 10000 to 50000) to a more standard range, such as 0 to 1 or -1 to +1. This process is particularly useful when dealing with multiple features in a dataset, as it provides several benefits:</p><ul>  <li>    <p>Improved convergence of gradient descent: Scaling the features can accelerate the convergence of gradient descent algorithms, as it ensures that all features are on the same scale and contribute equally to the optimization process.</p>  </li>  <li>    <p>Prevention of floating-point precision issues: During training, if the weights for a feature column exceed the floating-point precision limit, they may be set to NaN (Not a Number). This can cause a chain reaction, resulting in all the numbers in the model becoming NaN. Scaling the features helps to prevent this issue by keeping the weights within a manageable range.</p>  </li>  <li>    <p>Reduction of bias: Without feature scaling, a model may be biased towards features with a wider range of values. This is because the model will give more weight to these features during the training process. Scaling the features ensures that all features are treated equally, regardless of their range.</p>  </li>  <li>    <p>It is possible to apply different scaling thresholds to distinct features. For instance, one feature could be scaled between -2 to +2, while another might range from -4 to +4. However, using an excessively large scale, such as -10,000 to +10,000, could lead to suboptimal results.</p>  </li></ul><h3 id=\"z-score-normalization\">Z-Score Normalization</h3><ul>  <li>    <p>Z-score normalization, also known as standardization or z-score scaling, is a statistical method used to make datasets comparable by transforming them into a standard normal distribution. This technique is commonly applied in statistics and machine learning for analyzing data points on a standardized scale.</p>  </li>  <li>    <p>To calculate the z-score for a data point (x) in a distribution, we use the formula: z = (x - μ) / σ, where μ is the mean of the dataset, and σ is the standard deviation. The z-score ensures that different features share a common scale, making comparison and analysis more straightforward. This proves especially useful when dealing with variables of diverse units or scales, preventing any single variable from dominating the analysis due to its magnitude.</p>  </li>  <li>    <p>After z-score normalization, each feature column in the transformed dataset has a mean of 0 and a standard deviation of 1. This normalization results in a standard normal distribution, making it easier to interpret and work with for statistical analyses.</p>    <p><em><strong>Note</strong>: The fit and transform steps for z-score normalization should be applied initially to the complete dataset. Since z-score normalization involves both fitting and transforming methods, it’s essential to use the same normalizer when transforming the test data.</em></p>  </li></ul><h3 id=\"feature-crosses\">Feature Crosses</h3><p>A <strong>feature cross</strong> serves as a synthetic feature, injecting nonlinearity into the feature space by multiplying two or more input features. In simpler terms, it enables a linear model to grasp and interpret non-linear relationships.</p><p>Consider having two features, x1 and x2, insufficient for our linear model to comprehend non-linearity. Enter the feature cross, x3, defined as the product of x1 and x2:</p><pre><code class=\"language-text\">x3 = x1*x2</code></pre><p>This newly introduced feature cross seamlessly integrates into the linear formula:</p><pre><code class=\"language-text\">Y = b + w1*x1 + w2*x2 + w3*x3</code></pre><p>Even though w3 encodes nonlinear information, the linear model adapts naturally during training, requiring no special adjustments.</p><p>Feature crosses prove highly effective in enhancing and scale linear models when dealing with massive datasets which often have non linearity. Integrating feature crosses into our modeling practices significantly improves architecture and results. They play a crucial role in capturing complex relationships between features that might be not related when considered in isolation. By enabling the model to learn both individual and combined effects of features, feature crosses prove essential in capturing the difficulties of the of real-world data.</p><p>For a matter of fact, sometimes in real-world applications, feature crosses are used not just for continuous features, but also for one-hot feature vectors. One-hot feature vector crosses act as logical conjunctions, revealing unknown patterns and interactions in the data which might not be known previously.</p><p>For an instance, think of online shopping, now there might be some one-hot encoding features related to user behavior, shopping type, and product type, now feature crosses allows the model to create features like “Product Segment A AND Shopping Type Mobile.” This logical conjunction helps in capturing difficult patterns, I mean it may give some insights regarding when and how Product Segment A is being choosed when the person is shopping for a Mobile set, potentially improving predictive performance overall. Sometimes when our linear model is not performing upto mark, creating the features crosses might be a good choice.</p><h3 id=\"classification-metrics\">Classification Metrics</h3><p>Accuracy is a commonly used metric to evaluate the performance of a classification model. It is defined as the ratio of correctly predicted instances to the total number of instances. While accuracy is a straightforward and intuitive metric, it can be misleading in certain situations. In cases when we have in-balanced dataset, a classifier can achieve a higher accuracy by just simply predicting the majority class. However, this doesn’t mean the model is performing well. It might be failing to correctly identify instances of the minority class, which could be the more critical class in certain applications.</p><p>Let’s consider an example of a medical diagnostic test for a rare disease. Suppose we have a dataset with 1000 instances, and only 10 of them belong to the positive class (people with the disease), while the remaining 990 instances belong to the negative class (people without the disease).</p><p>Now, let’s imagine we have a simple classifier that predicts everyone as negative. Here’s how the confusion matrix would look like:</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/assets/images/tptn.png?raw=true\" alt=\"accuracy\" /></p><p>Now, let’s calculate the accuracy:</p><p><em>Accuracy = Number of Correct Predictions​ / Total number of instances =&gt; 990 / 1000  =&gt;  99 %</em></p><p>The accuracy seems very high (99%), but the model is not performing well in identifying individuals with the disease. It predicts every dataset point as negative, so it misses the cases of the positive class. In a medical context, failing to identify individuals with the disease (false negatives) can have serious consequences.</p><p>In this example, accuracy is misleading because the dataset is imbalanced, and the classifier achieves high accuracy by simply predicting the majority class. In such situations, accuracy alone doesn’t provide a complete picture of the model’s performance, and it’s important to consider other metrics like sensitivity (recall) or the F1 score, which take into account the ability of the model to correctly identify positive instances.</p><ol>  <li>    <p>Precision and Recall are generally used to get the better understanding of the classification process we did. Precision means out of all the cases where your model marked something as positive, how many of them were actually true. So let’s say if marking an email correctly as spam if considered as Positive thing, then out of all the cases where your model marked some emails as Positive, how many of those emails were actually spam. On the other hand, Recall means out of all the positive cases from your dataset, what percentage did our model marked correctly as positive. So if you have 100 emails and 40 of them are spammed and your model predicted 30 of the 40 spammed emails as Spammed then the recall would be 30/40.</p>  </li>  <li>    <p>Remember that there is always some kind of trade-off between the Precision and Recall. As let’s say our threshold is 0.4 initially, that means all the cases where our classification model gives us the probability equal to or greater than 0.4, it’s considered to be Positive, so if we are more concerned for higher precision, or we want our model to be very sure that if it marks something as positive , it would be positive actually then we can increase the threshold to maybe a higher number of 0.6, so now only those instances when the probability from the classification model is higher than or equal to 0.6 will be considered positive which in turn will decrease the false positive and increase the Precision overall</p>  </li>  <li>    <p>But if we increase the threshold to 0.6 then it might happen that the instances when the probability for some dataset points are below the 0.6, let’s say 0.5 but our model will still marked them negative as it’s not above or equal to the threshold that we decided already, so now the model misses that, and it will only predict something as positive if it’s beyond that threshold and in that process it might miss the actual ones. So for those cases Recall will decrease</p>  </li>  <li>    <p>Tuning a threshold for logistic regression is different from tuning hyperparameters such as learning rate. Part of choosing a threshold is assessing how much you’ll suffer for making a mistake. For example, mistakenly labeling a non-spam message as spam is very bad. However, mistakenly labeling a spam message as non-spam is unpleasant, but hardly the end of your job.</p>  </li>  <li>    <p>In general, a model that outperforms another model on both precision and recall is likely the better model. Obviously, we’ll need to make sure that comparison is being done at a precision / recall point that is useful in practice for this to be meaningful. For example, suppose our spam detection model needs to have at least 90% precision to be useful and avoid unnecessary false alarms. In this case, comparing one model at {20% precision, 99% recall} to another at {15% precision, 98% recall} is not particularly instructive, as neither model meets the 90% precision requirement. But with that caveat in mind, this is a good way to think about comparing models when using precision and recall.</p>  </li>  <li>    <p>Precision and Recall solely depends on the threshold we choose, so it’s important to come up with an adequate set of threshold values.</p>  </li></ol><h3 id=\"roc-and-auc\">ROC and AUC</h3><p>ROC (Receiver Operating Characteristic) and AUC (Area Under the Curve) are closely related concepts used to evaluate the performance of classification models, particularly binary classifiers. Here are the key differences between ROC and AUC:</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/assets/images/ROC_Curve.png?raw=true\" alt=\"rocauccurve\" /></p><ol>  <li><strong>ROC (Receiver Operating Characteristic):</strong>    <ul>      <li>        <p><strong>Definition:</strong> The ROC curve is a graphical representation of the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) at various thresholds.</p>      </li>      <li>        <p><strong>Components:</strong> The ROC curve is typically a plot of true positive rate (y-axis) against the false positive rate (x-axis) for different threshold values.</p>      </li>      <li>        <p><strong>Interpretation:</strong> A model with a better performance will have an ROC curve that is closer to the top-left corner of the plot, indicating higher true positive rates and lower false positive rates across different threshold values.</p>      </li>    </ul>  </li>  <li><strong>AUC (Area Under the Curve):</strong>    <ul>      <li>        <p><strong>Definition:</strong> AUC is a scalar value that represents the area under the ROC curve. It provides a single numerical summary of the model’s ability to distinguish between the two classes.</p>      </li>      <li>        <p><strong>Range:</strong> AUC values range from 0 to 1, where a higher AUC indicates better discrimination performance. A model with an AUC of 0.5 is no better than random, while an AUC of 1 represents a perfect classifier.</p>      </li>      <li>        <p><strong>Interpretation:</strong> The AUC is a useful metric for comparing and ranking different models. A higher AUC suggests a better overall ability of the model to discriminate between positive and negative instances across all possible threshold values. That being said, a model with higher AUC suggests that our model ranks a random positive example more highly than a random negative example.</p>      </li>      <li>        <p>AUC is desirable for the following two reasons , first it is <strong>scale-invariant</strong> as it measures how well predictions are ranked, rather than their absolute values. Secondly it is <strong>classification-threshold-invariant</strong>. as it measures the quality of the model’s predictions irrespective of what classification threshold is chosen.</p>      </li>    </ul>    <p>However, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases:</p>  </li></ol><ul>  <li>    <p><strong>Scale invariance is not always desirable.</strong> For example, sometimes we really do need well calibrated probability outputs, and AUC won’t tell us about that.</p>  </li>  <li>    <p><strong>Classification-threshold invariance is not always desirable.</strong> In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives even if that results in a significant increase of false negatives. I mean if you are doing that job, You’d want to avoid marking important emails as spam, even if it means missing some spam emails. AUC isn’t a useful metric for this type of optimization.</p>  </li></ul><h3 id=\"key-differences\">Key Differences:</h3><ul>  <li>    <p><strong>Format:</strong> ROC is a curve (plot), while AUC is a single scalar value.</p>  </li>  <li>    <p><strong>Graphical Representation:</strong> ROC visually displays the trade-off between true positive rate and false positive rate, while AUC condenses this information into a single number.</p>  </li>  <li>    <p><strong>Interpretation:</strong> ROC is useful for understanding the model’s behavior across different thresholds, while AUC provides a summary measure of overall model performance.</p>  </li>  <li>    <p><em>Note : To understand what Threshold term means, refer to topic of Classification Metrics above this section.</em></p>  </li></ul>",
            "url": "http://localhost:4000/2024/01/14/ml-fine-print",
            
            
            
            "tags": ["Machine Learning","Deep Learning"],
            
            "date_published": "2024-01-14T00:00:00+05:30",
            "date_modified": "2024-01-14T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        }
    
    ]
}