{
    "version": "https://jsonfeed.org/version/1",
    "title": "Blixxi Labs",
    "home_page_url": "http://localhost:4000/",
    "feed_url": "http://localhost:4000/feed.json",
    "description": "Everything related to AI",
    "icon": "http://localhost:4000/apple-touch-icon.png",
    "favicon": "http://localhost:4000/favicon.ico",
    "expired": false,
    
    "author": "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}",
    
"items": [
    
        {
            "id": "http://localhost:4000/2024/03/29/effortlessly-loading-and-processing-images-with-lance-a-code-walkthrough",
            "title": "Effortlessly Loading and Processing Images with Lance",
            "summary": "How you can use the lance format to work with big sized data",
            "content_text": "Working with large image datasets in machine learning can be challenging, often requiring significant computational resources and efficient data-handling techniques. While widely used for image storage, traditional file formats like JPEG or PNG are not optimized for efficient data loading and processing in Machine learning workflows. This is where the Lance format shines, offering a modern, columnar data storage solution designed specifically for machine learning applications.The Lance format stores data in a compressed columnar format, enabling efficient storage, fast data loading, and fast random access to data subsets. Additionally, the Lance format is maintained on disk, which provides a couple of advantages: It will persist through a system failure and doesn’t rely on keeping everything in memory, which can run out. This also lends itself to enhanced data privacy and security, as the data doesn’t need to be transferred over a network.One of the other key advantages of the Lance format is its ability to store diverse data types, such as images, text, and numerical data, in a unified format. Imagine having a data lake where each kind of data can be stored seamlessly without separating underlying data types. This flexibility is particularly valuable in machine learning pipelines, where different data types often need to be processed together. This unparalleled flexibility is a game-changer in machine learning pipelines, where different modalities of data often need to be processed together for tasks like multimodal learning, audio-visual analysis, or natural language processing with visual inputs.With Lance, you can effortlessly consider all kinds of data, from images to videos and audio files to text data and numerical values, all within the same columnar storage format. This means you can have a single, streamlined data pipeline that can handle any combination of data types without the need for complex data transformations or conversions. Lance easily handles it without worrying about compatibility issues or dealing with separate storage formats for different data types. And the best part? You can store and retrieve all these diverse data types within the same column.In contrast, while efficient for tabular data, traditional formats like Parquet may need to handle diverse data types better. By converting all data into a single, unified format using Lance, you can retrieve and process any type of data without dealing with multiple formats or complex data structures.In this article, I’ll walk through a Python code example that demonstrates how to convert a dataset of GTA5 images into the Lance format and subsequently load them into a Pandas DataFrame for further processing.import osimport pandas as pdimport pyarrow as paimport lanceimport timefrom tqdm import tqdmWe start by importing the necessary libraries, including os for directory handling, pandas for data manipulation, pyarrow for working with Arrow data formats, lance for interacting with the Lance format, and tqdm for displaying progress bars.def process_images():    current_dir = os.getcwd()    images_folder = os.path.join(current_dir, \"./image\")    # Define schema for RecordBatch    schema = pa.schema([('image', pa.binary())])    image_files = [filename for filename in os.listdir(images_folder)                if filename.endswith((\".png\", \".jpg\", \".jpeg\"))]    # Iterate over all images in the folder with tqdm    for filename in tqdm(image_files, desc=\"Processing Images\"):        # Construct the full path to the image        image_path = os.path.join(images_folder, filename)        # Read and convert the image to a binary format        with open(image_path, 'rb') as f:            binary_data = f.read()        image_array = pa.array([binary_data], type=pa.binary())        # Yield RecordBatch for each image        yield pa.RecordBatch.from_arrays([image_array], schema=schema)The process_images function is responsible for iterating over all image files in a specified directory and converting them into PyArrow RecordBatch objects. It first defines the schema for the RecordBatch, specifying that each batch will contain a single binary column named ‘image’.It then iterates over all image files in the directory, reads each image’s binary data, and yields a RecordBatch containing that image’s binary data.def write_to_lance():    schema = pa.schema([        pa.field(\"image\", pa.binary())    ])    reader = pa.RecordBatchReader.from_batches(schema, process_images())    lance.write_dataset(        reader,        \"image_dataset.lance\",        schema,    )The write_to_lance function creates a RecordBatchReader from the process_images generator and writes the resulting data to a Lance dataset named “image_dataset.lance”. This step converts the image data into the efficient, columnar Lance format, optimizing it for fast data loading and random access.def loading_into_pandas():    uri = \"image_dataset.lance\"    ds = lance.dataset(uri)    # Accumulate data from batches into a list    data = []    for batch in ds.to_batches(columns=[\"image\"], batch_size=10):        tbl = batch.to_pandas()        data.append(tbl)    # Concatenate all DataFrames into a single DataFrame    df = pd.concat(data, ignore_index=True)    print(\"Pandas DataFrame is ready\")    print(\"Total Rows: \", df.shape[0])The loading_into_pandas function demonstrates how to load the image data from the Lance dataset into a Pandas DataFrame. It first creates a Lance dataset object from the “image_dataset.lance” file. Then, it iterates over batches of data, converting each batch into a Pandas DataFrame and appending it to a list. Finally, it concatenates all the DataFrames in the list into a single DataFrame, making the image data accessible for further processing or analysis.if __name__ == \"__main__\":    start = time.time()    write_to_lance()    loading_into_pandas()    end = time.time()    print(f\"Time(sec): {end - start}\")The central part of the script calls the write_to_lance and loading_into_pandas functions, measuring the total execution time for the entire process.By leveraging the Lance format, this code demonstrates how to efficiently store and load large image datasets for machine learning applications. The columnar storage and compression techniques Lance uses result in reduced storage requirements and faster data loading times, making it an ideal choice for working with large-scale image data.Moreover, the random access capabilities of Lance allow for selective loading of specific data subsets, enabling efficient data augmentation techniques and custom data loading strategies tailored to your machine learning workflow.TLDR: Lance format provides a powerful and efficient solution for handling multimodal data in machine learning pipelines, streamlining data storage, loading, and processing tasks. By adopting Lance, we can improve our machine learning projects’ overall performance and resource efficiency while also benefiting from the ability to store diverse data types in a unified format and maintain data locality and privacy. Here is the whole script for your reference.import osimport pandas as pdimport pyarrow as paimport lanceimport timefrom tqdm import tqdmdef process_images():    current_dir = os.getcwd()    images_folder = os.path.join(current_dir, \"./image\")    # Define schema for RecordBatch    schema = pa.schema([('image', pa.binary())])    image_files = [filename for filename in os.listdir(images_folder)                if filename.endswith((\".png\", \".jpg\", \".jpeg\"))]    # Iterate over all images in the folder with tqdm    for filename in tqdm(image_files, desc=\"Processing Images\"):        # Construct the full path to the image        image_path = os.path.join(images_folder, filename)        # Read and convert the image to a binary format        with open(image_path, 'rb') as f:            binary_data = f.read()        image_array = pa.array([binary_data], type=pa.binary())        # Yield RecordBatch for each image        yield pa.RecordBatch.from_arrays([image_array], schema=schema)def write_to_lance():        schema = pa.schema([        pa.field(\"image\", pa.binary())    ])    reader = pa.RecordBatchReader.from_batches(schema, process_images())    lance.write_dataset(        reader,        \"image_dataset.lance\",        schema,    )def loading_into_pandas():    uri = \"image_dataset.lance\"    ds = lance.dataset(uri)    # Accumulate data from batches into a list    data = []    for batch in ds.to_batches(columns=[\"image\"], batch_size=10):        tbl = batch.to_pandas()        data.append(tbl)    # Concatenate all DataFrames into a single DataFrame    df = pd.concat(data, ignore_index=True)    print(\"Pandas DataFrame is ready\")    print(\"Total Rows: \", df.shape[0])if __name__ == \"__main__\":    start = time.time()    write_to_lance()    loading_into_pandas()    end = time.time()    print(f\"Time(sec): {end - start}\")Imagine using Lance-formatted image data to make machine learning and deep learning projects faster. Something big is coming up, stay tuned.",
            "content_html": "<p>Working with large image datasets in machine learning can be challenging, often requiring significant computational resources and efficient data-handling techniques. While widely used for image storage, traditional file formats like JPEG or PNG are not optimized for efficient data loading and processing in Machine learning workflows. This is where the Lance format shines, offering a modern, columnar data storage solution designed specifically for machine learning applications.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/loading_and_processing_image_with_lance/image.png?raw=true\" alt=\"meme_for_ml_workloads\" /></p><p>The Lance format stores data in a compressed columnar format, enabling efficient storage, fast data loading, and fast random access to data subsets. Additionally, the Lance format is maintained on disk, which provides a couple of advantages: It will persist through a system failure and doesn’t rely on keeping everything in memory, which can run out. This also lends itself to enhanced data privacy and security, as the data doesn’t need to be transferred over a network.</p><p>One of the other key advantages of the Lance format is its ability to store diverse data types, such as images, text, and numerical data, in a unified format. Imagine having a data lake where each kind of data can be stored seamlessly without separating underlying data types. This flexibility is particularly valuable in machine learning pipelines, where different data types often need to be processed together. This unparalleled flexibility is a game-changer in machine learning pipelines, where different modalities of data often need to be processed together for tasks like multimodal learning, audio-visual analysis, or natural language processing with visual inputs.</p><p>With Lance, you can effortlessly consider all kinds of data, from images to videos and audio files to text data and numerical values, all within the same columnar storage format. This means you can have a single, streamlined data pipeline that can handle any combination of data types without the need for complex data transformations or conversions. Lance easily handles it without worrying about compatibility issues or dealing with separate storage formats for different data types. And the best part? You can store and retrieve all these diverse data types within the same column.</p><p>In contrast, while efficient for tabular data, traditional formats like Parquet may need to handle diverse data types better. By converting all data into a single, unified format using Lance, you can retrieve and process any type of data without dealing with multiple formats or complex data structures.In this article, I’ll walk through a Python code example that demonstrates how to convert a dataset of GTA5 images into the Lance format and subsequently load them into a Pandas DataFrame for further processing.</p><pre><code class=\"language-python\">import osimport pandas as pdimport pyarrow as paimport lanceimport timefrom tqdm import tqdm</code></pre><p>We start by importing the necessary libraries, including os for directory handling, pandas for data manipulation, pyarrow for working with Arrow data formats, lance for interacting with the Lance format, and tqdm for displaying progress bars.</p><pre><code class=\"language-python\">def process_images():    current_dir = os.getcwd()    images_folder = os.path.join(current_dir, \"./image\")    # Define schema for RecordBatch    schema = pa.schema([('image', pa.binary())])    image_files = [filename for filename in os.listdir(images_folder)                if filename.endswith((\".png\", \".jpg\", \".jpeg\"))]    # Iterate over all images in the folder with tqdm    for filename in tqdm(image_files, desc=\"Processing Images\"):        # Construct the full path to the image        image_path = os.path.join(images_folder, filename)        # Read and convert the image to a binary format        with open(image_path, 'rb') as f:            binary_data = f.read()        image_array = pa.array([binary_data], type=pa.binary())        # Yield RecordBatch for each image        yield pa.RecordBatch.from_arrays([image_array], schema=schema)</code></pre><p>The process_images function is responsible for iterating over all image files in a specified directory and converting them into PyArrow RecordBatch objects. It first defines the schema for the RecordBatch, specifying that each batch will contain a single binary column named ‘image’.</p><p>It then iterates over all image files in the directory, reads each image’s binary data, and yields a RecordBatch containing that image’s binary data.</p><pre><code class=\"language-python\">def write_to_lance():    schema = pa.schema([        pa.field(\"image\", pa.binary())    ])    reader = pa.RecordBatchReader.from_batches(schema, process_images())    lance.write_dataset(        reader,        \"image_dataset.lance\",        schema,    )</code></pre><p>The write_to_lance function creates a RecordBatchReader from the process_images generator and writes the resulting data to a Lance dataset named “image_dataset.lance”. This step converts the image data into the efficient, columnar Lance format, optimizing it for fast data loading and random access.</p><pre><code class=\"language-python\">def loading_into_pandas():    uri = \"image_dataset.lance\"    ds = lance.dataset(uri)    # Accumulate data from batches into a list    data = []    for batch in ds.to_batches(columns=[\"image\"], batch_size=10):        tbl = batch.to_pandas()        data.append(tbl)    # Concatenate all DataFrames into a single DataFrame    df = pd.concat(data, ignore_index=True)    print(\"Pandas DataFrame is ready\")    print(\"Total Rows: \", df.shape[0])</code></pre><p>The loading_into_pandas function demonstrates how to load the image data from the Lance dataset into a Pandas DataFrame. It first creates a Lance dataset object from the “image_dataset.lance” file. Then, it iterates over batches of data, converting each batch into a Pandas DataFrame and appending it to a list. Finally, it concatenates all the DataFrames in the list into a single DataFrame, making the image data accessible for further processing or analysis.</p><pre><code class=\"language-python\">if __name__ == \"__main__\":    start = time.time()    write_to_lance()    loading_into_pandas()    end = time.time()    print(f\"Time(sec): {end - start}\")</code></pre><p>The central part of the script calls the write_to_lance and loading_into_pandas functions, measuring the total execution time for the entire process.By leveraging the Lance format, this code demonstrates how to efficiently store and load large image datasets for machine learning applications. The columnar storage and compression techniques Lance uses result in reduced storage requirements and faster data loading times, making it an ideal choice for working with large-scale image data.</p><p>Moreover, the random access capabilities of Lance allow for selective loading of specific data subsets, enabling efficient data augmentation techniques and custom data loading strategies tailored to your machine learning workflow.</p><p>TLDR: Lance format provides a powerful and efficient solution for handling multimodal data in machine learning pipelines, streamlining data storage, loading, and processing tasks. By adopting Lance, we can improve our machine learning projects’ overall performance and resource efficiency while also benefiting from the ability to store diverse data types in a unified format and maintain data locality and privacy. Here is the whole script for your reference.</p><pre><code class=\"language-python\">import osimport pandas as pdimport pyarrow as paimport lanceimport timefrom tqdm import tqdmdef process_images():    current_dir = os.getcwd()    images_folder = os.path.join(current_dir, \"./image\")    # Define schema for RecordBatch    schema = pa.schema([('image', pa.binary())])    image_files = [filename for filename in os.listdir(images_folder)                if filename.endswith((\".png\", \".jpg\", \".jpeg\"))]    # Iterate over all images in the folder with tqdm    for filename in tqdm(image_files, desc=\"Processing Images\"):        # Construct the full path to the image        image_path = os.path.join(images_folder, filename)        # Read and convert the image to a binary format        with open(image_path, 'rb') as f:            binary_data = f.read()        image_array = pa.array([binary_data], type=pa.binary())        # Yield RecordBatch for each image        yield pa.RecordBatch.from_arrays([image_array], schema=schema)def write_to_lance():        schema = pa.schema([        pa.field(\"image\", pa.binary())    ])    reader = pa.RecordBatchReader.from_batches(schema, process_images())    lance.write_dataset(        reader,        \"image_dataset.lance\",        schema,    )def loading_into_pandas():    uri = \"image_dataset.lance\"    ds = lance.dataset(uri)    # Accumulate data from batches into a list    data = []    for batch in ds.to_batches(columns=[\"image\"], batch_size=10):        tbl = batch.to_pandas()        data.append(tbl)    # Concatenate all DataFrames into a single DataFrame    df = pd.concat(data, ignore_index=True)    print(\"Pandas DataFrame is ready\")    print(\"Total Rows: \", df.shape[0])if __name__ == \"__main__\":    start = time.time()    write_to_lance()    loading_into_pandas()    end = time.time()    print(f\"Time(sec): {end - start}\")</code></pre><p>Imagine using Lance-formatted image data to make machine learning and deep learning projects faster. Something big is coming up, stay tuned.</p>",
            "url": "http://localhost:4000/2024/03/29/effortlessly-loading-and-processing-images-with-lance-a-code-walkthrough",
            
            
            
            "tags": ["Lance","LanceDB"],
            
            "date_published": "2024-03-29T00:00:00+05:30",
            "date_modified": "2024-03-29T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/03/15/embedded-databases",
            "title": "Embedded Databases",
            "summary": "How LanceDB is beating ass of every other Embedded Database",
            "content_text": "In today’s world, when everyone’s curious about trying out generative AI tools and how they work, you’ve probably heard about embedded databases. Most of us tend to think about client-server based setups when databases come to mind. And honestly, that’s somewhat accurate.However, client-server architectures aren’t really built to handle heavy analytical and ML workloads. Essentially, the processing tasks fall into two main categories: OLTP (online transactional processing) and OLAP (online analytical processing). So, when you’re changing your Instagram profile picture or uploading a photo on Facebook, you’re essentially involved in OLTP tasks, which focus on quick and easy processing. On the flip side, when we deal with OLAP, it’s all about handling complex computations such as retrieving queries from extensive datasets, combining tables, and aggregating data for big data purposes. Now, we need something that can handle our large ML workloads effectively and perform optimally across datasets ranging from small to large scales.Columnar Oriented DatastoresHard drives store data in terms of blocks, so whenever an operation is performed, the entire block containing the data is loaded into memory for reading by the OS. Now, Row-oriented databases aim to store whole rows of the database in the same block, whereas columnar databases store column entries in the same block.This implies that when you need to perform column-oriented operations like updating columns, aggregations, or selecting a column entry, column-oriented databases outperform row-oriented ones in terms of speed. However, if you need to add a new data point entry with multiple columns, then row-oriented databases perform better.Now, the point is, there’s something called Apache Arrow, which is a language-agnostic columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs.This is too technical. Let me break it down for you. Machine learning is all about feeding huge amounts of data into complex mathematical models to find patterns and make predictions, right? Now, Apache Arrow turbocharges this process by providing a standardized way to store and work with data that’s optimized for modern hardware like powerful GPUs. So Instead of dealing with clunky row-based formats, Arrow’s columnar layout lets you focus on the specific data features you need, drastically cutting down processing time. And since Arrow keeps data in memory, AKA RAM, rather than on sluggish disk storage like hard drives or SSDs, your models can crunch through datasets at blistering speeds. The end result? You can iterate faster, train better models, and stay ahead of the competition.Still confused, right? I was too. Well, let’s take an example. If you’re building a model to predict housing prices based on factors like square footage, number of bedrooms, location, etc., Arrow’s columnar format would allow you to easily isolate and work with just the columns containing those specific features, ignoring any other irrelevant data columns. This focused, selective data access is more efficient than dealing with row-based formats where you’d have to sort through all the data indiscriminately.Now that’s where the power of Arrow based columnar databases comes into play.Lance Data FormatBuilding on the advantages of Apache Arrow’s columnar, in-memory approach for machine learning, there’s another game-changing data format that takes things to a whole new level – the Lance Data Format.Designed from the ground up with modern ML workflows in mind, Lance is an absolute speed demon when it comes to querying and prepping data for training models. But it’s not just about raw speed – Lance has some seriously impressive versatility under the hood.Unlike formats limited to tabular data, Lance can handle all kinds of data types like images, videos, 3D point clouds, audio, and more. It’s like a Swiss Army knife of data formats for ML. Btw, Don’t just take my word for it because I love LanceDB, instead – benchmarks have shown that Lance can provide random data access involving read and write operation a mind-boggling approximately 1000 times faster than Parquet, another popular columnar format. This blistering speed comes from unique storage memory layout used by Lance.The other important thing LanceDB provides is the usage of Zero-copy versioning, essentially it means that when you create a new version of your data, LanceDB doesn’t have to make an entire copy – it just stores the changes efficiently. This saves a ton of time and storage space compared to traditional versioning methods. And optimized vector operations allow Lance to process data in bulk, taking full advantage of modern hardware like GPUs and vectorized CPUs. It’s all part of Lance’s cloud-native design.In-processBefore understanding what Embedded Systems really do, First, we need to understand what a database management system (DBMS) is in Layman. Now in simple terms a DBMS is a software system that allows you to create, manage, and interact with databases (obviously duhh). I mean It provides a way to store, retrieve, and manipulate data in an organized and more efficient manner.Now, an embedded database is a type of DBMS that is tightly integrated with the application layer. This means that the database that you are working with is not a separate process or service running on its own; instead, it runs within the same process as the application itself.This tight integration makes it easier for the application to interact with the database, as there’s no need for inter-process communication or network communication unlike traditional client-server database systems where the database is a  separate process or a serviceThis type of thing is called “in-process”. Remember this for the rest of your life. It might be the most important thing to remember when judging the other embedded databases out there.In simple terms, the application can directly access and manipulate the database without going through additional layers or protocols. CUTTING THE BS.On-disk storageRemember I said earlier that Embedded systems have this ability to store the data in-memory and work at blistering speeds. While embedded databases can store data in memory, they also have this capability to store larger datasets on disk. This allows them to scale to large amounts of data (terabytes or billion dataset points, think of embeddings of billion tokens) while still providing relatively low query latencies and response times. So it’s like the best of the both worlds.ServerlessOk new term! Sometimes the terms “Embedded” and “Serverless” are sometimes used interchangeably in the database community, but they actually refer to different concepts. “Embedded” refers to the database being tightly integrated with the application as we seen earlier in case of Embedded databases while Serverless refers to the separation of storage and compute resources, and it’s often used in the context of microservices architectures.To make it more concise, think of the serverless database as it’s composed of two different containers, the storage layer (where the data is stored) and the compute layer (where the data is processed). Now this separation allows the compute resources to be dynamically allocated and scaled up or down based on the workload.And btw, when we are talking about the Serverless model, it is often associated with the cloud based services, where you don’t have to manage the underlying infra..ScalabilityOk so you decided to work on the simple RAG system and wanted to test it out or just maybe play with it and came up with the various vector databases, you did your experiment,  you are happy and done. Now when you got serious and came up with something and wanted to scale your RAG system for let’s say 1 billion embeddings, you open up your earlier setup, ingested more data, created more embeddings and when the time came, your traditional embedding database gave you nightmares in terms of the latency as well as stability.Now, Think of an open-source embedding database designed to seamlessly handle a variety of setups and effortlessly scales up to billions of vectors locally., scales up to billions of embeddings, fetch the relevant embeddings with amazing searching capabilities and data never leaves your local machine, feels too good to be true right?! Well there is LanceDB again. I mean from the moment you dirty your hands for your next RAG system all upto the time you put something as big as production, LanceDB scales amazingly well…MultimodalityThe current embedded databases should not only support textual data but also be compatible with various file formats. It’s no longer just about text ingestion. Some time ago, I developed a GTA-5 multimodal RAG application that displays the GTA5 landscape upon entering a query. I highly recommend giving it a read to understand why the Multimodal RAG system is the new hotshot and why companies are eager to integrate it into their core. Honestly, I haven’t come across any embedded vector database other than LanceDB that can effortlessly ingest any kind of file format.By the way, multimodality does make sense because LanceDB is built on top of the Lance format. As mentioned earlier, it supports diverse data types, including images, audio, and text, making it incredibly flexible for applications with various data formats.Searching and IntegrationsOk, so we stored the Embeddings, we scaled our RAG too, now for a given query, we want to find the relevant embeddings, and that’s where LanceDB shines. Now Searching in LanceDB is as easy as it could be, you can just query your data in a number of ways - via SQL, full-text search , and vector search. As it supports the Hybrid search too which is one of the biggest favorable for the amazing search capabilities in LanceDB.But it’s not just about searching guys, it integrates well enough with native Python, JavaScript/TypeScript, Pandas, Pydantic, that means you can easily integrate it with your favorite programming languages, in addition to that, it has direct integrations with cloud storage providers like AWS S3 and Azure Blob Storage. This means that we can directly query data stored on the cloud, without any added ETL steps.Woah, I do love LanceDBMore or less, we looked at these things right :  Columnar Oriented Databases  Lance Format  In-process  On-disk storage  Serverless  Embedded systems.  Scalability  Multimodality  Searching and IntegrationsWell all of them are bundled together with no cost upfront, ready to serve in one installation click and voila baby, you have your new best friend, maybe more than that, who knows? So what are you waiting for? Here is the reference, see you soon.",
            "content_html": "<p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/embedded_databases/production_fucked.png?raw=true\" alt=\"production_fucked\" /></p><p>In today’s world, when everyone’s curious about trying out generative AI tools and how they work, you’ve probably heard about embedded databases. Most of us tend to think about client-server based setups when databases come to mind. And honestly, that’s somewhat accurate.</p><p>However, client-server architectures aren’t really built to handle heavy analytical and ML workloads. Essentially, the processing tasks fall into two main categories: OLTP (online transactional processing) and OLAP (online analytical processing). So, when you’re changing your Instagram profile picture or uploading a photo on Facebook, you’re essentially involved in OLTP tasks, which focus on quick and easy processing. On the flip side, when we deal with OLAP, it’s all about handling complex computations such as retrieving queries from extensive datasets, combining tables, and aggregating data for big data purposes. Now, we need something that can handle our large ML workloads effectively and perform optimally across datasets ranging from small to large scales.</p><h3 id=\"columnar-oriented-datastores\">Columnar Oriented Datastores</h3><p>Hard drives store data in terms of blocks, so whenever an operation is performed, the entire block containing the data is loaded into memory for reading by the OS. Now, Row-oriented databases aim to store whole rows of the database in the same block, whereas columnar databases store column entries in the same block.</p><p>This implies that when you need to perform column-oriented operations like updating columns, aggregations, or selecting a column entry, column-oriented databases outperform row-oriented ones in terms of speed. However, if you need to add a new data point entry with multiple columns, then row-oriented databases perform better.</p><p>Now, the point is, there’s something called Apache Arrow, which is a language-agnostic columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs.</p><p>This is too technical. Let me break it down for you. Machine learning is all about feeding huge amounts of data into complex mathematical models to find patterns and make predictions, right? Now, Apache Arrow turbocharges this process by providing a standardized way to store and work with data that’s <a href=\"https://developer.nvidia.com/blog/accelerating-apache-spark-3-0-with-gpus-and-rapids/\">optimized</a> for modern hardware like powerful GPUs. So Instead of dealing with clunky row-based formats, Arrow’s columnar layout lets you focus on the specific data features you need, drastically cutting down processing time. And since Arrow keeps data in memory, AKA RAM, rather than on sluggish disk storage like hard drives or SSDs, your models can crunch through datasets at blistering speeds. The end result? You can iterate faster, train better models, and stay ahead of the competition.</p><p>Still confused, right? I was too. Well, let’s take an example. If you’re building a model to predict housing prices based on factors like square footage, number of bedrooms, location, etc., Arrow’s columnar format would allow you to easily isolate and work with just the columns containing those specific features, ignoring any other irrelevant data columns. This focused, selective data access is more efficient than dealing with row-based formats where you’d have to sort through all the data indiscriminately.</p><p>Now that’s where the power of Arrow based columnar databases comes into play.</p><h3 id=\"lance-data-format\">Lance Data Format</h3><p>Building on the advantages of Apache Arrow’s columnar, in-memory approach for machine learning, there’s another game-changing data format that takes things to a whole new level – the Lance Data Format.</p><p>Designed from the ground up with modern ML workflows in mind, Lance is an absolute speed demon when it comes to querying and prepping data for training models. But it’s not just about raw speed – Lance has some seriously impressive versatility under the hood.</p><p>Unlike formats limited to tabular data, Lance can handle all kinds of data types like images, videos, 3D point clouds, audio, and more. It’s like a Swiss Army knife of data formats for ML. Btw, Don’t just take my word for it because I love LanceDB, instead – <a href=\"https://blog.lancedb.com/announcing-lancedb-5cb0deaa46ee-2/\">benchmarks</a> have shown that Lance can provide random data access involving read and write operation a mind-boggling approximately 1000 times faster than Parquet, another popular columnar format. This blistering speed comes from unique storage memory layout used by Lance.</p><p>The other important thing LanceDB provides is the usage of Zero-copy versioning, essentially it means that when you create a new version of your data, LanceDB doesn’t have to make an entire copy – it just stores the changes efficiently. This saves a ton of time and storage space compared to traditional versioning methods. And optimized vector operations allow Lance to process data in bulk, taking full advantage of modern hardware like GPUs and vectorized CPUs. It’s all part of Lance’s cloud-native design.</p><h3 id=\"in-process\">In-process</h3><p>Before understanding what Embedded Systems really do, First, we need to understand what a database management system (DBMS) is in Layman. Now in simple terms a DBMS is a software system that allows you to create, manage, and interact with databases (obviously duhh). I mean It provides a way to store, retrieve, and manipulate data in an organized and more efficient manner.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/embedded_databases/in_process_setting.png?raw=true\" alt=\"inprocess_setting\" /></p><p>Now, an embedded database is a type of DBMS that is tightly integrated with the application layer. This means that the database that you are working with is not a separate process or service running on its own; instead, it runs within the same process as the application itself.</p><p>This tight integration makes it easier for the application to interact with the database, as there’s no need for inter-process communication or network communication unlike traditional client-server database systems where the database is a  separate process or a service</p><p>This type of thing is called “in-process”. Remember this for the rest of your life. It might be the most important thing to remember when judging the other embedded databases out there.</p><p>In simple terms, the application can directly access and manipulate the database without going through additional layers or protocols. CUTTING THE BS.</p><h3 id=\"on-disk-storage\"><strong>On-disk storage</strong></h3><p>Remember I said earlier that Embedded systems have this ability to store the data in-memory and work at blistering speeds. While embedded databases can store data in memory, they also have this capability to store larger datasets on disk. This allows them to scale to large amounts of data (terabytes or billion dataset points, think of embeddings of billion tokens) while still providing relatively low query latencies and response times. So it’s like the best of the both worlds.</p><h3 id=\"serverless\">Serverless</h3><p>Ok new term! Sometimes the terms “Embedded” and “Serverless” are sometimes used interchangeably in the database community, but they actually refer to different concepts. “Embedded” refers to the database being tightly integrated with the application as we seen earlier in case of Embedded databases while Serverless refers to the separation of storage and compute resources, and it’s often used in the context of microservices architectures.</p><p>To make it more concise, think of the serverless database as it’s composed of two different containers, the storage layer (where the data is stored) and the compute layer (where the data is processed). Now this separation allows the compute resources to be dynamically allocated and scaled up or down based on the workload.</p><p>And btw, when we are talking about the Serverless model, it is often associated with the cloud based services, where you don’t have to manage the underlying infra..</p><h3 id=\"scalability\">Scalability</h3><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/embedded_databases/lancedb_scaled.png?raw=true\" alt=\"lancedb_scaled\" /></p><p>Ok so you decided to work on the simple RAG system and wanted to test it out or just maybe play with it and came up with the various vector databases, you did your experiment,  you are happy and done. Now when you got serious and came up with something and wanted to scale your RAG system for let’s say 1 billion embeddings, you open up your earlier setup, ingested more data, created more embeddings and when the time came, your traditional embedding database gave you nightmares in terms of the latency as well as stability.</p><p>Now, Think of an open-source embedding database designed to seamlessly handle a variety of setups and effortlessly scales up to billions of vectors locally., scales up to billions of embeddings, fetch the relevant embeddings with amazing searching capabilities and data never leaves your local machine, feels too good to be true right?! Well there is LanceDB again. I mean from the moment you dirty your hands for your next RAG system all upto the time you put something as big as production, LanceDB scales amazingly well…</p><h3 id=\"multimodality\">Multimodality</h3><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/embedded_databases/multimodality_in_lancedb.png?raw=true\" alt=\"multimodality\" /></p><p>The current embedded databases should not only support textual data but also be compatible with various file formats. It’s no longer just about text ingestion. Some time ago, I developed a GTA-5 multimodal RAG application that displays the GTA5 landscape upon entering a query. I highly recommend giving it a <a href=\"https://vipul-maheshwari.github.io/2024/03/03/multimodal-rag-application\">read</a> to understand why the Multimodal RAG system is the new hotshot and why companies are eager to integrate it into their core. Honestly, I haven’t come across any embedded vector database other than LanceDB that can effortlessly ingest any kind of file format.</p><p>By the way, multimodality does make sense because LanceDB is built on top of the Lance format. As mentioned earlier, it supports diverse data types, including images, audio, and text, making it incredibly flexible for applications with various data formats.</p><h3 id=\"searching-and-integrations\">Searching and Integrations</h3><p>Ok, so we stored the Embeddings, we scaled our RAG too, now for a given query, we want to find the relevant embeddings, and that’s where LanceDB shines. Now Searching in LanceDB is as easy as it could be, you can just query your data in a number of ways - via SQL, full-text search , and vector search. As it supports the Hybrid search too which is one of the biggest favorable for the amazing search capabilities in LanceDB.</p><p>But it’s not just about searching guys, it integrates well enough with native Python, JavaScript/TypeScript, Pandas, Pydantic, that means you can easily integrate it with your favorite programming languages, in addition to that, it has direct integrations with cloud storage providers like AWS S3 and Azure Blob Storage. This means that we can directly query data stored on the cloud, without any added ETL steps.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/embedded_databases/grandma_knows.png?raw=true\" alt=\"grandma_knows\" /></p><p>Woah, I do love LanceDB</p><h3 id=\"more-or-less-we-looked-at-these-things-right-\">More or less, we looked at these things right :</h3><ol>  <li>Columnar Oriented Databases</li>  <li>Lance Format</li>  <li>In-process</li>  <li>On-disk storage</li>  <li>Serverless</li>  <li>Embedded systems.</li>  <li>Scalability</li>  <li>Multimodality</li>  <li>Searching and Integrations</li></ol><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/embedded_databases/gen_ai_applications.png?raw=true\" alt=\"gen_ai_application\" /></p><p>Well all of them are bundled together with no cost upfront, ready to serve in one installation click and voila baby, you have your new best friend, maybe more than that, who knows? So what are you waiting for? Here is the <a href=\"https://lancedb.com/\">reference</a>, see you soon.</p>",
            "url": "http://localhost:4000/2024/03/15/embedded-databases",
            
            
            
            "tags": ["LLM","Embedded Databases","LanceDB"],
            
            "date_published": "2024-03-15T00:00:00+05:30",
            "date_modified": "2024-03-15T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/03/03/multimodal-rag-application",
            "title": "Multimodal RAG applications",
            "summary": "Multimodal RAG applications using lanceDB",
            "content_text": "Artificial Intelligence (AI) has been actively working with text for quite some time, but the world isn’t solely centered around words. If you take a moment to look around, you’ll find a mix of text, images, videos, audios, and their combinations.Today we are going to work on Multimodality which is basically a concept that essentially empowers AI models with the capacity to perceive, listen, and comprehend data in diverse formats together with the text. Pretty much like how we do!In an ideal situation, we should be able to mix different types of data together and show them to a generative AI model at the same time and iterate on it. It could be as simple as telling the AI model, “Hey, a few days ago, I sent you a picture of a brown, short dog. Can you find that picture for me?” and the model should then give us the details of that picture. Basically, we want the AI to understand things more like how we humans do,  becoming really good at handling and responding to all kinds of information.But the challenge here is to make a computer understand one data format with its related reference, and that could be a mix of text, audio, thermal imagery, and videos. Now to make this happen, we use something called Embeddings. It’s really a numeric vector which contains a bunch of numbers written together that might not mean much to us but are understood by machines very well.Cat is equal to CatLet’s think of the text components for now, so we are currently aiming that our model should learn that words like “Dog” and “Cat” are closely linked to the word “Pet.” Now this understanding is easily achievable by using an embedding model which will convert these text words into their respective embeddings first and then the model is trained to follow a straightforward logic: if words are related, they are close together in the vector space, if not, they would be separated by the adequate distance.But to help a model recognize that an image of a “Cat” and the word “Cat” are similar, we rely on Multimodal Embeddings. To simplify things a bit, imagine there is a magic box which is capable of handling various inputs – images, audios, text, and more.Now, when we feed the box with an image of a “Cat” with the text “Cat,” it performs its magic and produces two numeric vectors. When these two vectors were given to a machine, it made machines think, “Hmm, based on these numeric values, it seems like both are connected to “Cat”. So that’s exactly what we were aiming for! Our goal was to help machines to recognize the close connection between an image of a “Cat” and the text “Cat”. However, to validate this concept, when we plot those two numeric vectors in a vector space, it turns out they are very close to each other. This outcome exactly mirrors what we observed earlier with the proximity of the two text words “Cat” and “Dog” in the vector space.Ladies and gentlemen, that’s the essence of Multimodality. 👏So we made our model to comprehend the association between “Cat” images and the word “Cat.” Well this is it, I mean if you are able to do this, you would have ingested the audio, images, videos as well as the word “Cat” and the model will understand how the cat is being portrayed across all kinds of file format..RAG is here..Well if you don’t know what RAG means, I would highly advise you to read this article here which I wrote some days back and loved by tons of people, not exaggerating it but yeah, it’s good to get the balls rolling..So there are impressive models like DALLE-2 that provide text-to-image functionality. Essentially, you input text, and the model generates relevant images for you. But can we create a system similar to Multimodal RAG, where the model produces output images based on our own data? Alright, so the goal for today is to create an AI model that when asked something like, “How many girls were there in my party?” 💀 not only provides textual information but also includes a relevant image related to it. Think of it as an extension of a simple RAG system, but now incorporating images.Before we dive in, remember that Multimodality isn’t limited to just text-to-image or image-to-text as it encompasses the freedom to input and output any type of data. However, for now, let’s concentrate on the interaction from image to text exclusively.Contrastive learningNow the question is, What exactly was that box doing? The magic it performs is known as Contrastive Learning. While the term might sound complex, it’s not that tricky. To simplify, consider a dataset with images, along with a caption describing what the image represents.Alright, now what happens is: we give our text-image model with these Positive and Negative samples, where each sample consists of an image and a descriptive text. Positive samples are those where the image and text are correctly aligned – for instance, a picture of a cat matched with the text “this is an image of a cat.” Conversely, negative samples involve a mismatch, like presenting an image of a dog alongside the text “this is an image of a cat.”Now we train our text-image model to recognize that positive samples offer accurate interpretations, while negative samples are misleading and should be disregarded during training. In formal terms this technique is called CLIP (Contrastive Language-Image Pre-training) introduced by OpenAI where authors trained an image-text model on something around 400 million image caption pairs taken from the internet and everytime model makes a mistake, the contrastive loss function increases and penalize it to make sure the model trains well. The same kind of principles are applied to the other modality combinations as well, so the voice of cat with the word cat is a positive sample for speech-text model, a video of cat with the descriptive text “this is a cat” is a positive sample for video-text model.Show timeWell you don’t have to build that box from scratch because folks have already done it for us. There’s a Multimodal embedding model, like the “ViT-L/14” from OpenAI. This model can handle various data types, including text, images, videos, audios, and even thermal and gyroscope data. Now, onto the next question: how do we store those embeddings?For that we’ll need a vector database that can efficiently fetch, query, and retrieve relevant embeddings for us,  ideally one that supports multimodal data and doesn’t burn a hole in our wallets. That’s where LanceDB comes into play.Vector databaseWhen we talk about the vector database, there are ton of options available in the current market, but there is something about the LanceDB which makes it stands out as an optimal choice for a vector database, As far as I have used it, it address the limitations of traditional embedded databases in handling AI/ML workloads. When I say traditional, it typically means those database management tools which are not aligned with the usage of heavy computation that comes with the ML infra.TLDR; LanceDB operates on a serverless architecture, meaning storage and compute are separated into two distinct units. This design makes it exceptionally fast for RAG use cases, ensuring fast fetching and retrieval. Additionally, it has some notable advantages – being open source, utilizing its Lance columnar data format built on top of Apache Arrow for high efficiency, persistent storage capabilities, and incorporating its own Disk Approximate Nearest Neighbor search. All these factors collectively make LanceDB an ideal solution for accessing and working with multimodal data. I love you LanceDB ❤️.Data timeTo add some excitement, I’ve crafted a GTA-V Image Captioning dataset, featuring thousands of images, each paired with a descriptive text illustrating the image’s content. Now, when we train our magic box, the expectation is clear – if I ask that box to provide me an image of “road with a stop sign,” it should deliver a GTA-V image of a road with a stop sign on it. Otherwise, what’s the point, right?FAQ  We will be using “ViT-L/14” to convert our multimodal data into its respective embeddings.  LanceDB as our vector database to store the relevant embeddings.  GTA-V Image Captioning dataset for our magic box.Environment SetupI am using a MacBook Air M1, and it’s important to note that some kinds of dependencies and configurations may vary depending on the type of system that you are running, so it’s important to take that into account.Here are the steps to install the relevant dependencies# Create a virtual environmentpython3 -m venv env# Activate the virtual environmentsource env/bin/activate# Upgrade pip in the virtual environmentpip install --upgrade pip# Install required dependenciespip3 install lancedb clip torch datasets pillow pip3 install git+https://github.com/openai/CLIP.gitAnd don’t forget to get your access token from the hugging face to download the data.Downloading the DataDataset can easily be fetched using the datasets library.import clipimport torchimport osfrom datasets import load_datasetds = load_dataset(\"vipulmaheshwari/GTA-Image-Captioning-Dataset\")device = torch.device(\"mps\")model, preprocess = clip.load(\"ViT-L-14\", device=device)Downloading the dataset may require some time, so please take a moment to relax while this process completes. Once the download is finished, you can visualize some sample points like this:from textwrap import wrapimport matplotlib.pyplot as pltimport numpy as npdef plot_images(images, captions):    plt.figure(figsize=(15, 7))    for i in range(len(images)):        ax = plt.subplot(1, len(images), i + 1)        caption = captions[i]        caption = \"\\n\".join(wrap(caption, 12))        plt.title(caption)        plt.imshow(images[i])        plt.axis(\"off\")# Assuming ds is a dictionary with \"train\" key containing a list of samplessample_dataset = ds[\"train\"]random_indices = np.random.choice(len(sample_dataset), size=2, replace=False)random_indices = [index.item() for index in random_indices]# Get the random images and their captionsrandom_images = [np.array(sample_dataset[index][\"image\"]) for index in random_indices]random_captions = [sample_dataset[index][\"text\"] for index in random_indices]# Plot the random images with their captionsplot_images(random_images, random_captions)# Show the plotplt.show()Storing the EmbeddingsThe dataset consists of two key features: the image and its corresponding descriptive text. Initially, our task is to create a LanceDB table to store the embeddings. This process is straightforward – all you need to do is define the relevant schema. In our case, the columns include “vector” for storing the multimodal embeddings, a “text” column for the descriptive text, and a “label” column for the corresponding IDs.import pyarrow as paimport lancedbimport tqdmdb = lancedb.connect('./data/tables')schema = pa.schema(  [      pa.field(\"vector\", pa.list_(pa.float32(), 512)),      pa.field(\"text\", pa.string()),      pa.field(\"id\", pa.int32())  ])tbl = db.create_table(\"gta_data\", schema=schema, mode=\"overwrite\")Executing this will generate a table with the specified schema, and it’s ready to store the embeddings along with the relevant columns. It’s as straightforward as that – almost too easy!Encode the ImagesNow, we’ll simply take the images from the dataset, feed them into an encoding function that leverages our Multimodal Embedding model, and generate the corresponding embeddings. These embeddings will then be stored in the database.def embed_image(img):    processed_image = preprocess(img)    unsqueezed_image = processed_image.unsqueeze(0).to(device)    embeddings = model.encode_image(unsqueezed_image)        # Detach, move to CPU, convert to numpy array, and extract the first element as a list    result = embeddings.detach().cpu().numpy()[0].tolist()    return resultSo our embed_image function takes an input image, prepocesses it through our CLIP model preprocessor, encode the preprocessed image and returns a list representing the embeddings of that image. This returned embedding serves as a concise numerical representation, capturing all the key features and patterns within the image for downstream tasks or analysis. The next thing is to call this function for all the images and store the relevant embeddings in the database.data = []for i in range(len(ds[\"train\"])):    img = ds[\"train\"][i]['image']    text = ds[\"train\"][i]['text']        # Encode the image    encoded_img = embed_image(img)    data.append({\"vector\": encoded_img, \"text\": text, \"id\" : i})Here, we’re just taking a list, adding the numeric embeddings, reference text and the current index id to it. All that’s left is to include this list in our LanceDB table. And voila, our datalake for the embeddings is set up and good to go!tbl.add(data)tbl.to_pandas()Up until now, we’ve efficiently converted the images into their respective multimodal embeddings and stored them in the LanceDB table. Now the LanceDB tables offer a convenient feature: if there’s a need to add or remove images, it’s remarkably straightforward. Just encode the new image and add it, following to the same steps we followed for the previous images.Query searchOur next move is to embed our text query using the same multimodal embedding model we used for our images. Remember that “box” I mentioned earlier? Essentially, we want this box to create embeddings for both our images and our texts which ensures that the representation of different types of data happens in the same way. Following this, we just need to initiate a search to find the nearest image embeddings that matches our text query.def embed_txt(txt):    tokenized_text = clip.tokenize([txt]).to(device)    embeddings = model.encode_text(tokenized_text)        # Detach, move to CPU, convert to numpy array, and extract the first element as a list    result = embeddings.detach().cpu().numpy()[0].tolist()    return resultres = tbl.search(embed_txt(\"a road with a stop\")).limit(3).to_pandas()res0 | [0.064575195, .. ] | there is a stop sign...| 569 |\t131.9957281 | [-0.07989502, .. ] | there is a bus that... | 423 | 135.0478522 | [0.06756592, .. ]  | amazing view of a ...\t| 30  | 135.309937Let’s slow down a bit and understand what just happened. Putting simply, the code snippet executes a search algorithm in its core to pinpoint the most relevant image embedding that aligns with our text query. The resulting output, as showcased above, gives us the embeddings which closely resembles our text query.  In the result, the second column presents the embedding vector, while the third column contains the description of the image that closely matches our text query. Essentially, we’ve determined which image closely corresponds to our text query by examining the embeddings of both our text query and the image.It’s similar to saying, If these numbers represent the word “Cat”, I spot an image with a similar set of numbers, so most likely it’s a match for an image of a “Cat”. 😺If you are looking for the explanation of how the search happens, I will write a detailed explanation in the coming write ups because it’s so exciting to look under the hood and see how the searching happens. Essentially there is something called Approximate Nearest Neighbors (ANN) which is a technique used to efficiently find the closest points in high-dimensional spaces. ANN is extensively used in data mining, machine learning, computer vision and NLP use cases. So when we passed our embedded text query to the searching algorithm and asked it to give us the closest sample point in the vector space, it used a type of ANN algorithm to get it for us. Specifically LanceDB utilizes DANN (Deep Approximate Nearest Neighbor) for searching the relevant embeddings within its ecosystem..In our results, we have five columns. The first is the index number, the second is the embedding vector, the third is the description of the image matching our text query, and the fourth is the label of the image. However, let’s focus on the last column – Distance. When I mentioned the ANN algorithm, it simply draws a line between the current data point (in our case, the embedding of our text query) and identifies which data point (image embedding) is closest to it. If you observe that the other data points in the results have a greater distance compared to the top one, it indicates they are a bit further away or more unrelated to our query. Just to make it clear, the calculation of distance is a part of the algorithm itself.D-DAYNow that we have all the necessary information, displaying the most relevant image for our query is straightforward. Simply take the relevant label of the top-matched embedding vector and showcase the corresponding image.data_id = int(res['id'][0])display(ds[\"train\"][data_id]['image'])print(ds[\"train\"][data_id]['text'])there is a truck driving down a street with a stop signWhat’s next?To make things more interesting, I’m currently working on creating an extensive GTA-V captioning dataset. This dataset will include a larger number of images paired with their respective reference text, providing us with a richer set of queries to explore and experiment with.. Nevertheless, there’s always room for refining the model. We can explore creating a customized CLIP model, adjusting various parameters. Increasing the number of training epochs may afford the model more time to grasp the relevance between embeddings. Additionally, there’s an impressive multimodal embedding model developed by the Meta known as ImageBind. We can consider trying ImageBind as an alternative to our current multimodal embedding model and compare the outcomes. With numerous options available, the fundamental concept behind the Multimodal RAG workflow remains largely consistent.Here’s how everything comes together in one frame and this is the Collab for your reference",
            "content_html": "<p><em>Artificial Intelligence (AI) has been actively working with text for quite some time, but the world isn’t solely centered around words. If you take a moment to look around, you’ll find a mix of text, images, videos, audios, and their combinations.</em></p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/Renevant%20Cheetah-66.jpg?raw=true\" alt=\"boomer_ai\" /></p><p>Today we are going to work on Multimodality which is basically a concept that essentially empowers AI models with the capacity to perceive, listen, and comprehend data in diverse formats together with the text. Pretty much like how we do!</p><p>In an ideal situation, we should be able to mix different types of data together and show them to a generative AI model at the same time and iterate on it. It could be as simple as telling the AI model, “Hey, a few days ago, I sent you a picture of a brown, short dog. Can you find that picture for me?” and the model should then give us the details of that picture. Basically, we want the AI to understand things more like how we humans do,  becoming really good at handling and responding to all kinds of information.</p><p>But the challenge here is to make a computer understand one data format with its related reference, and that could be a mix of text, audio, thermal imagery, and videos. Now to make this happen, we use something called Embeddings. It’s really a numeric vector which contains a bunch of numbers written together that might not mean much to us but are understood by machines very well.</p><h3 id=\"cat-is-equal-to-cat\">Cat is equal to Cat</h3><p>Let’s think of the text components for now, so we are currently aiming that our model should learn that words like “Dog” and “Cat” are closely linked to the word “Pet.” Now this understanding is easily achievable by using an embedding model which will convert these text words into their respective embeddings first and then the model is trained to follow a straightforward logic: if words are related, they are close together in the vector space, if not, they would be separated by the adequate distance.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/embeddings.png?raw=true\" alt=\"embeddings\" /></p><p>But to help a model recognize that an image of a “Cat” and the word “Cat” are similar, we rely on Multimodal Embeddings. To simplify things a bit, imagine there is a magic box which is capable of handling various inputs – images, audios, text, and more.</p><p>Now, when we feed the box with an image of a “Cat” with the text “Cat,” it performs its magic and produces two numeric vectors. When these two vectors were given to a machine, it made machines think, “Hmm, based on these numeric values, it seems like both are connected to “Cat”. So that’s exactly what we were aiming for! Our goal was to help machines to recognize the close connection between an image of a “Cat” and the text “Cat”. However, to validate this concept, when we plot those two numeric vectors in a vector space, it turns out they are very close to each other. This outcome exactly mirrors what we observed earlier with the proximity of the two text words “Cat” and “Dog” in the vector space.</p><h2 id=\"ladies-and-gentlemen-thats-the-essence-of-multimodality-\">Ladies and gentlemen, that’s the essence of Multimodality. 👏</h2><p>So we made our model to comprehend the association between “Cat” images and the word “Cat.” Well this is it, I mean if you are able to do this, you would have ingested the audio, images, videos as well as the word “Cat” and the model will understand how the cat is being portrayed across all kinds of file format..</p><h3 id=\"rag-is-here\">RAG is here..</h3><p>Well if you don’t know what RAG means, I would highly advise you to read this article <a href=\"https://vipul-maheshwari.github.io/2024/02/14/rag-application-with-langchain\">here</a> which I wrote some days back and loved by tons of people, not exaggerating it but yeah, it’s good to get the balls rolling..</p><p>So there are impressive models like DALLE-2 that provide text-to-image functionality. Essentially, you input text, and the model generates relevant images for you. But can we create a system similar to Multimodal RAG, where the model produces output images based on our own data? Alright, so the goal for today is to create an AI model that when asked something like, “How many girls were there in my party?” 💀 not only provides textual information but also includes a relevant image related to it. Think of it as an extension of a simple RAG system, but now incorporating images.</p><p>Before we dive in, remember that Multimodality isn’t limited to just text-to-image or image-to-text as it encompasses the freedom to input and output any type of data. However, for now, let’s concentrate on the interaction from image to text exclusively.</p><h3 id=\"contrastive-learning\">Contrastive learning</h3><p>Now the question is, What exactly was that box doing? The magic it performs is known as Contrastive Learning. While the term might sound complex, it’s not that tricky. To simplify, consider a dataset with images, along with a caption describing what the image represents.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/clipmodel.png?raw=true\" alt=\"clipmodel\" /></p><p>Alright, now what happens is: we give our text-image model with these Positive and Negative samples, where each sample consists of an image and a descriptive text. Positive samples are those where the image and text are correctly aligned – for instance, a picture of a cat matched with the text “this is an image of a cat.” Conversely, negative samples involve a mismatch, like presenting an image of a dog alongside the text “this is an image of a cat.”</p><p>Now we train our text-image model to recognize that positive samples offer accurate interpretations, while negative samples are misleading and should be disregarded during training. In formal terms this technique is called <a href=\"https://openai.com/research/clip\">CLIP</a> (Contrastive Language-Image Pre-training) introduced by OpenAI where authors trained an image-text model on something around 400 million image caption pairs taken from the internet and everytime model makes a mistake, the contrastive loss function increases and penalize it to make sure the model trains well. The same kind of principles are applied to the other modality combinations as well, so the voice of cat with the word cat is a positive sample for speech-text model, a video of cat with the descriptive text “this is a cat” is a positive sample for video-text model.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/easy.png?raw=true\" alt=\"easy\" /></p><h3 id=\"show-time\">Show time</h3><p>Well you don’t have to build that box from scratch because folks have already done it for us. There’s a Multimodal embedding model, like the “ViT-L/14” from OpenAI. This model can handle various data types, including text, images, videos, audios, and even thermal and gyroscope data. Now, onto the next question: how do we store those embeddings?</p><p>For that we’ll need a vector database that can efficiently fetch, query, and retrieve relevant embeddings for us,  ideally one that supports multimodal data and doesn’t burn a hole in our wallets. That’s where LanceDB comes into play.</p><h3 id=\"vector-database\">Vector database</h3><p>When we talk about the vector database, there are ton of options available in the current market, but there is something about the LanceDB which makes it stands out as an optimal choice for a vector database, As far as I have used it, it address the limitations of traditional embedded databases in handling AI/ML workloads. When I say traditional, it typically means those database management tools which are not aligned with the usage of heavy computation that comes with the ML infra.</p><p>TLDR; LanceDB operates on a serverless architecture, meaning storage and compute are separated into two distinct units. This design makes it exceptionally fast for RAG use cases, ensuring fast fetching and retrieval. Additionally, it has some notable advantages – being open source, utilizing its Lance columnar data format built on top of Apache Arrow for high efficiency, persistent storage capabilities, and incorporating its own Disk Approximate Nearest Neighbor search. All these factors collectively make LanceDB an ideal solution for accessing and working with multimodal data. I love you LanceDB ❤️.</p><h3 id=\"data-time\">Data time</h3><p>To add some excitement, I’ve crafted a GTA-V Image Captioning dataset, featuring thousands of images, each paired with a descriptive text illustrating the image’s content. Now, when we train our magic box, the expectation is clear – if I ask that box to provide me an image of “road with a stop sign,” it should deliver a GTA-V image of a road with a stop sign on it. Otherwise, what’s the point, right?</p><h3 id=\"faq\">FAQ</h3><ol>  <li>We will be using “ViT-L/14” to convert our multimodal data into its respective embeddings.</li>  <li>LanceDB as our vector database to store the relevant embeddings.</li>  <li>GTA-V Image Captioning dataset for our magic box.</li></ol><h3 id=\"environment-setup\">Environment Setup</h3><p>I am using a MacBook Air M1, and it’s important to note that some kinds of dependencies and configurations may vary depending on the type of system that you are running, so it’s important to take that into account.</p><p>Here are the steps to install the relevant dependencies</p><pre><code class=\"language-python\"># Create a virtual environmentpython3 -m venv env# Activate the virtual environmentsource env/bin/activate# Upgrade pip in the virtual environmentpip install --upgrade pip# Install required dependenciespip3 install lancedb clip torch datasets pillow pip3 install git+https://github.com/openai/CLIP.git</code></pre><p>And don’t forget to get your access token from the hugging face to download the data.</p><h3 id=\"downloading-the-data\">Downloading the Data</h3><p>Dataset can easily be fetched using the datasets library.</p><pre><code class=\"language-python\">import clipimport torchimport osfrom datasets import load_datasetds = load_dataset(\"vipulmaheshwari/GTA-Image-Captioning-Dataset\")device = torch.device(\"mps\")model, preprocess = clip.load(\"ViT-L-14\", device=device)</code></pre><p>Downloading the dataset may require some time, so please take a moment to relax while this process completes. Once the download is finished, you can visualize some sample points like this:</p><pre><code class=\"language-python\">from textwrap import wrapimport matplotlib.pyplot as pltimport numpy as npdef plot_images(images, captions):    plt.figure(figsize=(15, 7))    for i in range(len(images)):        ax = plt.subplot(1, len(images), i + 1)        caption = captions[i]        caption = \"\\n\".join(wrap(caption, 12))        plt.title(caption)        plt.imshow(images[i])        plt.axis(\"off\")# Assuming ds is a dictionary with \"train\" key containing a list of samplessample_dataset = ds[\"train\"]random_indices = np.random.choice(len(sample_dataset), size=2, replace=False)random_indices = [index.item() for index in random_indices]# Get the random images and their captionsrandom_images = [np.array(sample_dataset[index][\"image\"]) for index in random_indices]random_captions = [sample_dataset[index][\"text\"] for index in random_indices]# Plot the random images with their captionsplot_images(random_images, random_captions)# Show the plotplt.show()</code></pre><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/output3.png?raw=true\" alt=\"output3\" /></p><h3 id=\"storing-the-embeddings\">Storing the Embeddings</h3><p>The dataset consists of two key features: the image and its corresponding descriptive text. Initially, our task is to create a LanceDB table to store the embeddings. This process is straightforward – all you need to do is define the relevant schema. In our case, the columns include “vector” for storing the multimodal embeddings, a “text” column for the descriptive text, and a “label” column for the corresponding IDs.</p><pre><code class=\"language-python\">import pyarrow as paimport lancedbimport tqdmdb = lancedb.connect('./data/tables')schema = pa.schema(  [      pa.field(\"vector\", pa.list_(pa.float32(), 512)),      pa.field(\"text\", pa.string()),      pa.field(\"id\", pa.int32())  ])tbl = db.create_table(\"gta_data\", schema=schema, mode=\"overwrite\")</code></pre><p>Executing this will generate a table with the specified schema, and it’s ready to store the embeddings along with the relevant columns. It’s as straightforward as that – almost too easy!</p><h3 id=\"encode-the-images\">Encode the Images</h3><p>Now, we’ll simply take the images from the dataset, feed them into an encoding function that leverages our Multimodal Embedding model, and generate the corresponding embeddings. These embeddings will then be stored in the database.</p><pre><code class=\"language-python\">def embed_image(img):    processed_image = preprocess(img)    unsqueezed_image = processed_image.unsqueeze(0).to(device)    embeddings = model.encode_image(unsqueezed_image)        # Detach, move to CPU, convert to numpy array, and extract the first element as a list    result = embeddings.detach().cpu().numpy()[0].tolist()    return result</code></pre><p>So our <code>embed_image</code> function takes an input image, prepocesses it through our CLIP model preprocessor, encode the preprocessed image and returns a list representing the embeddings of that image. This returned embedding serves as a concise numerical representation, capturing all the key features and patterns within the image for downstream tasks or analysis. The next thing is to call this function for all the images and store the relevant embeddings in the database.</p><pre><code class=\"language-python\">data = []for i in range(len(ds[\"train\"])):    img = ds[\"train\"][i]['image']    text = ds[\"train\"][i]['text']        # Encode the image    encoded_img = embed_image(img)    data.append({\"vector\": encoded_img, \"text\": text, \"id\" : i})</code></pre><p>Here, we’re just taking a list, adding the numeric embeddings, reference text and the current index id to it. All that’s left is to include this list in our LanceDB table. And voila, our datalake for the embeddings is set up and good to go!</p><pre><code class=\"language-python\">tbl.add(data)tbl.to_pandas()</code></pre><p>Up until now, we’ve efficiently converted the images into their respective multimodal embeddings and stored them in the LanceDB table. Now the LanceDB tables offer a convenient feature: if there’s a need to add or remove images, it’s remarkably straightforward. Just encode the new image and add it, following to the same steps we followed for the previous images.</p><h3 id=\"query-search\">Query search</h3><p>Our next move is to embed our text query using the same multimodal embedding model we used for our images. Remember that “box” I mentioned earlier? Essentially, we want this box to create embeddings for both our images and our texts which ensures that the representation of different types of data happens in the same way. Following this, we just need to initiate a search to find the nearest image embeddings that matches our text query.</p><pre><code class=\"language-python\">def embed_txt(txt):    tokenized_text = clip.tokenize([txt]).to(device)    embeddings = model.encode_text(tokenized_text)        # Detach, move to CPU, convert to numpy array, and extract the first element as a list    result = embeddings.detach().cpu().numpy()[0].tolist()    return resultres = tbl.search(embed_txt(\"a road with a stop\")).limit(3).to_pandas()res</code></pre><pre><code class=\"language-txt\">0 | [0.064575195, .. ] | there is a stop sign...| 569 |\t131.9957281 | [-0.07989502, .. ] | there is a bus that... | 423 | 135.0478522 | [0.06756592, .. ]  | amazing view of a ...\t| 30  | 135.309937</code></pre><p>Let’s slow down a bit and understand what just happened. Putting simply, the code snippet executes a search algorithm in its core to pinpoint the most relevant image embedding that aligns with our text query. The resulting output, as showcased above, gives us the embeddings which closely resembles our text query.  In the result, the second column presents the embedding vector, while the third column contains the description of the image that closely matches our text query. Essentially, we’ve determined which image closely corresponds to our text query by examining the embeddings of both our text query and the image.</p><h3 id=\"its-similar-to-saying-if-these-numbers-represent-the-word-cat-i-spot-an-image-with-a-similar-set-of-numbers-so-most-likely-its-a-match-for-an-image-of-a-cat-\">It’s similar to saying, If these numbers represent the word “Cat”, I spot an image with a similar set of numbers, so most likely it’s a match for an image of a “Cat”. 😺</h3><p>If you are looking for the explanation of how the search happens, I will write a detailed explanation in the coming write ups because it’s so exciting to look under the hood and see how the searching happens. Essentially there is something called Approximate Nearest Neighbors (ANN) which is a technique used to efficiently find the closest points in high-dimensional spaces. ANN is extensively used in data mining, machine learning, computer vision and NLP use cases. So when we passed our embedded text query to the searching algorithm and asked it to give us the closest sample point in the vector space, it used a type of ANN algorithm to get it for us. Specifically LanceDB utilizes DANN (Deep Approximate Nearest Neighbor) for searching the relevant embeddings within its ecosystem..</p><p>In our results, we have five columns. The first is the index number, the second is the embedding vector, the third is the description of the image matching our text query, and the fourth is the label of the image. However, let’s focus on the last column – Distance. When I mentioned the ANN algorithm, it simply draws a line between the current data point (in our case, the embedding of our text query) and identifies which data point (image embedding) is closest to it. If you observe that the other data points in the results have a greater distance compared to the top one, it indicates they are a bit further away or more unrelated to our query. Just to make it clear, the calculation of distance is a part of the algorithm itself.</p><h2 id=\"d-day\">D-DAY</h2><p>Now that we have all the necessary information, displaying the most relevant image for our query is straightforward. Simply take the relevant label of the top-matched embedding vector and showcase the corresponding image.</p><pre><code class=\"language-python\">data_id = int(res['id'][0])display(ds[\"train\"][data_id]['image'])print(ds[\"train\"][data_id]['text'])</code></pre><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/output_final.png?raw=true\" alt=\"output_final\" /></p><pre><code class=\"language-python\">there is a truck driving down a street with a stop sign</code></pre><h3 id=\"whats-next\">What’s next?</h3><p>To make things more interesting, I’m currently working on creating an extensive GTA-V captioning dataset. This dataset will include a larger number of images paired with their respective reference text, providing us with a richer set of queries to explore and experiment with.. Nevertheless, there’s always room for refining the model. We can explore creating a customized CLIP model, adjusting various parameters. Increasing the number of training epochs may afford the model more time to grasp the relevance between embeddings. Additionally, there’s an impressive multimodal embedding model developed by the Meta known as <a href=\"https://imagebind.metademolab.com/\">ImageBind</a>. We can consider trying ImageBind as an alternative to our current multimodal embedding model and compare the outcomes. With numerous options available, the fundamental concept behind the Multimodal RAG workflow remains largely consistent.</p><p>Here’s how everything comes together in one frame and this is the <a href=\"https://colab.research.google.com/drive/1LM-WrDSBXpiMZ94CtaMCaGHlkxqGR6WK?usp=sharing\">Collab</a> for your reference</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/multimodalrag.png?raw=true\" alt=\"multimodal_rag\" /></p>",
            "url": "http://localhost:4000/2024/03/03/multimodal-rag-application",
            
            
            
            "tags": ["LLM","Deep Learning"],
            
            "date_published": "2024-03-03T00:00:00+05:30",
            "date_modified": "2024-03-03T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/02/14/rag-application-with-langchain",
            "title": "Create LLM apps using RAG",
            "summary": "RAG and Langcahin for creating the personalized bots",
            "content_text": "If you’re considering making a personalized bot for your documents or your website that responds to you, you’re in the right spot. I’m here to help you create a bot using Langchain and RAG strategies for this purpose.Understanding the Limitations of ChatGPT and LLMsChatGPTs and other Large Language Models (LLMs) are extensively trained on text corpora to comprehend language semantics and coherence. Despite their impressive capabilities, these models have limitations that require careful consideration for particular use cases. One significant challenge is the potential for hallucinations, where the model might generate inaccurate or contextually irrelevant information.Imagine requesting the model to enhance your company policies; in such scenarios, ChatGPTs and other Large Language Models might struggle to provide factual responses because they lack training on your company’s data. Instead, they may generate nonsensical or irrelevant responses, which can be unhelpful. So, how can we ensure that an LLM comprehends our specific data and generates responses accordingly? This is where techniques like Retrieval Augmentation Generation (RAG) come to the rescue.What is RAG?RAG or Retrieval Augmented Generation uses three main workflows to generate and give the better response      Information Retrieval: When a user asks a question, the AI system retrieves the relevant data from a well-maintained knowledge library or external sources like databases, articles, APIs, or document repositories. This is achieved by converting the query into a numerical format or vector that can be understood by machines.        LLM:The retrieved data is then presented to the LLM or Large Language Model, along with the user’s query. The LLM uses this new knowledge and its training data to generate the response.        Response: Finally, the LLM generates a response that is more accurate and relevant since it has been augmented with the retrieved information. I mean we gave LLM some additional information from our Knowledge library which allows LLMs to provide more contextually relevant and factual responses, solving the problem of models when they are just hallucinating or providing irrelevant answers.  Let’s take the example of company policies again. Suppose you have an HR bot that handles queries related to your Company policies. Now if someone asks anything specific to the policies, The bot can pull the most recent policy documents from the knowledge library, pass the relevant context to a well crafted prompt which is then passed further to the LLM for generating the response.To make it easier, Imagine a LLM as your knowledgeable friend who seems to know everything, from Geography to Computer Science, from Politics to Philosophy. Now, picture yourself asking this friend a few questions:  “Who handles my laundry on weekends?”  “Who lives next door to me?”  “What brand of peanut butter do I prefer?”Chances are, your friend wouldn’t be able to answer these questions. Most of the time, no. But let’s say this distant friend becomes closer to you over time, he comes to your place regularly, knows your parents very well, you both hangout pretty often, you go on outings, blah blah blah.. You got the point.I mean he is gaining access to personal and insider information about you.  Now, when you pose the same questions, he can somehow answer those questions with more relevance now because he is better suited with your personal insights.Similarly, a LLM, when provided with additional information or access to your data, won’t guess or hallucinate. Instead, it can leverage that access data to provide more relevant and accurate answers.To break it down, here are the exact steps to create any RAG application…  Extract the relevant information from your data sources.  Break the information into the small chunks.  Store the chunks as their embedddings into a vector database.  Create a prompt template which will be fed to the LLM with the query and the context.  Convert the query to it’s relevant embedding using same embedding model.  Fetch k number of relevant documents related to the query from the vector database.  Pass the relevant documents to the LLM and get the response.FAQs      We will be using Langchain for this task, Basically it’s like a wrapper which lets you talk and manage your LLM operations better. Note that the Langchain is updating very fast and some functions and other classes might moved to the different modules. So if something doesn’t work, just check if you are importing the libraries from the right sources!        Along with it we will be using Hugging Face, an open-source library for building, training, and deploying state-of-the-art machine learning models, especially about NLP. To use the HuggingFace we need the access token, Get your access token here        For our models, we’ll need two key components: a LLM (Large Language Model) and an embedding model. While paid sources like OpenAI offer these, we’ll be utilizing open-source models to ensure accessibility for everyone.        Now we need a Vector Database to store our embeddings, For that task, we’ve got LanceDB – it’s like a super-smart data lake for handling lots of information. It’s a top-notch vector database, making it the go-to choice for dealing with complex data like vector embeddings.. And the best part? It won’t burn a dent in your pocket because it’s open source and free to use!!        To keep things simple, our data ingestion process will involve using a URL and some PDFs. While you can incorporate additional data sources if needed, we’ll concentrate solely on these two for now.  With Langchain for the interface, Hugging Face for fetching the models, along with open-source components, we’re all set to go! This way, we will save some bucks while still having everything we need. Let’s move to the next stepsEnvironment SetupI am using a MacBook Air M1, and it’s important to note that certain dependencies and configurations may vary depending on the type of system you are using. Now open your favorite editor, create a python environment and install the relevant dependencies# Create a virtual environmentpython3 -m venv env# Activate the virtual environmentsource env/bin/activate# Upgrade pip in the virtual environmentpip install --upgrade pip# Install required dependenciespip3 install lancedb langchain langchain_community prettytable sentence-transformers huggingface-hub bs4 pypdf pandas# This is optional, I did it for removing a warningpip3 uninstall urllib3pip3 install 'urllib3&lt;2.0'Now create a .env file in the same directory to place your Hugging Face api credentials like thisHUGGINGFACEHUB_API_TOKEN = hf_KKNWfBqgwCUOHdHFrBwQ.....Ensure the name “HUGGINGFACEHUB_API_TOKEN” remains unchanged, as it is crucial for authentication purposes.If you prefer a straightforward approach without relying on external packages or file loading, you can directly configure the environment variable within your code like this..HF_TOKEN = \"hf_KKNWfBqgwCUOHdHFrBwQ.....\"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKENFinally create a data folder in the project’s root directory, designated as the central repository for storing PDF documents. You can add some sample PDFs for testing purposes; for instance, I am using the Yolo V7 and Transformers paper for demonstration. It’s important to note that this designated folder will function as our primary source for data ingestion.It seems like everything is in order, and we’re all set!Step 1 : Extracting the relevant informationTo get your RAG application running, the first thing we need to do is to extract the relevant information from the various data sources. It can be a website page, a PDF file, a notion link, a google doc whatever it is, it needs to be extracted from it’s original source first.import osfrom langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, DirectoryLoader# Put the token values inside the double quotesHF_TOKEN = \"hf_*******\"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN# Loading the web url and data url_loader = WebBaseLoader(\"https://gameofthrones.fandom.com/wiki/Jon_Snow\")documents_loader = DirectoryLoader('data', glob=\"./*.pdf\", loader_cls=PyPDFLoader)# Creating the instancesurl_docs = url_loader.load()data_docs = documents_loader.load()# Combining all the data that we ingesteddocs = url_docs + data_docsThis will ingest all the data from the URL link and the PDFs.Step 2 : Breaking the information into smaller chunksWe’ve all the necessary data for developing our RAG application. Now, it’s time to break down this information into smaller chunks. Later, we’ll utilize an embedding model to convert these chunks into their respective embeddings. But why is it important?Think of it like this: If you’re tasked with digesting a 100-page book all at once and then asked a specific question about it, it would be challenging to retrieve the necessary information from the entire book to provide an answer. However, if you’re permitted to break the book into smaller, manageable chunks—let’s say 10 pages each—and each chunk is labeled with an index from 0 to 9, the process becomes much simpler. When the same question is posed after this breakdown, you can easily locate the relevant chunk based on its index and then extract the information needed to answer the question accurately.Picture the book as your extracted information, with each 10-page segment representing a small chunk of data, and the index pages as the embedding. Essentially, we’ll apply an embedding model to these chunks to transform the information into their respective embeddings. While as humans, we may not directly comprehend or relate to these embeddings, they serve as numeric representations of the chunks to our application.  This is how you can do this in Pythonfrom langchain.text_splitter import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)chunks = text_splitter.split_documents(docs)Now the chunk_size parameter specifies the maximum number of characters that a chunk can contain, while the chunk_overlap parameter specifies the number of characters that should overlap between two adjacent chunks. With the chunk_overlap set to 50, the last 50 characters of the adjacent chunks will be shared between each other.This approach helps to prevent important information from being split across two chunks, ensuring that each chunk contains sufficient contextual information of their neighbor chunks for the subsequent processing or analysis.Shared information at the boundary of neighboring chunks enables a more seamless transition and understanding of the text’s content. The best strategy for choosing the chunk_size and chunk_overlap parameters largely depends on the nature of the documents and the purpose of the application.Step 3 : Creating the embeddings and store them into a vectordatabaseThere are two primary methods to generate embeddings for our text chunks. The first involves downloading a model, managing preprocessing, and conducting computations independently. Alternatively, we can leverage Hugging Face’s model hub, which offers a variety of pre-trained models for various NLP tasks, including embedding generation.Opting for the latter approach allows us to utilize one of Hugging Face’s embedding models. With this method, we simply provide our text chunks to the chosen model, saving us from the resource-intensive computations on our local machines. 💀Hugging Face’s model hub provides numerous options for embedding models, and you can explore the leaderboard to select the most suitable one for your requirements. For now, we’ll proceed with “sentence-transformers/all-MiniLM-L6-v2.” This model is pretty fast and highly efficient in our task!!from langchain_community.embeddings import HuggingFaceEmbeddingsembedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': 'cpu'})Here’s a way to see the number of embeddings for each chunkquery = \"Hello I want to see the length of the embeddings for this document.\"len(embeddings.embed_documents([query])[0])# 384We have the embeddings for our chunks, now we need a vector database to store them.When it comes to vector databases, there are plenty of options out there suiting various needs. Databases like Pinecone offer adequate performance and advanced features but come with a hefty price tag. On the other hand, open-source alternatives like FAISS or Chroma may lack some extras but are more than sufficient for those not who don’t require extensive scalability.But wait, I am dropping a bomb here, I’ve recently come across LanceDB, So it’s an open-source vector database similar to FAISS and Chroma. What makes LanceDB stand out is not just its open-source nature but its unparalleled scalability. In fact, after a closer look, I realized that I haven’t done justice in highlighting the true value propositions of LanceDB earlier!!Surprisingly, LanceDB is the most scalable vector database available, outperforming even the likes of Pinecone, Chroma, Qdrant, and others. Scaling up to a billion vectors locally on your laptop is a feat only achievable with LanceDB. I mean this capability is a game-changer, especially when you compare it to other vector databases struggling even with a hundred million vectors. What’s more mind blowing is that LanceDB manages to offer this unprecedented scalability at a fraction of cost, I mean they are offering the utilities and database tools at much cheaper rates than its closest counterparts.So now, We’ll create an instance of LanceDB vector database by calling lancedb.connect(\"lance_database\"). This line essentially sets up a connection to the LanceDB database named “lance_database.” Next, we create a table within the database named “rag_sample” using the create_table function. Now we initialzed this table with a single data entry which includes a numeric vector generated by the embed_query function. So text “Hello World” is first converted to it’s numeric representation (fancy name of embeddings) and then it’s mapped to id number 1. Like a key-value pair. Lastly, the mode=”overwrite” parameter ensures that if the table “rag_sample” already exists, it will be overwritten with the new data.This happens with all the text chunks and it’s quite straightforward. This is how it looks in Python..import lancedbfrom langchain_community.vectorstores import LanceDBdb = lancedb.connect(\"lance_database\")table = db.create_table(    \"rag_sample\",    data=[        {            \"vector\": embeddings.embed_query(\"Hello World\"),            \"text\": \"Hello World\",            \"id\": \"1\",        }    ],    mode=\"overwrite\",)docsearch = LanceDB.from_documents(chunks, embeddings, connection=table)NO ROCKET SCIENCE HA!Step 4 : Create a prompt template which will be fed to the LLMOk now comes the prompt template. So when you write a question to the ChatGPT and it answers that question, you are basically providing a prompt to the model so that it can understand what the question is. When companies train the models, they decide what kind of prompt they are going to use for invoking the model and ask the question. For example, if you are working with “Mistral 7B instruct” and you want the optimal results it’s recommended to use the following chat template:&lt;s&gt;[INST] Instruction [/INST] Model answer&lt;/s&gt;[INST] Follow-up instruction [/INST]Note that &lt;s&gt; and &lt;/s&gt; are special tokens to represent beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings. It’s just that the Mistral 7B instruct is made in such a way that the model looks for those special tokens to understand the question better. Different types of LLMs have different kinds of instructed prompts.Now for our case we are going to use huggingfaceh4/zephyr-7b-alpha which is a text generation model. Just to make it clear, Zephyr-7B-α has not been aligned or formated to human preferences with techniques like RLHF (Reinforcement Learning with Human Feedback) or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so).Instead of writing a Prompt of our own, I will use ChatPromptTemplate class which creates a prompt template for the chat models. In layman terms, instead of writing a specified prompt I am letting ChatPromptTemplate to do it for me. Here is an example prompt template that is being generated from the manual messsages.from langchain_core.prompts import ChatPromptTemplatechat_template = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),        (\"human\", \"Hello, how are you doing?\"),        (\"ai\", \"I'm doing well, thanks!\"),        (\"human\", \"{user_input}\"),    ])messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")If you don’t want to write the manual instructions, you can just use the from_template function to generate a more generic prompt template which I used for this project. Here it is..from langchain_core.prompts import ChatPromptTemplatetemplate = \"\"\"{query}\"\"\"prompt = ChatPromptTemplate.from_template(template)Our prompt is set! We’ve crafted a single message, assuming it’s from a human xD . If you’re not using the from_messages function, the ChatPromptTemplate will ensure your prompt works seamlessly with the language model by reserving some additional system messages. There’s always room for improvement with more generic prompts to achieve better results. For now this setup should work..Step 5 : Convert the query to it’s relevant embedding using same embedding model.Now, let’s talk about the query or question we want to ask our RAG application. We can’t just pass the query to our model and expect information in return. Instead, we need to pass the query through the same embedding model used for the chunks earlier. Why is this important? Well, by embedding queries, we allow models to compare them efficiently with previously processed chunks of text. This enables tasks like finding similar documents or generating relevant responses.To understand it better, Imagine you and your friend speak different languages, like English and Hindi, and you need to understand each other’s writings. If your friend hands you a page in Hindi, you won’t understand it directly. So, your friend translates it first, turning the Hindi into English for you. So now if your friend asks you a question in Hindi, you can easily translate that question into English first and look up for the relevant answers in that translated English Text..Similarly, we initially transformed textual information into their corresponding embeddings. Now,  when you pose a query, it undergoes a similar kind of conversion into the numeric form using the same embedding model applied previously to process our textual chunks. This consistent approach allows for efficient retrieval of relevant responses.Step 6 : Fetch K number of documents.Now, let’s talk about the retriever. Its job is to dive into the vector database and perform a search to find relevant documents. It returns a set number, let’s call it “k”, of these documents, which are ranked based on their contextual relevance to the query or question you asked. You can set “k” as a parameter, indicating how many relevant documents you want - whether it’s 2, 5, or 10. Generally, if you have a smaller amount of data, it’s best to stick with a lower “k”, around 2. For longer documents or larger datasets, a “k” between 10 and 20 is recommended.Different search techniques can be employed to fetch relevant documents more effectively and quickly from a vector database. The choice depends on various factors such as your specific use case, the amount of data you have, what kind of vector database you are using and the context of your problem.retriever = docsearch.as_retriever(search_kwargs={\"k\": 3})docs = retriever.get_relevant_documents(\"what did you know about Yolo V7?\")print(docs)When you run this code, the retriever will fetch 3 most most relevant documents from the vector database. All these documents will be the contexts for our LLM model to generate the response for our query.Step 7 : Pass the relevant documents to the LLM and get the response.So far, we’ve asked our retriever to fetch a set number of relevant documents from the database. Now, we need a language model (LLM) to generate a relevant response based on that context. To ensure robustness, let’s remember that at the beginning of this blog, I mentioned that LLMs like ChatGPT can sometimes generate irrelevant responses, especially when asked about specific use cases or contexts. However, this time, we’re providing the context from our own data to the LLM as a reference. So, it will consider this reference along with its general capabilities to answer the question. That’s the whole idea behind using RAG!Now, let’s dive into implementing the language model (LLM) aspect of our RAG setup. We’ll be using zephyr model architecture from the Hugging Face Hub. Here’s how we do it in Python:from langchain_community.llms import HuggingFaceHub# Model architecturellm_repo_id = \"huggingfaceh4/zephyr-7b-alpha\"model_kwargs = {\"temperature\": 0.5, \"max_length\": 4096, \"max_new_tokens\": 2048}model = HuggingFaceHub(repo_id=llm_repo_id, model_kwargs=model_kwargs)In this code snippet, we’re instantiating our language model using the Hugging Face Hub. Specifically, we’re selecting the zephyr 7 billion model which is placed in this repository ID “huggingfaceh4/zephyr-7b-alpha”. Choosing this model isn’t arbitrary; as I said before, it’s based on the model’s suitability for our specific task and requirements. As we are already implementing only Open Source components, Zephyr 7 billion works good enough to generate a useful response with minimal overhead and low latency.This model comes with some additional parameters to fine-tune its behavior. We’ve set the temperature to 0.5, which controls the randomness of the generated text. As a lower temperature tends to result in more conservative and predictable outputs and when the temperature is set to max which is 1, the model tries to be as creative as it could, so based on what type of output you want for your use case, you can tweak this parameter. For the sake of the simplicity and demonstration purposes, I set it to 0.5 to make sure we get decent results. Next is max_length parameter which defines the maximum length of the generated text and it includes the size of your prompt as well as the response.max_new_tokens sets the threshold on the maximum number of new tokens that can be generated. As a general rule of thumb, the max_new_tokens should always be less than or equal to the max_length parameter. Why? Think about it..Step 8 : Create a chain for invoking the LLM.We have everything we want for our RAG application. The last thing we need to do is to create a chain for invoking the LLM on our query to generate the response. There are different types of chains for the different types of use cases, if you like your LLM to remember the context of the chat over the time like the ChatGPT , you would need a memory instance which can be shared among multiple conversation pieces, for such cases, there are conversational chains available.For now we just need a chain which can combine our retrieved contexts and pass it with the query to the LLM to generate the response.from langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughrag_chain = (    {\"context\": retriever,  \"query\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())response = rag_chain.invoke(\"Who killed Jon Snow?\")We have our Prompt, model, context and the query! All of them are combined into a single chain. It’s pretty much what all the chains does! Now before running the final code, I want to give a quick check on these two helper functions: RunnablePassthrough() and StrOutputParser().The RunnablePassthrough class in LangChain serves to pass inputs unchanged or with additional keys. In our chain, a prompt expects input in the form of a map with keys “context” and “question.” However, user input only includes the “question.” or the “query”.  Here, RunnablePassthrough is utilized to pass the user’s question under the “question” key while retrieving the context using a retriever. It just ensures that the input to the prompt conforms to the expected format.Secondally, StrOutputParser is typically employed in RAG chains to parse the output of the model into a human-readable string. In the layman terms, It is responsible for transforming the model’s output into a more coherent and grammatically correct sentence, which is generally better readable by Humans! That’s it!D-DayTo make sure we get the entire idea even if the response gets cut off, I’ve implemented a function called get_complete_sentence(). Basically this function helps extract the last complete sentence from the text. So, even if the response hits the maximum token limit that we set upon and it gets truncated midway, we will still get a coherent understanding of the message.For practical testing, I suggest storing some low sized PDFs in the data folder of your project. You can choose PDFs related to various topics or domains that you want the chatbot to interact with. Additionally, providing a URL as a reference for the chatbot can be helpful for testing. For example, you could use a Wikipedia page, a research paper, or any other online document relevant to your testing goals. During my testing, I used a URL containing information about Jon Snow from Game of Thrones,  and PDFs of Transformers paper, and the YOLO V7 paper to evaluate the bot’s performance. Let’s see how our bot performs in varied content.import osimport timeimport lancedbfrom langchain_community.vectorstores import LanceDBfrom langchain_community.llms import HuggingFaceHubfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain_community.vectorstores import LanceDBfrom langchain_community.embeddings import HuggingFaceEmbeddingsfrom langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, DirectoryLoaderfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom prettytable import PrettyTableHF_TOKEN = \"hf*********\"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN# Loading the web URL and breaking down the information into chunksstart_time = time.time()loader = WebBaseLoader(\"https://gameofthrones.fandom.com/wiki/Jon_Snow\")documents_loader = DirectoryLoader('data', glob=\"./*.pdf\", loader_cls=PyPDFLoader)# URL loaderurl_docs = loader.load()# Document loaderdata_docs = documents_loader.load()# Combining all the information into a single variabledocs = url_docs + data_docs# Specify chunk size and overlapchunk_size = 256chunk_overlap = 20text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)chunks = text_splitter.split_documents(docs)# Specify Embedding Modelembedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': 'cpu'})# Specify Vector Databasevectorstore_start_time = time.time()database_name = \"LanceDB\"db = lancedb.connect(\"src/lance_database\")table = db.create_table(    \"rag_sample\",    data=[        {            \"vector\": embeddings.embed_query(\"Hello World\"),            \"text\": \"Hello World\",            \"id\": \"1\",        }    ],    mode=\"overwrite\",)docsearch = LanceDB.from_documents(chunks, embeddings, connection=table)vectorstore_end_time = time.time()# Specify Retrieval Informationsearch_kwargs = {\"k\": 3}retriever = docsearch.as_retriever(search_kwargs = {\"k\": 3})# Specify Model Architecturellm_repo_id = \"huggingfaceh4/zephyr-7b-alpha\"model_kwargs = {\"temperature\": 0.5, \"max_length\": 4096, \"max_new_tokens\": 2048}model = HuggingFaceHub(repo_id=llm_repo_id, model_kwargs=model_kwargs)template = \"\"\"{query}\"\"\"prompt = ChatPromptTemplate.from_template(template)rag_chain_start_time = time.time()rag_chain = (    {\"context\": retriever, \"query\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())rag_chain_end_time = time.time()def get_complete_sentence(response):    last_period_index = response.rfind('.')    if last_period_index != -1:        return response[:last_period_index + 1]    else:        return response# Invoke the RAG chain and retrieve the responserag_invoke_start_time = time.time()response = rag_chain.invoke(\"Who killed Jon Snow?\")rag_invoke_end_time = time.time()# Get the complete sentencecomplete_sentence_start_time = time.time()complete_sentence = get_complete_sentence(response)complete_sentence_end_time = time.time()# Create a tabletable = PrettyTable()table.field_names = [\"Task\", \"Time Taken (Seconds)\"]# Add rows to the tabletable.add_row([\"Vectorstore Creation\", round(vectorstore_end_time - vectorstore_start_time, 2)])table.add_row([\"RAG Chain Setup\", round(rag_chain_end_time - rag_chain_start_time, 2)])table.add_row([\"RAG Chain Invocation\", round(rag_invoke_end_time - rag_invoke_start_time, 2)])table.add_row([\"Complete Sentence Extraction\", round(complete_sentence_end_time - complete_sentence_start_time, 2)])# Additional information in the tabletable.add_row([\"Embedding Model\", embedding_model_name])table.add_row([\"LLM (Language Model) Repo ID\", llm_repo_id])table.add_row([\"Vector Database\", database_name])table.add_row([\"Temperature\", model_kwargs[\"temperature\"]])table.add_row([\"Max Length Tokens\", model_kwargs[\"max_length\"]])table.add_row([\"Max New Tokens\", model_kwargs[\"max_new_tokens\"]])table.add_row([\"Chunk Size\", chunk_size])table.add_row([\"Chunk Overlap\", chunk_overlap])table.add_row([\"Number of Documents\", len(docs)])print(\"\\nComplete Sentence:\")print(complete_sentence)# Print the tableprint(\"\\nExecution Timings:\")print(table)To enhance readability and present the execution information in a structured tabular format, I have used PrettyTable library. You can add it to your virtual environment by using the command pip3 install prettytable.So this is the response I received in less than &lt; 1 minute, which is quite considerable for the starters. The time it takes can vary depending on your system’s configuration, but I believe you’ll get decent results in just a few minutes. So, please be patient if it’s taking a bit longer.Human: Question : Who killed Jon Snow?Answer: In the TV series Game of Thrones, Jon Snow was stabbed by his fellow Night's Watch members in season 5, episode 9, \"The Dance of Dragons.\" However, he was later resurrected by Melisandre in season 6, episode 3, \"Oathbreaker.\" So, technically, no one killed Jon Snow in the show.Execution Timings:+------------------------------+----------------------------------------+|             Task             |          Time Taken (Seconds)          |+------------------------------+----------------------------------------+|     Vectorstore Creation     |                 16.21                  ||       RAG Chain Setup        |                  0.03                  ||     RAG Chain Invocation     |                  2.06                  || Complete Sentence Extraction |                  0.0                   ||       Embedding Model        | sentence-transformers/all-MiniLM-L6-v2 || LLM (Language Model) Repo ID |     huggingfaceh4/zephyr-7b-alpha      ||       Vector Database        |                LanceDB                 ||         Temperature          |                  0.5                   ||      Max Length Tokens       |                  4096                  ||        Max New Tokens        |                  2048                  ||          Chunk Size          |                  256                   ||        Chunk Overlap         |                   20                   ||     Number of Documents      |                   39                   |+------------------------------+----------------------------------------+Have fun experimenting with various data sources! You can try changing the website addresses, adding new PDF files or maybe change the template a bit. LLMs are fun, you never know what you get!What’s next?There are plenty of things we can adjust here. We could switch to a more effective embedding model for better indexing, try different searching techniques for the retriever, add a reranker to improve the ranking of documents, or use a more advanced LLM with a larger context window and faster response times. Essentially, every RAG application is just an enhanced version based on these factors. However, the fundamental concept of how a RAG application works always remains the same.Here is the collab link for the reference..",
            "content_html": "<p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/make_your_application_with_rag/cat.png?raw=true\" alt=\"cat\" /></p><p><em>If you’re considering making a personalized bot for your documents or your website that responds to you, you’re in the right spot. I’m here to help you create a bot using Langchain and RAG strategies for this purpose.</em></p><h3 id=\"understanding-the-limitations-of-chatgpt-and-llms\">Understanding the Limitations of ChatGPT and LLMs</h3><p>ChatGPTs and other Large Language Models (LLMs) are extensively trained on text corpora to comprehend language semantics and coherence. Despite their impressive capabilities, these models have limitations that require careful consideration for particular use cases. One significant challenge is the potential for hallucinations, where the model might generate inaccurate or contextually irrelevant information.</p><p>Imagine requesting the model to enhance your company policies; in such scenarios, ChatGPTs and other Large Language Models might struggle to provide factual responses because they lack training on your company’s data. Instead, they may generate nonsensical or irrelevant responses, which can be unhelpful. So, how can we ensure that an LLM comprehends our specific data and generates responses accordingly? This is where techniques like Retrieval Augmentation Generation (RAG) come to the rescue.</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/make_your_application_with_rag/LLM_without_RAG.png?raw=true\" alt=\"RAG\" /></p><h3 id=\"what-is-rag\">What is RAG?</h3><p>RAG or Retrieval Augmented Generation uses three main workflows to generate and give the better response</p><ul>  <li>    <p>Information Retrieval: When a user asks a question, the AI system retrieves the relevant data from a well-maintained knowledge library or external sources like databases, articles, APIs, or document repositories. This is achieved by converting the query into a numerical format or vector that can be understood by machines.</p>  </li>  <li>    <p>LLM:The retrieved data is then presented to the LLM or Large Language Model, along with the user’s query. The LLM uses this new knowledge and its training data to generate the response.</p>  </li>  <li>    <p>Response: Finally, the LLM generates a response that is more accurate and relevant since it has been augmented with the retrieved information. I mean we gave LLM some additional information from our Knowledge library which allows LLMs to provide more contextually relevant and factual responses, solving the problem of models when they are just hallucinating or providing irrelevant answers.</p>  </li></ul><p>Let’s take the example of company policies again. Suppose you have an HR bot that handles queries related to your Company policies. Now if someone asks anything specific to the policies, The bot can pull the most recent policy documents from the knowledge library, pass the relevant context to a well crafted prompt which is then passed further to the LLM for generating the response.</p><p>To make it easier, Imagine a LLM as your knowledgeable friend who seems to know everything, from Geography to Computer Science, from Politics to Philosophy. Now, picture yourself asking this friend a few questions:</p><ul>  <li>“Who handles my laundry on weekends?”</li>  <li>“Who lives next door to me?”</li>  <li>“What brand of peanut butter do I prefer?”</li></ul><p>Chances are, your friend wouldn’t be able to answer these questions. Most of the time, no. But let’s say this distant friend becomes closer to you over time, he comes to your place regularly, knows your parents very well, you both hangout pretty often, you go on outings, blah blah blah.. You got the point.</p><p>I mean he is gaining access to personal and insider information about you.  Now, when you pose the same questions, he can somehow answer those questions with more relevance now because he is better suited with your personal insights.</p><p>Similarly, a LLM, when provided with additional information or access to your data, won’t guess or hallucinate. Instead, it can leverage that access data to provide more relevant and accurate answers.</p><h3 id=\"to-break-it-down-here-are-the-exact-steps-to-create-any-rag-application\">To break it down, here are the exact steps to create any RAG application…</h3><ol>  <li>Extract the relevant information from your data sources.</li>  <li>Break the information into the small chunks.</li>  <li>Store the chunks as their embedddings into a vector database.</li>  <li>Create a prompt template which will be fed to the LLM with the query and the context.</li>  <li>Convert the query to it’s relevant embedding using same embedding model.</li>  <li>Fetch k number of relevant documents related to the query from the vector database.</li>  <li>Pass the relevant documents to the LLM and get the response.</li></ol><h3 id=\"faqs\">FAQs</h3><ol>  <li>    <p>We will be using <a href=\"https://python.langchain.com/docs/get_started/introduction\">Langchain</a> for this task, Basically it’s like a wrapper which lets you talk and manage your LLM operations better. Note that the Langchain is updating very fast and some functions and other classes might moved to the different modules. So if something doesn’t work, just check if you are importing the libraries from the right sources!</p>  </li>  <li>    <p>Along with it we will be using <a href=\"https://huggingface.co/\">Hugging Face</a>, an open-source library for building, training, and deploying state-of-the-art machine learning models, especially about NLP. To use the HuggingFace we need the access token, Get your access token <a href=\"https://huggingface.co/docs/hub/security-tokens\">here</a></p>  </li>  <li>    <p>For our models, we’ll need two key components: a LLM (Large Language Model) and an embedding model. While paid sources like OpenAI offer these, we’ll be utilizing open-source models to ensure accessibility for everyone.</p>  </li>  <li>    <p>Now we need a Vector Database to store our embeddings, For that task, we’ve got <a href=\"https://lancedb.com/\">LanceDB</a> – it’s like a super-smart data lake for handling lots of information. It’s a top-notch vector database, making it the go-to choice for dealing with complex data like vector embeddings.. And the best part? It won’t burn a dent in your pocket because it’s open source and free to use!!</p>  </li>  <li>    <p>To keep things simple, our data ingestion process will involve using a URL and some PDFs. While you can incorporate additional data sources if needed, we’ll concentrate solely on these two for now.</p>  </li></ol><p>With Langchain for the interface, Hugging Face for fetching the models, along with open-source components, we’re all set to go! This way, we will save some bucks while still having everything we need. Let’s move to the next steps</p><h3 id=\"environment-setup\">Environment Setup</h3><p>I am using a MacBook Air M1, and it’s important to note that certain dependencies and configurations may vary depending on the type of system you are using. Now open your favorite editor, create a python environment and install the relevant dependencies</p><pre><code class=\"language-python\"># Create a virtual environmentpython3 -m venv env# Activate the virtual environmentsource env/bin/activate# Upgrade pip in the virtual environmentpip install --upgrade pip# Install required dependenciespip3 install lancedb langchain langchain_community prettytable sentence-transformers huggingface-hub bs4 pypdf pandas# This is optional, I did it for removing a warningpip3 uninstall urllib3pip3 install 'urllib3&lt;2.0'</code></pre><p>Now create a .env file in the same directory to place your Hugging Face api credentials like this</p><pre><code class=\"language-python\">HUGGINGFACEHUB_API_TOKEN = hf_KKNWfBqgwCUOHdHFrBwQ.....</code></pre><p>Ensure the name “HUGGINGFACEHUB_API_TOKEN” remains unchanged, as it is crucial for authentication purposes.</p><p>If you prefer a straightforward approach without relying on external packages or file loading, you can directly configure the environment variable within your code like this..</p><pre><code class=\"language-python\">HF_TOKEN = \"hf_KKNWfBqgwCUOHdHFrBwQ.....\"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN</code></pre><p>Finally create a data folder in the project’s root directory, designated as the central repository for storing PDF documents. You can add some sample PDFs for testing purposes; for instance, I am using the <a href=\"https://arxiv.org/pdf/2207.02696.pdf\">Yolo V7</a> and <a href=\"https://arxiv.org/abs/1706.03762\">Transformers</a> paper for demonstration. It’s important to note that this designated folder will function as our primary source for data ingestion.</p><p>It seems like everything is in order, and we’re all set!</p><h3 id=\"step-1--extracting-the-relevant-information\">Step 1 : Extracting the relevant information</h3><p>To get your RAG application running, the first thing we need to do is to extract the relevant information from the various data sources. It can be a website page, a PDF file, a notion link, a google doc whatever it is, it needs to be extracted from it’s original source first.</p><pre><code class=\"language-python\">import osfrom langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, DirectoryLoader# Put the token values inside the double quotesHF_TOKEN = \"hf_*******\"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN# Loading the web url and data url_loader = WebBaseLoader(\"https://gameofthrones.fandom.com/wiki/Jon_Snow\")documents_loader = DirectoryLoader('data', glob=\"./*.pdf\", loader_cls=PyPDFLoader)# Creating the instancesurl_docs = url_loader.load()data_docs = documents_loader.load()# Combining all the data that we ingesteddocs = url_docs + data_docs</code></pre><p>This will ingest all the data from the URL link and the PDFs.</p><h3 id=\"step-2--breaking-the-information-into-smaller-chunks\">Step 2 : Breaking the information into smaller chunks</h3><p>We’ve all the necessary data for developing our RAG application. Now, it’s time to break down this information into smaller chunks. Later, we’ll utilize an embedding model to convert these chunks into their respective embeddings. But why is it important?</p><p>Think of it like this: If you’re tasked with digesting a 100-page book all at once and then asked a specific question about it, it would be challenging to retrieve the necessary information from the entire book to provide an answer. However, if you’re permitted to break the book into smaller, manageable chunks—let’s say 10 pages each—and each chunk is labeled with an index from 0 to 9, the process becomes much simpler. When the same question is posed after this breakdown, you can easily locate the relevant chunk based on its index and then extract the information needed to answer the question accurately.</p><p>Picture the book as your extracted information, with each 10-page segment representing a small chunk of data, and the index pages as the embedding. Essentially, we’ll apply an embedding model to these chunks to transform the information into their respective embeddings. While as humans, we may not directly comprehend or relate to these embeddings, they serve as numeric representations of the chunks to our application.  This is how you can do this in Python</p><pre><code class=\"language-python\">from langchain.text_splitter import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)chunks = text_splitter.split_documents(docs)</code></pre><p>Now the chunk_size parameter specifies the maximum number of characters that a chunk can contain, while the chunk_overlap parameter specifies the number of characters that should overlap between two adjacent chunks. With the chunk_overlap set to 50, the last 50 characters of the adjacent chunks will be shared between each other.</p><p>This approach helps to prevent important information from being split across two chunks, ensuring that each chunk contains sufficient contextual information of their neighbor chunks for the subsequent processing or analysis.</p><p>Shared information at the boundary of neighboring chunks enables a more seamless transition and understanding of the text’s content. The best strategy for choosing the chunk_size and chunk_overlap parameters largely depends on the nature of the documents and the purpose of the application.</p><h3 id=\"step-3--creating-the-embeddings-and-store-them-into-a-vectordatabase\">Step 3 : Creating the embeddings and store them into a vectordatabase</h3><p>There are two primary methods to generate embeddings for our text chunks. The first involves downloading a model, managing preprocessing, and conducting computations independently. Alternatively, we can leverage Hugging Face’s model hub, which offers a variety of pre-trained models for various NLP tasks, including embedding generation.</p><p>Opting for the latter approach allows us to utilize one of Hugging Face’s embedding models. With this method, we simply provide our text chunks to the chosen model, saving us from the resource-intensive computations on our local machines. 💀</p><p>Hugging Face’s model hub provides numerous options for embedding models, and you can explore the <a href=\"https://huggingface.co/spaces/mteb/leaderboard\">leaderboard</a> to select the most suitable one for your requirements. For now, we’ll proceed with “sentence-transformers/all-MiniLM-L6-v2.” This model is pretty fast and highly efficient in our task!!</p><pre><code class=\"language-python\">from langchain_community.embeddings import HuggingFaceEmbeddingsembedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': 'cpu'})</code></pre><p>Here’s a way to see the number of embeddings for each chunk</p><pre><code class=\"language-python\">query = \"Hello I want to see the length of the embeddings for this document.\"len(embeddings.embed_documents([query])[0])# 384</code></pre><p>We have the embeddings for our chunks, now we need a vector database to store them.</p><p>When it comes to vector databases, there are plenty of options out there suiting various needs. Databases like Pinecone offer adequate performance and advanced features but come with a hefty price tag. On the other hand, open-source alternatives like FAISS or Chroma may lack some extras but are more than sufficient for those not who don’t require extensive scalability.</p><p>But wait, I am dropping a bomb here, I’ve recently come across LanceDB, So it’s an open-source vector database similar to FAISS and Chroma. What makes LanceDB stand out is not just its open-source nature but its unparalleled scalability. In fact, after a closer look, I realized that I haven’t done justice in highlighting the true value propositions of LanceDB earlier!!</p><p>Surprisingly, LanceDB is the most scalable vector database available, outperforming even the likes of Pinecone, Chroma, Qdrant, and others. Scaling up to a billion vectors locally on your laptop is a feat only achievable with LanceDB. I mean this capability is a game-changer, especially when you compare it to other vector databases struggling even with a hundred million vectors. What’s more mind blowing is that LanceDB manages to offer this unprecedented scalability at a fraction of cost, I mean they are offering the utilities and database tools at much cheaper rates than its closest counterparts.</p><p>So now, We’ll create an instance of LanceDB vector database by calling <code>lancedb.connect(\"lance_database\")</code>. This line essentially sets up a connection to the LanceDB database named “lance_database.” Next, we create a table within the database named “rag_sample” using the create_table function. Now we initialzed this table with a single data entry which includes a numeric vector generated by the embed_query function. So text “Hello World” is first converted to it’s numeric representation (fancy name of embeddings) and then it’s mapped to <code>id</code> number 1. Like a key-value pair. Lastly, the mode=”overwrite” parameter ensures that if the table “rag_sample” already exists, it will be overwritten with the new data.</p><p>This happens with all the text chunks and it’s quite straightforward. This is how it looks in Python..</p><pre><code class=\"language-python\">import lancedbfrom langchain_community.vectorstores import LanceDBdb = lancedb.connect(\"lance_database\")table = db.create_table(    \"rag_sample\",    data=[        {            \"vector\": embeddings.embed_query(\"Hello World\"),            \"text\": \"Hello World\",            \"id\": \"1\",        }    ],    mode=\"overwrite\",)docsearch = LanceDB.from_documents(chunks, embeddings, connection=table)</code></pre><p>NO ROCKET SCIENCE HA!</p><h3 id=\"step-4--create-a-prompt-template-which-will-be-fed-to-the-llm\">Step 4 : Create a prompt template which will be fed to the LLM</h3><p>Ok now comes the prompt template. So when you write a question to the ChatGPT and it answers that question, you are basically providing a prompt to the model so that it can understand what the question is. When companies train the models, they decide what kind of prompt they are going to use for invoking the model and ask the question. For example, if you are working with “Mistral 7B instruct” and you want the optimal results it’s recommended to use the following chat template:</p><pre><code class=\"language-python\">&lt;s&gt;[INST] Instruction [/INST] Model answer&lt;/s&gt;[INST] Follow-up instruction [/INST]</code></pre><p>Note that &lt;s&gt; and &lt;/s&gt; are special tokens to represent beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings. It’s just that the Mistral 7B instruct is made in such a way that the model looks for those special tokens to understand the question better. Different types of LLMs have different kinds of instructed prompts.</p><p>Now for our case we are going to use <a href=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\">huggingfaceh4/zephyr-7b-alpha</a> which is a text generation model. Just to make it clear, Zephyr-7B-α has not been aligned or formated to human preferences with techniques like RLHF (Reinforcement Learning with Human Feedback) or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so).</p><p>Instead of writing a Prompt of our own, I will use ChatPromptTemplate class which creates a prompt template for the chat models. In layman terms, instead of writing a specified prompt I am letting ChatPromptTemplate to do it for me. Here is an example prompt template that is being generated from the manual messsages.</p><pre><code class=\"language-python\">from langchain_core.prompts import ChatPromptTemplatechat_template = ChatPromptTemplate.from_messages(    [        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),        (\"human\", \"Hello, how are you doing?\"),        (\"ai\", \"I'm doing well, thanks!\"),        (\"human\", \"{user_input}\"),    ])messages = chat_template.format_messages(name=\"Bob\", user_input=\"What is your name?\")</code></pre><p>If you don’t want to write the manual instructions, you can just use the <em>from_template</em> function to generate a more generic prompt template which I used for this project. Here it is..</p><pre><code class=\"language-python\">from langchain_core.prompts import ChatPromptTemplatetemplate = \"\"\"{query}\"\"\"prompt = ChatPromptTemplate.from_template(template)</code></pre><p>Our prompt is set! We’ve crafted a single message, assuming it’s from a human xD . If you’re not using the from_messages function, the ChatPromptTemplate will ensure your prompt works seamlessly with the language model by reserving some additional system messages. There’s always room for improvement with more generic prompts to achieve better results. For now this setup should work..</p><h3 id=\"step-5--convert-the-query-to-its-relevant-embedding-using-same-embedding-model\">Step 5 : Convert the query to it’s relevant embedding using same embedding model.</h3><p>Now, let’s talk about the query or question we want to ask our RAG application. We can’t just pass the query to our model and expect information in return. Instead, we need to pass the query through the same embedding model used for the chunks earlier. Why is this important? Well, by embedding queries, we allow models to compare them efficiently with previously processed chunks of text. This enables tasks like finding similar documents or generating relevant responses.</p><p>To understand it better, Imagine you and your friend speak different languages, like English and Hindi, and you need to understand each other’s writings. If your friend hands you a page in Hindi, you won’t understand it directly. So, your friend translates it first, turning the Hindi into English for you. So now if your friend asks you a question in Hindi, you can easily translate that question into English first and look up for the relevant answers in that translated English Text..</p><p>Similarly, we initially transformed textual information into their corresponding embeddings. Now,  when you pose a query, it undergoes a similar kind of conversion into the numeric form using the same embedding model applied previously to process our textual chunks. This consistent approach allows for efficient retrieval of relevant responses.</p><h3 id=\"step-6--fetch-k-number-of-documents\">Step 6 : Fetch K number of documents.</h3><p>Now, let’s talk about the retriever. Its job is to dive into the vector database and perform a search to find relevant documents. It returns a set number, let’s call it “k”, of these documents, which are ranked based on their contextual relevance to the query or question you asked. You can set “k” as a parameter, indicating how many relevant documents you want - whether it’s 2, 5, or 10. Generally, if you have a smaller amount of data, it’s best to stick with a lower “k”, around 2. For longer documents or larger datasets, a “k” between 10 and 20 is recommended.</p><p>Different <a href=\"https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore\">search techniques</a> can be employed to fetch relevant documents more effectively and quickly from a vector database. The choice depends on various factors such as your specific use case, the amount of data you have, what kind of vector database you are using and the context of your problem.</p><pre><code class=\"language-python\">retriever = docsearch.as_retriever(search_kwargs={\"k\": 3})docs = retriever.get_relevant_documents(\"what did you know about Yolo V7?\")print(docs)</code></pre><p>When you run this code, the retriever will fetch 3 most most relevant documents from the vector database. All these documents will be the contexts for our LLM model to generate the response for our query.</p><h3 id=\"step-7--pass-the-relevant-documents-to-the-llm-and-get-the-response\">Step 7 : Pass the relevant documents to the LLM and get the response.</h3><p>So far, we’ve asked our retriever to fetch a set number of relevant documents from the database. Now, we need a language model (LLM) to generate a relevant response based on that context. To ensure robustness, let’s remember that at the beginning of this blog, I mentioned that LLMs like ChatGPT can sometimes generate irrelevant responses, especially when asked about specific use cases or contexts. However, this time, we’re providing the context from our own data to the LLM as a reference. So, it will consider this reference along with its general capabilities to answer the question. That’s the whole idea behind using RAG!</p><p>Now, let’s dive into implementing the language model (LLM) aspect of our RAG setup. We’ll be using zephyr model architecture from the Hugging Face Hub. Here’s how we do it in Python:</p><pre><code class=\"language-python\">from langchain_community.llms import HuggingFaceHub# Model architecturellm_repo_id = \"huggingfaceh4/zephyr-7b-alpha\"model_kwargs = {\"temperature\": 0.5, \"max_length\": 4096, \"max_new_tokens\": 2048}model = HuggingFaceHub(repo_id=llm_repo_id, model_kwargs=model_kwargs)</code></pre><p>In this code snippet, we’re instantiating our language model using the Hugging Face Hub. Specifically, we’re selecting the zephyr 7 billion model which is placed in this repository ID <a href=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\">“huggingfaceh4/zephyr-7b-alpha”</a>. Choosing this model isn’t arbitrary; as I said before, it’s based on the model’s suitability for our specific task and requirements. As we are already implementing only Open Source components, Zephyr 7 billion works good enough to generate a useful response with minimal overhead and low latency.</p><p>This model comes with some additional parameters to fine-tune its behavior. We’ve set the temperature to 0.5, which controls the randomness of the generated text. As a lower temperature tends to result in more conservative and predictable outputs and when the temperature is set to max which is 1, the model tries to be as creative as it could, so based on what type of output you want for your use case, you can tweak this parameter. For the sake of the simplicity and demonstration purposes, I set it to 0.5 to make sure we get decent results. Next is max_length parameter which defines the maximum length of the generated text and it includes the size of your prompt as well as the response.</p><p>max_new_tokens sets the threshold on the maximum number of new tokens that can be generated. As a general rule of thumb, the max_new_tokens should always be less than or equal to the max_length parameter. Why? Think about it..</p><h3 id=\"step-8--create-a-chain-for-invoking-the-llm\">Step 8 : Create a chain for invoking the LLM.</h3><p>We have everything we want for our RAG application. The last thing we need to do is to create a chain for invoking the LLM on our query to generate the response. There are different types of chains for the different types of use cases, if you like your LLM to remember the context of the chat over the time like the ChatGPT , you would need a memory instance which can be shared among multiple conversation pieces, for such cases, there are conversational chains available.</p><p>For now we just need a chain which can combine our retrieved contexts and pass it with the query to the LLM to generate the response.</p><pre><code class=\"language-python\">from langchain_core.output_parsers import StrOutputParserfrom langchain_core.runnables import RunnablePassthroughrag_chain = (    {\"context\": retriever,  \"query\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())response = rag_chain.invoke(\"Who killed Jon Snow?\")</code></pre><p>We have our Prompt, model, context and the query! All of them are combined into a single chain. It’s pretty much what all the chains does! Now before running the final code, I want to give a quick check on these two helper functions: <code>RunnablePassthrough()</code> and <code>StrOutputParser()</code>.</p><p>The <code>RunnablePassthrough</code> class in <code>LangChain</code> serves to pass inputs unchanged or with additional keys. In our chain, a prompt expects input in the form of a map with keys “context” and “question.” However, user input only includes the “question.” or the “query”.  Here, <code>RunnablePassthrough</code> is utilized to pass the user’s question under the “question” key while retrieving the context using a retriever. It just ensures that the input to the prompt conforms to the expected format.</p><p>Secondally, <code>StrOutputParser</code> is typically employed in RAG chains to parse the output of the model into a human-readable string. In the layman terms, It is responsible for transforming the model’s output into a more coherent and grammatically correct sentence, which is generally better readable by Humans! That’s it!</p><h3 id=\"d-day\">D-Day</h3><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/make_your_application_with_rag/LLM_with_RAG.png?raw=true\" alt=\"With_RAG\" /></p><p>To make sure we get the entire idea even if the response gets cut off, I’ve implemented a function called <code>get_complete_sentence()</code>. Basically this function helps extract the last complete sentence from the text. So, even if the response hits the maximum token limit that we set upon and it gets truncated midway, we will still get a coherent understanding of the message.</p><p>For practical testing, I suggest storing some low sized PDFs in the data folder of your project. You can choose PDFs related to various topics or domains that you want the chatbot to interact with. Additionally, providing a URL as a reference for the chatbot can be helpful for testing. For example, you could use a Wikipedia page, a research paper, or any other online document relevant to your testing goals. During my testing, I used a URL containing information about Jon Snow from Game of Thrones,  and PDFs of Transformers paper, and the YOLO V7 paper to evaluate the bot’s performance. Let’s see how our bot performs in varied content.</p><pre><code class=\"language-python\">import osimport timeimport lancedbfrom langchain_community.vectorstores import LanceDBfrom langchain_community.llms import HuggingFaceHubfrom langchain.text_splitter import RecursiveCharacterTextSplitterfrom langchain_community.vectorstores import LanceDBfrom langchain_community.embeddings import HuggingFaceEmbeddingsfrom langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, DirectoryLoaderfrom langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.runnables import RunnablePassthroughfrom prettytable import PrettyTableHF_TOKEN = \"hf*********\"os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN# Loading the web URL and breaking down the information into chunksstart_time = time.time()loader = WebBaseLoader(\"https://gameofthrones.fandom.com/wiki/Jon_Snow\")documents_loader = DirectoryLoader('data', glob=\"./*.pdf\", loader_cls=PyPDFLoader)# URL loaderurl_docs = loader.load()# Document loaderdata_docs = documents_loader.load()# Combining all the information into a single variabledocs = url_docs + data_docs# Specify chunk size and overlapchunk_size = 256chunk_overlap = 20text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)chunks = text_splitter.split_documents(docs)# Specify Embedding Modelembedding_model_name = 'sentence-transformers/all-MiniLM-L6-v2'embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs={'device': 'cpu'})# Specify Vector Databasevectorstore_start_time = time.time()database_name = \"LanceDB\"db = lancedb.connect(\"src/lance_database\")table = db.create_table(    \"rag_sample\",    data=[        {            \"vector\": embeddings.embed_query(\"Hello World\"),            \"text\": \"Hello World\",            \"id\": \"1\",        }    ],    mode=\"overwrite\",)docsearch = LanceDB.from_documents(chunks, embeddings, connection=table)vectorstore_end_time = time.time()# Specify Retrieval Informationsearch_kwargs = {\"k\": 3}retriever = docsearch.as_retriever(search_kwargs = {\"k\": 3})# Specify Model Architecturellm_repo_id = \"huggingfaceh4/zephyr-7b-alpha\"model_kwargs = {\"temperature\": 0.5, \"max_length\": 4096, \"max_new_tokens\": 2048}model = HuggingFaceHub(repo_id=llm_repo_id, model_kwargs=model_kwargs)template = \"\"\"{query}\"\"\"prompt = ChatPromptTemplate.from_template(template)rag_chain_start_time = time.time()rag_chain = (    {\"context\": retriever, \"query\": RunnablePassthrough()}    | prompt    | model    | StrOutputParser())rag_chain_end_time = time.time()def get_complete_sentence(response):    last_period_index = response.rfind('.')    if last_period_index != -1:        return response[:last_period_index + 1]    else:        return response# Invoke the RAG chain and retrieve the responserag_invoke_start_time = time.time()response = rag_chain.invoke(\"Who killed Jon Snow?\")rag_invoke_end_time = time.time()# Get the complete sentencecomplete_sentence_start_time = time.time()complete_sentence = get_complete_sentence(response)complete_sentence_end_time = time.time()# Create a tabletable = PrettyTable()table.field_names = [\"Task\", \"Time Taken (Seconds)\"]# Add rows to the tabletable.add_row([\"Vectorstore Creation\", round(vectorstore_end_time - vectorstore_start_time, 2)])table.add_row([\"RAG Chain Setup\", round(rag_chain_end_time - rag_chain_start_time, 2)])table.add_row([\"RAG Chain Invocation\", round(rag_invoke_end_time - rag_invoke_start_time, 2)])table.add_row([\"Complete Sentence Extraction\", round(complete_sentence_end_time - complete_sentence_start_time, 2)])# Additional information in the tabletable.add_row([\"Embedding Model\", embedding_model_name])table.add_row([\"LLM (Language Model) Repo ID\", llm_repo_id])table.add_row([\"Vector Database\", database_name])table.add_row([\"Temperature\", model_kwargs[\"temperature\"]])table.add_row([\"Max Length Tokens\", model_kwargs[\"max_length\"]])table.add_row([\"Max New Tokens\", model_kwargs[\"max_new_tokens\"]])table.add_row([\"Chunk Size\", chunk_size])table.add_row([\"Chunk Overlap\", chunk_overlap])table.add_row([\"Number of Documents\", len(docs)])print(\"\\nComplete Sentence:\")print(complete_sentence)# Print the tableprint(\"\\nExecution Timings:\")print(table)</code></pre><p>To enhance readability and present the execution information in a structured tabular format, I have used <code>PrettyTable</code> library. You can add it to your virtual environment by using the command <code>pip3 install prettytable</code>.</p><p>So this is the response I received in less than &lt; 1 minute, which is quite considerable for the starters. The time it takes can vary depending on your system’s configuration, but I believe you’ll get decent results in just a few minutes. So, please be patient if it’s taking a bit longer.</p><pre><code class=\"language-python\">Human: Question : Who killed Jon Snow?Answer: In the TV series Game of Thrones, Jon Snow was stabbed by his fellow Night's Watch members in season 5, episode 9, \"The Dance of Dragons.\" However, he was later resurrected by Melisandre in season 6, episode 3, \"Oathbreaker.\" So, technically, no one killed Jon Snow in the show.Execution Timings:+------------------------------+----------------------------------------+|             Task             |          Time Taken (Seconds)          |+------------------------------+----------------------------------------+|     Vectorstore Creation     |                 16.21                  ||       RAG Chain Setup        |                  0.03                  ||     RAG Chain Invocation     |                  2.06                  || Complete Sentence Extraction |                  0.0                   ||       Embedding Model        | sentence-transformers/all-MiniLM-L6-v2 || LLM (Language Model) Repo ID |     huggingfaceh4/zephyr-7b-alpha      ||       Vector Database        |                LanceDB                 ||         Temperature          |                  0.5                   ||      Max Length Tokens       |                  4096                  ||        Max New Tokens        |                  2048                  ||          Chunk Size          |                  256                   ||        Chunk Overlap         |                   20                   ||     Number of Documents      |                   39                   |+------------------------------+----------------------------------------+</code></pre><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/make_your_application_with_rag/cat2.png?raw=true\" alt=\"cat2\" /></p><p>Have fun experimenting with various data sources! You can try changing the website addresses, adding new PDF files or maybe change the template a bit. LLMs are fun, you never know what you get!</p><h3 id=\"whats-next\">What’s next?</h3><p>There are plenty of things we can adjust here. We could switch to a more effective embedding model for better indexing, try different searching techniques for the retriever, add a reranker to improve the ranking of documents, or use a more advanced LLM with a larger context window and faster response times. Essentially, every RAG application is just an enhanced version based on these factors. However, the fundamental concept of how a RAG application works always remains the same.</p><p>Here is the <a href=\"https://colab.research.google.com/drive/1YsOfovVdNPBwCDMWHvLfOaNtqXn4qXTs?usp=sharing\">collab</a> link for the reference..</p>",
            "url": "http://localhost:4000/2024/02/14/rag-application-with-langchain",
            
            
            
            "tags": ["LLM","Deep Learning"],
            
            "date_published": "2024-02-14T00:00:00+05:30",
            "date_modified": "2024-02-14T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        }
    
    ]
}