{
    "version": "https://jsonfeed.org/version/1",
    "title": "Blixxi Labs",
    "home_page_url": "http://localhost:4000/",
    "feed_url": "http://localhost:4000/feed.json",
    "description": "Everything related to AI",
    "icon": "http://localhost:4000/apple-touch-icon.png",
    "favicon": "http://localhost:4000/favicon.ico",
    "expired": false,
    
    "author": "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}",
    
"items": [
    
        {
            "id": "http://localhost:4000/2024/01/23/rag-application-with-langchain",
            "title": "Create LLM apps using RAG",
            "summary": "RAG and Langcahin for creating the personalized bots",
            "content_text": "Last Updated: 30/01/2024Status: Working in progressIf you’ve ever thought about creating a custom bot for your documents or website that interacts based on specific data, you’re in the right place. I’m here to assist you in developing a bot that leverages Langchain and RAG strategies for this purpose.Understanding the Limitations of ChatGPT and LLMsChatGPTs and other Large Language Models (LLMs) are extensively trained on text corpora to comprehend language semantics and coherence. Despite their impressive capabilities, these models have limitations that require careful consideration for particular use cases. One significant challenge is the potential for hallucinations, where the model might generate inaccurate or contextually irrelevant information.Imagine requesting the model to enhance your company policies; in such scenarios, ChatGPTs and other Large Language Models might struggle to provide factual responses because they lack training on your company’s specific policies. Instead, they may generate nonsensical or irrelevant responses, which can be unhelpful. So, how can we ensure that an LLM comprehends our specific data and generates responses accordingly? This is where techniques like Retrieval Augmentation Generation (RAG) come to the rescue.What is RAG?RAG or Retrieval Augmented Generation uses three main workflows to generate and give the better response      Information Retrieval: When a user asks a question, the AI system retrieves the relevant data from a well-maintained knowledge library or external sources like databases, articles, APIs, or document repositories. This is achieved by converting the query into a numerical format or vector that can be understood by machines.        LLM: The retrieved data is then presented to the LLM or Large Language Model, along with the user’s query. The LLM uses this new knowledge and its training data to generate the response.        Response: Finally, the LLM generates a response that is more accurate and relevant since it has been augmented with the retrieved information. I mean we gave LLM some additional information from our Knowledge library which allows LLMs to provide more contextually relevant and factual responses, solving the problem of models when they are just hallucinating or providing irrelevant answer.  Let’s take an example of company policies again. Suppose you have an HR bot that handles queries related to your Company policies. Now if someones asks anything specific to the policies, The bot can pull the most recent policy documents from the knowledge libray, pass the relevant context to a well crafted prompt which is then passed further to the LLM for generating the response.To make it more easy, Imagine a LLM as your knowledgeable friend who seems to know everything, from Geography to Computer Science from Politics to Philosophy. Now, picture yourself asking this friend a few questions:  “Who handles my laundry on weekends?”  “Who lives next door to me?”  “What brand of peanut butter do I prefer?”Chances are, your friend wouldn’t be able to answer these questions, right? Most of the time, no.But let’s say this distant friend becomes closer to you over time, he comes at your place regularly, know your parents very well, you both hangout pretty often, you go on outings, blah blah blah.. You got the point right? I mean he is gaining access to personal and insider information about you.  Now, when you pose the same questions, they can somehow answer those question with more relevance now because because he is better suited with your personal insights.Similarly, an LLM, when provided with additional information or access to your use case data, won’t guess or hallucinate. Instead, it will leverage that data to provide relevant and accurate answers.To break it down, here are the exact steps to create any RAG application…  Extract the relevant information from your data sources.  Break the information into the small chunks.  Store the chunks as their embedddings into a vector database.  Create a prompt template which will be fed to the LLM with the query and the context.  Convert the query to it’s relevant embedding using same embedding model.  Fetch k number of relevant documents related to the query from the vector database.  Pass the relevant documents to the LLM and get the response.FAQs      We will be using  Langchain for this task, Basically it’s like a wrapper which lets you talk and manage to your LLM operations better.        Along with it we will be using  Hugging Face, it is like an open-source library for building, training, and deploying state-of-the-art machine learning models, especially about NLP. To use the HuggingFace we need the access token, Get your access token here        For our models, we’ll need two key components: an LLM (Large Language Model) and an embedding model. While paid sources like OpenAI offer these, we’ll be utilizing open-source models to ensure accessibility for everyone.  With Langchain for the interface, Hugging Face for fetching the models, along with open-source components, we’re all set to go! This way, we save some bucks while still having everything we need. Let’s move to the next stepsEnvironment SetupOpen your favorite editor, create a python environment and install the relevant dependenciespython3 -m venv envsource env/bin/activatepip3 install dotenv langchain langchain_community Now create a .env file in the same directory to place your Hugging face api credentials like thisHUGGINGFACEHUB_API_TOKEN = hf_KKNWfBqgwCUOHdHFrBwQ.....Ensure the name “HUGGINGFACEHUB_API_TOKEN” remains unchanged, as it is crucial for authentication purposes. It seems like everything is in order, and we’re all set!Step 1 : Extracting the relevant informationTo get your RAG application running, the first thing we need to do is to extract the relevant information from the various data sources. It can be a website page, a PDF file, a notion link, a google doc whatever it is, it needs to be extracted from it’s original source first.from langchain.document_loaders import PyPDFLoaderfrom langchain_community.document_loaders import WebBaseLoaderStep 2 : Breaking the information into smaller chunksWe already have the data or the information that we need for creating our RAG application, now comes the time we break down the information into smaller chunks as we will be using the embedding model later on to convert these chunks into there specific embeddings. But why it’s important?To understand this, think of it like this : If someone tells you to ingest a 100 page book in one go and ask you a relevant question about that book, it would be very difficult for you to gather the pieces of information from the book and give the answer, instead if you are allowed to break down the information into smaller chunks of 10 pages and each page have an index from 0 to 9 or a numbering,  and then same question is asked again, it would be easy for you to search for the relevant chunk and then based on that chunk of information you can easily answer the question.Now images that book as your extracted information, those 10 pages as your small chunks of information and those index as the embedding. In a nutshell we will be using an embedding model on our chunks to convert the information into there embeedings, as a human we might not relate or understand those embeddings but those embeddings are just numeric representation of those chunks.Step 3 : Creating the embeddings and store them into a vectordatabaseNow we are just going to pass those chunks to our embedding model and the embedding model will convert the chunks into their corresponding embeddings. There are two options for doing this, either you can download an embedding model in your local, manage the preprocessing and perform compuations locally and use it for the embedding creation,  or you can use Huggingface hub which hosts a vast collection of pre-trained models for various natural language processing (NLP) tasks, including embeddings.So what we are going to do is we will create an instance of an embedding model which is provided by Hugging Face, then we will pass the chunks to the embedding model and all the heavy stuff like handling preprocessing, offloading comupatation will be done on the Huggingface servers.There are so many options for choosing a right embedding model, you can look at the ![https://huggingface.co/spaces/mteb/leaderboard]leaderboard here and choose the right embedding model according to your needs. For now, we will just use the “bge-base-en-v1.5” which works amazinglly well for creating the embeddings for english texts and it’s compartively small so it’s fast enough to load and do it’s tasks.Ok so the embeddings are created. You could see how many number of embeddings are there for each chunk like this:Now comes the vectordatabase. There are ton of different vector databases available in the market, some of them are paid like Pinecone which works fast and have additional features over the ones which are open source like FAISS, Chroma. But to be honest, if you are not looking for anything like scaling your database to few hundred users who are fetching and storing the data from your database every minute, you are good to go with opensource ones. They works amazingingly well. So what we are going to do is we will create an instance of Chroma vector database, and will store our embeddins in it. That’s it. No rocekt science here.Step 4 : Create a prompt template which will be fed to the LLMOk now comes the prompt template. So when you write a question to the ChatGPT and it answers that question, you are basically providing a prompt to the model so that it can understand what’s the question is. When companies train the models, they decide what kind of prompt they are going to use for invoking the model and ask the question. So for example,if you are working with Mistral 7B instruct and you want the optimal results it’s recommended to use the following chat template:&lt;s&gt;[INST] Instruction [/INST] Model answer&lt;/s&gt;[INST] Follow-up instruction [/INST]Note that  and  are special tokens for beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings. It’s just that the Mistral 7B instruct is made in such a way that the model look for those special tokens to understand the question better. Different types of LLMs have different kinds of instructed prompts. For our case we will be working with zephyer and it’s prompt is like as this:Step 5 : Convert the query to it’s relevant embedding using same embedding model.Ok so far we have converted our textual information into chunks, then we embedded those chunks into there specific embeddings and further more we have stored our embedded chunks into the vector database. Now comes the query or the question that we want to ask to our RAG application. We just can’t pass the query to our model and ask for the information, instead the query will be passed to the same embedding model that is being used for chunks earlier and then only we can do the next set of tasks. Why it’s important? Wel by embedding queries, we enable models to efficiently compare them with previously processed chunks of text, as then only we can do tasks like finding similar documents or generating relevant responses. It’s like translating language into a language computers understand. Imagine you are English Speaker and your friend is a Hindi Speaker, no one understands each other language. Now you give a 1 page document written in Hindi to your English Speaker friend, now your friend will convert that document into the Hindi one first and then will understand the information.Now you asks a question to your english speaking friend in Hindi which is related to that documet. Now what would happen is, your friend will convert that question into the english first and then only will be able to understand the question and find relevant response from that information which you have shared earlier.Ok so your friend is an embedding model which converted your previous texts into the embeddings. Now when you asks a query or a question, it will be converted into the relevant embeddings from the same embedding model that is being used ealier for chunks and then some search operation will be performed to find the relevant response to your query.I hope you get it why we did convert our query into the embeddings first.Step 6 : Fetch K number of documents.Now comes the retriver. What it does is, it goes into the vector database, performs somekind of search to find the relevant documents (chunks) and return k number of relevant documents which are ranked accordingly based on which one is more contextually related to the query or the question that you asked. We can set k as a parameter which means if you want 2 relevant documents or 5 or 10. Genearlly, if you have less amount of data, it’s advisable to keep k to 2 and for longer documents, it’s recommended to keep k in bewteen 10 - 20Step 7 : Pass the relevant documents to the LLM and get the response.Now comes the last step. So to make things more robust, we will ask our retriever to fetch the k number of relevant documents from the database. We will then pass those k number of documents to our LLM as the context and will ask the LLM to generate a relevant response from that context. That’s it.What’s next?So there are so many things which can be tweaked here. We can change our embedding model for something more better and better in terms of indexing, searching technique for retriever can be changed too, as well as a better LLM to generate the relevant response.",
            "content_html": "<p>Last Updated: 30/01/2024<br />Status: Working in progress</p><p><em>If you’ve ever thought about creating a custom bot for your documents or website that interacts based on specific data, you’re in the right place. I’m here to assist you in developing a bot that leverages Langchain and RAG strategies for this purpose.</em></p><h3 id=\"understanding-the-limitations-of-chatgpt-and-llms\">Understanding the Limitations of ChatGPT and LLMs</h3><p>ChatGPTs and other Large Language Models (LLMs) are extensively trained on text corpora to comprehend language semantics and coherence. Despite their impressive capabilities, these models have limitations that require careful consideration for particular use cases. One significant challenge is the potential for hallucinations, where the model might generate inaccurate or contextually irrelevant information.</p><p>Imagine requesting the model to enhance your company policies; in such scenarios, ChatGPTs and other Large Language Models might struggle to provide factual responses because they lack training on your company’s specific policies. Instead, they may generate nonsensical or irrelevant responses, which can be unhelpful. So, how can we ensure that an LLM comprehends our specific data and generates responses accordingly? This is where techniques like Retrieval Augmentation Generation (RAG) come to the rescue.</p><h3 id=\"what-is-rag\">What is RAG?</h3><p>RAG or Retrieval Augmented Generation uses three main workflows to generate and give the better response</p><ul>  <li>    <p>Information Retrieval: When a user asks a question, the AI system retrieves the relevant data from a well-maintained knowledge library or external sources like databases, articles, APIs, or document repositories. This is achieved by converting the query into a numerical format or vector that can be understood by machines.</p>  </li>  <li>    <p>LLM: The retrieved data is then presented to the LLM or Large Language Model, along with the user’s query. The LLM uses this new knowledge and its training data to generate the response.</p>  </li>  <li>    <p>Response: Finally, the LLM generates a response that is more accurate and relevant since it has been augmented with the retrieved information. I mean we gave LLM some additional information from our Knowledge library which allows LLMs to provide more contextually relevant and factual responses, solving the problem of models when they are just hallucinating or providing irrelevant answer.</p>  </li></ul><p>Let’s take an example of company policies again. Suppose you have an HR bot that handles queries related to your Company policies. Now if someones asks anything specific to the policies, The bot can pull the most recent policy documents from the knowledge libray, pass the relevant context to a well crafted prompt which is then passed further to the LLM for generating the response.</p><p>To make it more easy, Imagine a LLM as your knowledgeable friend who seems to know everything, from Geography to Computer Science from Politics to Philosophy. Now, picture yourself asking this friend a few questions:</p><ul>  <li>“Who handles my laundry on weekends?”</li>  <li>“Who lives next door to me?”</li>  <li>“What brand of peanut butter do I prefer?”</li></ul><p>Chances are, your friend wouldn’t be able to answer these questions, right? Most of the time, no.But let’s say this distant friend becomes closer to you over time, he comes at your place regularly, know your parents very well, you both hangout pretty often, you go on outings, blah blah blah.. You got the point right? I mean he is gaining access to personal and insider information about you.  Now, when you pose the same questions, they can somehow answer those question with more relevance now because because he is better suited with your personal insights.</p><p>Similarly, an LLM, when provided with additional information or access to your use case data, won’t guess or hallucinate. Instead, it will leverage that data to provide relevant and accurate answers.</p><h3 id=\"to-break-it-down-here-are-the-exact-steps-to-create-any-rag-application\">To break it down, here are the exact steps to create any RAG application…</h3><ol>  <li>Extract the relevant information from your data sources.</li>  <li>Break the information into the small chunks.</li>  <li>Store the chunks as their embedddings into a vector database.</li>  <li>Create a prompt template which will be fed to the LLM with the query and the context.</li>  <li>Convert the query to it’s relevant embedding using same embedding model.</li>  <li>Fetch k number of relevant documents related to the query from the vector database.</li>  <li>Pass the relevant documents to the LLM and get the response.</li></ol><h3 id=\"faqs\">FAQs</h3><ol>  <li>    <p>We will be using  <u><a href=\"https://python.langchain.com/docs/get_started/introduction\">Langchain</a></u> for this task, Basically it’s like a wrapper which lets you talk and manage to your LLM operations better.</p>  </li>  <li>    <p>Along with it we will be using  <u><a href=\"https://huggingface.co/\">Hugging Face</a></u>, it is like an open-source library for building, training, and deploying state-of-the-art machine learning models, especially about NLP. To use the HuggingFace we need the access token, Get your access token <u><a href=\"https://huggingface.co/docs/hub/security-tokens\">here</a></u></p>  </li>  <li>    <p>For our models, we’ll need two key components: an LLM (Large Language Model) and an embedding model. While paid sources like OpenAI offer these, we’ll be utilizing open-source models to ensure accessibility for everyone.</p>  </li></ol><p>With Langchain for the interface, Hugging Face for fetching the models, along with open-source components, we’re all set to go! This way, we save some bucks while still having everything we need. Let’s move to the next steps</p><h3 id=\"environment-setup\">Environment Setup</h3><p>Open your favorite editor, create a python environment and install the relevant dependencies</p><pre><code class=\"language-python\">python3 -m venv envsource env/bin/activatepip3 install dotenv langchain langchain_community </code></pre><p>Now create a .env file in the same directory to place your Hugging face api credentials like this</p><pre><code class=\"language-python\">HUGGINGFACEHUB_API_TOKEN = hf_KKNWfBqgwCUOHdHFrBwQ.....</code></pre><p>Ensure the name “HUGGINGFACEHUB_API_TOKEN” remains unchanged, as it is crucial for authentication purposes. It seems like everything is in order, and we’re all set!</p><h3 id=\"step-1--extracting-the-relevant-information\">Step 1 : Extracting the relevant information</h3><p>To get your RAG application running, the first thing we need to do is to extract the relevant information from the various data sources. It can be a website page, a PDF file, a notion link, a google doc whatever it is, it needs to be extracted from it’s original source first.</p><pre><code class=\"language-python\">from langchain.document_loaders import PyPDFLoaderfrom langchain_community.document_loaders import WebBaseLoader</code></pre><h3 id=\"step-2--breaking-the-information-into-smaller-chunks\">Step 2 : Breaking the information into smaller chunks</h3><p>We already have the data or the information that we need for creating our RAG application, now comes the time we break down the information into smaller chunks as we will be using the embedding model later on to convert these chunks into there specific embeddings. But why it’s important?</p><p>To understand this, think of it like this : If someone tells you to ingest a 100 page book in one go and ask you a relevant question about that book, it would be very difficult for you to gather the pieces of information from the book and give the answer, instead if you are allowed to break down the information into smaller chunks of 10 pages and each page have an index from 0 to 9 or a numbering,  and then same question is asked again, it would be easy for you to search for the relevant chunk and then based on that chunk of information you can easily answer the question.</p><p>Now images that book as your extracted information, those 10 pages as your small chunks of information and those index as the embedding. In a nutshell we will be using an embedding model on our chunks to convert the information into there embeedings, as a human we might not relate or understand those embeddings but those embeddings are just numeric representation of those chunks.</p><h3 id=\"step-3--creating-the-embeddings-and-store-them-into-a-vectordatabase\">Step 3 : Creating the embeddings and store them into a vectordatabase</h3><p>Now we are just going to pass those chunks to our embedding model and the embedding model will convert the chunks into their corresponding embeddings. There are two options for doing this, either you can download an embedding model in your local, manage the preprocessing and perform compuations locally and use it for the embedding creation,  or you can use Huggingface hub which hosts a vast collection of pre-trained models for various natural language processing (NLP) tasks, including embeddings.</p><p>So what we are going to do is we will create an instance of an embedding model which is provided by Hugging Face, then we will pass the chunks to the embedding model and all the heavy stuff like handling preprocessing, offloading comupatation will be done on the Huggingface servers.</p><p>There are so many options for choosing a right embedding model, you can look at the ![https://huggingface.co/spaces/mteb/leaderboard]leaderboard here and choose the right embedding model according to your needs. For now, we will just use the “bge-base-en-v1.5” which works amazinglly well for creating the embeddings for english texts and it’s compartively small so it’s fast enough to load and do it’s tasks.</p><p>Ok so the embeddings are created. You could see how many number of embeddings are there for each chunk like this:</p><pre><code class=\"language-python\"></code></pre><p>Now comes the vectordatabase. There are ton of different vector databases available in the market, some of them are paid like Pinecone which works fast and have additional features over the ones which are open source like FAISS, Chroma. But to be honest, if you are not looking for anything like scaling your database to few hundred users who are fetching and storing the data from your database every minute, you are good to go with opensource ones. They works amazingingly well. So what we are going to do is we will create an instance of Chroma vector database, and will store our embeddins in it. That’s it. No rocekt science here.</p><h3 id=\"step-4--create-a-prompt-template-which-will-be-fed-to-the-llm\">Step 4 : Create a prompt template which will be fed to the LLM</h3><p>Ok now comes the prompt template. So when you write a question to the ChatGPT and it answers that question, you are basically providing a prompt to the model so that it can understand what’s the question is. When companies train the models, they decide what kind of prompt they are going to use for invoking the model and ask the question. So for example,if you are working with Mistral 7B instruct and you want the optimal results it’s recommended to use the following chat template:</p><pre><code class=\"language-python\">&lt;s&gt;[INST] Instruction [/INST] Model answer&lt;/s&gt;[INST] Follow-up instruction [/INST]</code></pre><p>Note that <s> and </s> are special tokens for beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings. It’s just that the Mistral 7B instruct is made in such a way that the model look for those special tokens to understand the question better. Different types of LLMs have different kinds of instructed prompts. For our case we will be working with zephyer and it’s prompt is like as this:</p><pre><code class=\"language-python\"></code></pre><h3 id=\"step-5--convert-the-query-to-its-relevant-embedding-using-same-embedding-model\">Step 5 : Convert the query to it’s relevant embedding using same embedding model.</h3><p>Ok so far we have converted our textual information into chunks, then we embedded those chunks into there specific embeddings and further more we have stored our embedded chunks into the vector database. Now comes the query or the question that we want to ask to our RAG application. We just can’t pass the query to our model and ask for the information, instead the query will be passed to the same embedding model that is being used for chunks earlier and then only we can do the next set of tasks. Why it’s important? Wel by embedding queries, we enable models to efficiently compare them with previously processed chunks of text, as then only we can do tasks like finding similar documents or generating relevant responses. It’s like translating language into a language computers understand. Imagine you are English Speaker and your friend is a Hindi Speaker, no one understands each other language. Now you give a 1 page document written in Hindi to your English Speaker friend, now your friend will convert that document into the Hindi one first and then will understand the information.</p><p>Now you asks a question to your english speaking friend in Hindi which is related to that documet. Now what would happen is, your friend will convert that question into the english first and then only will be able to understand the question and find relevant response from that information which you have shared earlier.</p><p>Ok so your friend is an embedding model which converted your previous texts into the embeddings. Now when you asks a query or a question, it will be converted into the relevant embeddings from the same embedding model that is being used ealier for chunks and then some search operation will be performed to find the relevant response to your query.</p><p>I hope you get it why we did convert our query into the embeddings first.</p><h3 id=\"step-6--fetch-k-number-of-documents\">Step 6 : Fetch K number of documents.</h3><p>Now comes the retriver. What it does is, it goes into the vector database, performs <em>somekind of search</em> to find the relevant documents (chunks) and return k number of relevant documents which are ranked accordingly based on which one is more contextually related to the query or the question that you asked. We can set k as a parameter which means if you want 2 relevant documents or 5 or 10. Genearlly, if you have less amount of data, it’s advisable to keep k to 2 and for longer documents, it’s recommended to keep k in bewteen 10 - 20</p><h3 id=\"step-7--pass-the-relevant-documents-to-the-llm-and-get-the-response\">Step 7 : Pass the relevant documents to the LLM and get the response.</h3><p>Now comes the last step. So to make things more robust, we will ask our retriever to fetch the k number of relevant documents from the database. We will then pass those k number of documents to our LLM as the context and will ask the LLM to generate a relevant response from that context. That’s it.</p><h3 id=\"whats-next\">What’s next?</h3><p>So there are so many things which can be tweaked here. We can change our embedding model for something more better and better in terms of indexing, searching technique for retriever can be changed too, as well as a better LLM to generate the relevant response.</p>",
            "url": "http://localhost:4000/2024/01/23/rag-application-with-langchain",
            
            
            
            "tags": ["LLM","Deep Learning"],
            
            "date_published": "2024-01-23T00:00:00+05:30",
            "date_modified": "2024-01-23T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://localhost:4000/2024/01/14/ml-fine-print",
            "title": "ML Fine Print",
            "summary": "Take a look at the fonts in this post that can make your text editor or terminal emulator look little bit nicer",
            "content_text": "ML Fine Print is my essential resource for recalling and understanding crucial details in the ML software cycle which often overlooked in the heat of real-world challenges. The documentation is designed for easy reference without a specific reading order.Overfitting and Regularization      Overfitting occurs when our model becomes too complex, fitting too closely to the noise in the training data. While it might perform well during training, it often falters during testing or real-world applications. This happens when our model gets too attuned to the specific variations in the training data, as it starts to memorize the data instead of capturing the relevant patterns in it, resulting in poor generalization to new, unseen data. A model with low bias and high variance is considered overfit and doesn’t perform well with new data.        Regularization steps comes in as a solution, introducing a penalty for excessive model complexity to curb overfitting. Techniques like Early Stopping, L2, and L1 regularization help strike a balance between fitting the training data and keeping the model reasonably simple.        Early Stopping involves halting training when a threshold of convergence is reached or when the validation loss starts to rise, preventing overfitting by avoiding excessive training. On the other hand, L2 regularization ensures that weights of various features don’t overlap excessively, as one feature weights shouldn’t overperform over the others promoting a balance. While the L1 regularization focuses on sparsity, driving some weights to absolute zero, doing so, it emphasize on keeping the informative features only.        Choosing the right lambda value in regularization is crucial. Too high, and your model may underfit; too low, and it may overfit. It’s a delicate tradeoff between simplicity and training data fit. The ideal lambda value is data-dependent, requiring hyperparameter tuning for optimal results. Despite potential increases in training loss, regularization often improves real-time predictions during inference, emphasizing the importance of overall model performance. Overall, regularization acts as a “keep things reasonable” rule for models, preventing extremes and ensuring their ability to make accurate predictions for new, unseen data.  Assumptions for Data SamplingIn data sampling, we often make certain assumptions to ensure the validity and reliability of our results. These assumptions include:      Independent and Identically Distributed (i.i.d.): Examples are drawn independently and identically from the same distribution. This means that the probability of selecting any particular example is the same for all examples, and the selection of one example does not influence the selection of any other example.        Stationarity: The distribution of the data does not change over time or across different parts of the data set. This means that the probability of observing a particular value or outcome is the same regardless of when or where in the data set it is observed        Same Distribution: All examples are drawn from the same distribution. This means that the underlying process that generates the data is the same for all examples.  Violations in Practice:      Non-Independence: In some cases, the i.i.d. assumption may be violated due to dependencies between examples. For example, in a model that chooses ads to display, the choice of ads may be influenced by the user’s previous ad history, creating a temporal relationship between observations.        Non-Stationarity: The stationarity assumption may be violated if the underlying distribution of the data changes over time or across different parts of the data set. For example, in a data set of retail sales information, user purchases may change seasonally, violating stationarity.        Different Distributions: The same distribution assumption may be violated if examples are drawn from different distributions. For example, in a data set of customer reviews, the distribution of reviews may differ between different products or services.  Test and Training Set      In machine learning, splitting the data into test and training sets is a crucial step. Randomization of the data before splitting is essential to ensure that the model does not train on a biased subset of the data. For instance, we would not want our Climate Predictor model to train solely on summer data and then be used for inference on test data consisting exclusively of winter data.        When dealing with a large dataset containing billions of sample points, a small percentage (5-10%) of the data can be sufficient for testing the model during inference. However, if the dataset is relatively small, alternative methods like cross-validation may be necessary for better results.        It is important to note that the test set should never be exposed to the model during training time. Repeated evaluation on the same test set can lead to implicit overfitting, which reduces the model’s ability to generalize to new, unseen data.        Furthermore, it is essential to shuffle the training dataset before creating a validation split. This is because sometimes, no matter how the training set and validation set are split, the loss curves may differ significantly. This issue is often caused by dissimilarities in the data between the training set and the validation set. Most libraries, including Pandas, split the dataset sequentially, which can lead to problems if the dataset points are arranged in a specific order. Therefore, it is recommended to randomize or shuffle the dataset points before splitting, ensuring that both the training and validation sets have an equivalent distribution of dataset points.  Validation Set      To avoid overfitting to a specific test set, it is advisable to use a validation set for tuning the model’s hyperparameters. This allows for more objective evaluation of the model’s performance and reduces the risk of implicit overfitting.        Using the same data set for testing and validation can limit the potential for tuning the hyperparameters or improving the model. It is beneficial to continuously acquire more data to refresh the test and validation sets, ensuring a more comprehensive evaluation of the model’s performance.        It is important to note that the internal model parameters are adjusted during the training process, while the hyperparameters are tuned based on the results obtained from the validation and test sets.  Convergence      Convergence is the state of model training when the loss function exhibits minimal change, indicating that further training will not significantly improve the model’s performance        It is worth noting that the loss function may remain constant for a number of iterations before it begins to decrease. This can lead to a false sense of convergence, and it is important to monitor the loss function over a longer period to ensure true convergence.  Feature EngineeringBinning Trick is generally used when we have the continuous numerical data and we want to convert them into the discrete bins or intervals. This technique is highly useful when we are working with the algorithms that works really well with the categorical data or when we wanted to reduced the impact of the outliers.Let’s say you are working on the housing price prediction problem and encoded the street_name as the numerical number starting from 0 to n, doing so will create the bias as our model would assume you have ordered the streets based on their average house prices, or for that matter, many houses are located at the corner of two streets, and there’s no way to encode that information in the street_name value if it contains a single index. To solve this problem, we can create a binary feature vector where each column represents a street name, with 1 indicating the presence of the house on that street. As if the house is present on two streets, the model will use the weights for both of the streets as the feature vector for that house would have 1 for those two streets.Techniques like One-Hot encoding and the Label encoding helps to create the meaningful representations for the data that we can’t use directly to feed to our model in it’s raw form.If you have a dataset with 1,000,000 different street names for the street_name feature, creating a binary vector with true or false for each name is inefficient in terms of storage and computation. Instead, a common approach is to use a sparse representation, storing only nonzero values.  That is if we have 35 street_name and our house belongs to the street 24 then instead of storing the 35 different bits as the indicators we could store the 24Sometimes, taking the logarithm of a distribution address long tail issues, especially in data analysis and statistics. The long tail typically refers to a distribution where a few values occur frequently (the “head” of the distribution), while many other values occur infrequently (the “tail” of the distribution). By taking the logarithm, we can compress the range of values, which can be particularly helpful when dealing with data that has a wide spread. This transformation is especially useful when the data has a positive skewness, meaning that the tail is on the right side of the distribution.Clipping the outliers beyond a certain set threshold is also a part of the feature engineering, necessarily for a given use case if the threshold is set to 4.0 then Clipping the feature value at 4.0 doesn’t mean that we ignore all values greater than 4.0. Rather, it means that all values that were greater than 4.0 now become 4.0.Dealing with the missing values, duplicates, bad labels and bad feature values is important to make sure we have a robust set of data pipeline which we can use for our models.Note:      We shouldn’t pass a sparse representation as described about as a direct feature input to a model. Instead, we should convert the sparse representation into a one-hot representation before training on it.        There is a minor difference between Sparse Vector and Sparse Representation. First one actually means the vector consisting mostly zeroes and the second one actually means the dense representation of a Sparse Vector.        If you have continuous values like latitudes and longitudes for a housing price prediction problem, it is advised to use the bins instead of floating points numbers for the model predictions, as using them in the floating point values provides almost no predictive powers, instead we can create the bins with the specified boundaries as neighborhoods at latitude 35.4 and 35.8 are in the same bucket, but neighborhoods in latitude 35.4 and 36.2 are in different buckets. This way the model will learn a separate weight for each bucket. For example, the model will learn one weight for all the neighborhoods in the “35” bin, a different weight for neighborhoods in the “36” bin, and so on.  ScalingScaling involves transforming feature values from their original range (e.g., 10000 to 50000) to a more standard range, such as 0 to 1 or -1 to +1. This process is particularly useful when dealing with multiple features in a dataset, as it provides several benefits:      Improved convergence of gradient descent: Scaling the features can accelerate the convergence of gradient descent algorithms, as it ensures that all features are on the same scale and contribute equally to the optimization process.        Prevention of floating-point precision issues: During training, if the weights for a feature column exceed the floating-point precision limit, they may be set to NaN (Not a Number). This can cause a chain reaction, resulting in all the numbers in the model becoming NaN. Scaling the features helps to prevent this issue by keeping the weights within a manageable range.        Reduction of bias: Without feature scaling, a model may be biased towards features with a wider range of values. This is because the model will give more weight to these features during the training process. Scaling the features ensures that all features are treated equally, regardless of their range.        It is possible to apply different scaling thresholds to distinct features. For instance, one feature could be scaled between -2 to +2, while another might range from -4 to +4. However, using an excessively large scale, such as -10,000 to +10,000, could lead to suboptimal results.  Z-Score Normalization      Z-score normalization, also known as standardization or z-score scaling, is a statistical method used to make datasets comparable by transforming them into a standard normal distribution. This technique is commonly applied in statistics and machine learning for analyzing data points on a standardized scale.        To calculate the z-score for a data point (x) in a distribution, we use the formula: z = (x - μ) / σ, where μ is the mean of the dataset, and σ is the standard deviation. The z-score ensures that different features share a common scale, making comparison and analysis more straightforward. This proves especially useful when dealing with variables of diverse units or scales, preventing any single variable from dominating the analysis due to its magnitude.        After z-score normalization, each feature column in the transformed dataset has a mean of 0 and a standard deviation of 1. This normalization results in a standard normal distribution, making it easier to interpret and work with for statistical analyses.    Note: The fit and transform steps for z-score normalization should be applied initially to the complete dataset. Since z-score normalization involves both fitting and transforming methods, it’s essential to use the same normalizer when transforming the test data.  Feature CrossesA feature cross serves as a synthetic feature, injecting nonlinearity into the feature space by multiplying two or more input features. In simpler terms, it enables a linear model to grasp and interpret non-linear relationships.Consider having two features, x1 and x2, insufficient for our linear model to comprehend non-linearity. Enter the feature cross, x3, defined as the product of x1 and x2:x3 = x1*x2This newly introduced feature cross seamlessly integrates into the linear formula:Y = b + w1*x1 + w2*x2 + w3*x3Even though w3 encodes nonlinear information, the linear model adapts naturally during training, requiring no special adjustments.Feature crosses prove highly effective in enhancing and scale linear models when dealing with massive datasets which often have non linearity. Integrating feature crosses into our modeling practices significantly improves architecture and results. They play a crucial role in capturing complex relationships between features that might be not related when considered in isolation. By enabling the model to learn both individual and combined effects of features, feature crosses prove essential in capturing the difficulties of the of real-world data.For a matter of fact, sometimes in real-world applications, feature crosses are used not just for continuous features, but also for one-hot feature vectors. One-hot feature vector crosses act as logical conjunctions, revealing unknown patterns and interactions in the data which might not be known previously.For an instance, think of online shopping, now there might be some one-hot encoding features related to user behavior, shopping type, and product type, now feature crosses allows the model to create features like “Product Segment A AND Shopping Type Mobile.” This logical conjunction helps in capturing difficult patterns, I mean it may give some insights regarding when and how Product Segment A is being choosed when the person is shopping for a Mobile set, potentially improving predictive performance overall. Sometimes when our linear model is not performing upto mark, creating the features crosses might be a good choice.Classification MetricsAccuracy is a commonly used metric to evaluate the performance of a classification model. It is defined as the ratio of correctly predicted instances to the total number of instances. While accuracy is a straightforward and intuitive metric, it can be misleading in certain situations. In cases when we have in-balanced dataset, a classifier can achieve a higher accuracy by just simply predicting the majority class. However, this doesn’t mean the model is performing well. It might be failing to correctly identify instances of the minority class, which could be the more critical class in certain applications.Let’s consider an example of a medical diagnostic test for a rare disease. Suppose we have a dataset with 1000 instances, and only 10 of them belong to the positive class (people with the disease), while the remaining 990 instances belong to the negative class (people without the disease).Now, let’s imagine we have a simple classifier that predicts everyone as negative. Here’s how the confusion matrix would look like:Now, let’s calculate the accuracy:Accuracy = Number of Correct Predictions​ / Total number of instances =&gt; 990 / 1000  =&gt;  99 %The accuracy seems very high (99%), but the model is not performing well in identifying individuals with the disease. It predicts every dataset point as negative, so it misses the cases of the positive class. In a medical context, failing to identify individuals with the disease (false negatives) can have serious consequences.In this example, accuracy is misleading because the dataset is imbalanced, and the classifier achieves high accuracy by simply predicting the majority class. In such situations, accuracy alone doesn’t provide a complete picture of the model’s performance, and it’s important to consider other metrics like sensitivity (recall) or the F1 score, which take into account the ability of the model to correctly identify positive instances.      Precision and Recall are generally used to get the better understanding of the classification process we did. Precision means out of all the cases where your model marked something as positive, how many of them were actually true. So let’s say if marking an email correctly as spam if considered as Positive thing, then out of all the cases where your model marked some emails as Positive, how many of those emails were actually spam. On the other hand, Recall means out of all the positive cases from your dataset, what percentage did our model marked correctly as positive. So if you have 100 emails and 40 of them are spammed and your model predicted 30 of the 40 spammed emails as Spammed then the recall would be 30/40.        Remember that there is always some kind of trade-off between the Precision and Recall. As let’s say our threshold is 0.4 initially, that means all the cases where our classification model gives us the probability equal to or greater than 0.4, it’s considered to be Positive, so if we are more concerned for higher precision, or we want our model to be very sure that if it marks something as positive , it would be positive actually then we can increase the threshold to maybe a higher number of 0.6, so now only those instances when the probability from the classification model is higher than or equal to 0.6 will be considered positive which in turn will decrease the false positive and increase the Precision overall        But if we increase the threshold to 0.6 then it might happen that the instances when the probability for some dataset points are below the 0.6, let’s say 0.5 but our model will still marked them negative as it’s not above or equal to the threshold that we decided already, so now the model misses that, and it will only predict something as positive if it’s beyond that threshold and in that process it might miss the actual ones. So for those cases Recall will decrease        Tuning a threshold for logistic regression is different from tuning hyperparameters such as learning rate. Part of choosing a threshold is assessing how much you’ll suffer for making a mistake. For example, mistakenly labeling a non-spam message as spam is very bad. However, mistakenly labeling a spam message as non-spam is unpleasant, but hardly the end of your job.        In general, a model that outperforms another model on both precision and recall is likely the better model. Obviously, we’ll need to make sure that comparison is being done at a precision / recall point that is useful in practice for this to be meaningful. For example, suppose our spam detection model needs to have at least 90% precision to be useful and avoid unnecessary false alarms. In this case, comparing one model at {20% precision, 99% recall} to another at {15% precision, 98% recall} is not particularly instructive, as neither model meets the 90% precision requirement. But with that caveat in mind, this is a good way to think about comparing models when using precision and recall.        Precision and Recall solely depends on the threshold we choose, so it’s important to come up with an adequate set of threshold values.  ROC and AUCROC (Receiver Operating Characteristic) and AUC (Area Under the Curve) are closely related concepts used to evaluate the performance of classification models, particularly binary classifiers. Here are the key differences between ROC and AUC:  ROC (Receiver Operating Characteristic):                  Definition: The ROC curve is a graphical representation of the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) at various thresholds.                    Components: The ROC curve is typically a plot of true positive rate (y-axis) against the false positive rate (x-axis) for different threshold values.                    Interpretation: A model with a better performance will have an ROC curve that is closer to the top-left corner of the plot, indicating higher true positive rates and lower false positive rates across different threshold values.              AUC (Area Under the Curve):                  Definition: AUC is a scalar value that represents the area under the ROC curve. It provides a single numerical summary of the model’s ability to distinguish between the two classes.                    Range: AUC values range from 0 to 1, where a higher AUC indicates better discrimination performance. A model with an AUC of 0.5 is no better than random, while an AUC of 1 represents a perfect classifier.                    Interpretation: The AUC is a useful metric for comparing and ranking different models. A higher AUC suggests a better overall ability of the model to discriminate between positive and negative instances across all possible threshold values. That being said, a model with higher AUC suggests that our model ranks a random positive example more highly than a random negative example.                    AUC is desirable for the following two reasons , first it is scale-invariant as it measures how well predictions are ranked, rather than their absolute values. Secondly it is classification-threshold-invariant. as it measures the quality of the model’s predictions irrespective of what classification threshold is chosen.              However, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases:        Scale invariance is not always desirable. For example, sometimes we really do need well calibrated probability outputs, and AUC won’t tell us about that.        Classification-threshold invariance is not always desirable. In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives even if that results in a significant increase of false negatives. I mean if you are doing that job, You’d want to avoid marking important emails as spam, even if it means missing some spam emails. AUC isn’t a useful metric for this type of optimization.  Key Differences:      Format: ROC is a curve (plot), while AUC is a single scalar value.        Graphical Representation: ROC visually displays the trade-off between true positive rate and false positive rate, while AUC condenses this information into a single number.        Interpretation: ROC is useful for understanding the model’s behavior across different thresholds, while AUC provides a summary measure of overall model performance.        Note : To understand what Threshold term means, refer to topic of Classification Metrics above this section.  ",
            "content_html": "<p><em>ML Fine Print is my essential resource for recalling and understanding crucial details in the ML software cycle which often overlooked in the heat of real-world challenges. The documentation is designed for easy reference without a specific reading order.</em></p><h3 id=\"overfitting-and-regularization\">Overfitting and Regularization</h3><ul>  <li>    <p>Overfitting occurs when our model becomes too complex, fitting too closely to the noise in the training data. While it might perform well during training, it often falters during testing or real-world applications. This happens when our model gets too attuned to the specific variations in the training data, as it starts to memorize the data instead of capturing the relevant patterns in it, resulting in poor generalization to new, unseen data. A model with low bias and high variance is considered overfit and doesn’t perform well with new data.</p>  </li>  <li>    <p>Regularization steps comes in as a solution, introducing a penalty for excessive model complexity to curb overfitting. Techniques like Early Stopping, L2, and L1 regularization help strike a balance between fitting the training data and keeping the model reasonably simple.</p>  </li>  <li>    <p>Early Stopping involves halting training when a threshold of convergence is reached or when the validation loss starts to rise, preventing overfitting by avoiding excessive training. On the other hand, L2 regularization ensures that weights of various features don’t overlap excessively, as one feature weights shouldn’t overperform over the others promoting a balance. While the L1 regularization focuses on sparsity, driving some weights to absolute zero, doing so, it emphasize on keeping the informative features only.</p>  </li>  <li>    <p>Choosing the right lambda value in regularization is crucial. Too high, and your model may underfit; too low, and it may overfit. It’s a delicate tradeoff between simplicity and training data fit. The ideal lambda value is data-dependent, requiring hyperparameter tuning for optimal results. Despite potential increases in training loss, regularization often improves real-time predictions during inference, emphasizing the importance of overall model performance. Overall, regularization acts as a “keep things reasonable” rule for models, preventing extremes and ensuring their ability to make accurate predictions for new, unseen data.</p>  </li></ul><h3 id=\"assumptions-for-data-sampling\">Assumptions for Data Sampling</h3><p>In data sampling, we often make certain assumptions to ensure the validity and reliability of our results. These assumptions include:</p><ol>  <li>    <p>Independent and Identically Distributed (i.i.d.): Examples are drawn independently and identically from the same distribution. This means that the probability of selecting any particular example is the same for all examples, and the selection of one example does not influence the selection of any other example.</p>  </li>  <li>    <p>Stationarity: The distribution of the data does not change over time or across different parts of the data set. This means that the probability of observing a particular value or outcome is the same regardless of when or where in the data set it is observed</p>  </li>  <li>    <p>Same Distribution: All examples are drawn from the same distribution. This means that the underlying process that generates the data is the same for all examples.</p>  </li></ol><h3 id=\"violations-in-practice\">Violations in Practice:</h3><ul>  <li>    <p>Non-Independence: In some cases, the i.i.d. assumption may be violated due to dependencies between examples. For example, in a model that chooses ads to display, the choice of ads may be influenced by the user’s previous ad history, creating a temporal relationship between observations.</p>  </li>  <li>    <p>Non-Stationarity: The stationarity assumption may be violated if the underlying distribution of the data changes over time or across different parts of the data set. For example, in a data set of retail sales information, user purchases may change seasonally, violating stationarity.</p>  </li>  <li>    <p>Different Distributions: The same distribution assumption may be violated if examples are drawn from different distributions. For example, in a data set of customer reviews, the distribution of reviews may differ between different products or services.</p>  </li></ul><h3 id=\"test-and-training-set\">Test and Training Set</h3><ul>  <li>    <p>In machine learning, splitting the data into test and training sets is a crucial step. Randomization of the data before splitting is essential to ensure that the model does not train on a biased subset of the data. For instance, we would not want our Climate Predictor model to train solely on summer data and then be used for inference on test data consisting exclusively of winter data.</p>  </li>  <li>    <p>When dealing with a large dataset containing billions of sample points, a small percentage (5-10%) of the data can be sufficient for testing the model during inference. However, if the dataset is relatively small, alternative methods like cross-validation may be necessary for better results.</p>  </li>  <li>    <p>It is important to note that the test set should never be exposed to the model during training time. Repeated evaluation on the same test set can lead to implicit overfitting, which reduces the model’s ability to generalize to new, unseen data.</p>  </li>  <li>    <p>Furthermore, it is essential to shuffle the training dataset before creating a validation split. This is because sometimes, no matter how the training set and validation set are split, the loss curves may differ significantly. This issue is often caused by dissimilarities in the data between the training set and the validation set. Most libraries, including Pandas, split the dataset sequentially, which can lead to problems if the dataset points are arranged in a specific order. Therefore, it is recommended to randomize or shuffle the dataset points before splitting, ensuring that both the training and validation sets have an equivalent distribution of dataset points.</p>  </li></ul><h3 id=\"validation-set\">Validation Set</h3><ul>  <li>    <p>To avoid overfitting to a specific test set, it is advisable to use a validation set for tuning the model’s hyperparameters. This allows for more objective evaluation of the model’s performance and reduces the risk of implicit overfitting.</p>  </li>  <li>    <p>Using the same data set for testing and validation can limit the potential for tuning the hyperparameters or improving the model. It is beneficial to continuously acquire more data to refresh the test and validation sets, ensuring a more comprehensive evaluation of the model’s performance.</p>  </li>  <li>    <p>It is important to note that the internal model parameters are adjusted during the training process, while the hyperparameters are tuned based on the results obtained from the validation and test sets.</p>  </li></ul><h3 id=\"convergence\">Convergence</h3><ul>  <li>    <p>Convergence is the state of model training when the loss function exhibits minimal change, indicating that further training will not significantly improve the model’s performance</p>  </li>  <li>    <p>It is worth noting that the loss function may remain constant for a number of iterations before it begins to decrease. This can lead to a false sense of convergence, and it is important to monitor the loss function over a longer period to ensure true convergence.</p>  </li></ul><h3 id=\"feature-engineering\">Feature Engineering</h3><p>Binning Trick is generally used when we have the continuous numerical data and we want to convert them into the discrete bins or intervals. This technique is highly useful when we are working with the algorithms that works really well with the categorical data or when we wanted to reduced the impact of the outliers.</p><p>Let’s say you are working on the housing price prediction problem and encoded the <strong>street_name</strong> as the numerical number starting from 0 to n, doing so will create the bias as our model would assume you have ordered the streets based on their average house prices, or for that matter, many houses are located at the corner of two streets, and there’s no way to encode that information in the <strong>street_name</strong> value if it contains a single index. To solve this problem, we can create a binary feature vector where each column represents a street name, with 1 indicating the presence of the house on that street. As if the house is present on two streets, the model will use the weights for both of the streets as the feature vector for that house would have 1 for those two streets.</p><p>Techniques like One-Hot encoding and the Label encoding helps to create the meaningful representations for the data that we can’t use directly to feed to our model in it’s raw form.</p><p>If you have a dataset with 1,000,000 different street names for the <em>street_name</em> feature, creating a binary vector with true or false for each name is inefficient in terms of storage and computation. Instead, a common approach is to use a sparse representation, storing only nonzero values.  That is if we have 35 <em>street_name</em> and our house belongs to the <em>street</em> 24 then instead of storing the 35 different bits as the indicators we could store the 24</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/assets/images/sparse_representation.png?raw=true\" alt=\"sparse representation\" /></p><p>Sometimes, taking the logarithm of a distribution address long tail issues, especially in data analysis and statistics. The long tail typically refers to a distribution where a few values occur frequently (the “head” of the distribution), while many other values occur infrequently (the “tail” of the distribution). By taking the logarithm, we can compress the range of values, which can be particularly helpful when dealing with data that has a wide spread. This transformation is especially useful when the data has a positive skewness, meaning that the tail is on the right side of the distribution.</p><p>Clipping the outliers beyond a certain set threshold is also a part of the feature engineering, necessarily for a given use case if the threshold is set to 4.0 then Clipping the feature value at 4.0 doesn’t mean that we ignore all values greater than 4.0. Rather, it means that all values that were greater than 4.0 now become 4.0.</p><p>Dealing with the missing values, duplicates, bad labels and bad feature values is important to make sure we have a robust set of data pipeline which we can use for our models.</p><p><strong>Note:</strong></p><ul>  <li>    <p>We shouldn’t pass a sparse representation as described about as a direct feature input to a model. Instead, we should convert the sparse representation into a one-hot representation before training on it.</p>  </li>  <li>    <p>There is a minor difference between Sparse Vector and Sparse Representation. First one actually means the vector consisting mostly zeroes and the second one actually means the <em>dense representation</em> of a Sparse Vector.</p>  </li>  <li>    <p>If you have continuous values like latitudes and longitudes for a housing price prediction problem, it is advised to use the bins instead of floating points numbers for the model predictions, as using them in the floating point values provides almost no predictive powers, instead we can create the bins with the specified boundaries as neighborhoods at latitude 35.4 and 35.8 are in the same bucket, but neighborhoods in latitude 35.4 and 36.2 are in different buckets. This way the model will learn a separate weight for each bucket. For example, the model will learn one weight for all the neighborhoods in the “35” bin, a different weight for neighborhoods in the “36” bin, and so on.</p>  </li></ul><h3 id=\"scaling\">Scaling</h3><p>Scaling involves transforming feature values from their original range (e.g., 10000 to 50000) to a more standard range, such as 0 to 1 or -1 to +1. This process is particularly useful when dealing with multiple features in a dataset, as it provides several benefits:</p><ul>  <li>    <p>Improved convergence of gradient descent: Scaling the features can accelerate the convergence of gradient descent algorithms, as it ensures that all features are on the same scale and contribute equally to the optimization process.</p>  </li>  <li>    <p>Prevention of floating-point precision issues: During training, if the weights for a feature column exceed the floating-point precision limit, they may be set to NaN (Not a Number). This can cause a chain reaction, resulting in all the numbers in the model becoming NaN. Scaling the features helps to prevent this issue by keeping the weights within a manageable range.</p>  </li>  <li>    <p>Reduction of bias: Without feature scaling, a model may be biased towards features with a wider range of values. This is because the model will give more weight to these features during the training process. Scaling the features ensures that all features are treated equally, regardless of their range.</p>  </li>  <li>    <p>It is possible to apply different scaling thresholds to distinct features. For instance, one feature could be scaled between -2 to +2, while another might range from -4 to +4. However, using an excessively large scale, such as -10,000 to +10,000, could lead to suboptimal results.</p>  </li></ul><h3 id=\"z-score-normalization\">Z-Score Normalization</h3><ul>  <li>    <p>Z-score normalization, also known as standardization or z-score scaling, is a statistical method used to make datasets comparable by transforming them into a standard normal distribution. This technique is commonly applied in statistics and machine learning for analyzing data points on a standardized scale.</p>  </li>  <li>    <p>To calculate the z-score for a data point (x) in a distribution, we use the formula: z = (x - μ) / σ, where μ is the mean of the dataset, and σ is the standard deviation. The z-score ensures that different features share a common scale, making comparison and analysis more straightforward. This proves especially useful when dealing with variables of diverse units or scales, preventing any single variable from dominating the analysis due to its magnitude.</p>  </li>  <li>    <p>After z-score normalization, each feature column in the transformed dataset has a mean of 0 and a standard deviation of 1. This normalization results in a standard normal distribution, making it easier to interpret and work with for statistical analyses.</p>    <p><em><strong>Note</strong>: The fit and transform steps for z-score normalization should be applied initially to the complete dataset. Since z-score normalization involves both fitting and transforming methods, it’s essential to use the same normalizer when transforming the test data.</em></p>  </li></ul><h3 id=\"feature-crosses\">Feature Crosses</h3><p>A <strong>feature cross</strong> serves as a synthetic feature, injecting nonlinearity into the feature space by multiplying two or more input features. In simpler terms, it enables a linear model to grasp and interpret non-linear relationships.</p><p>Consider having two features, x1 and x2, insufficient for our linear model to comprehend non-linearity. Enter the feature cross, x3, defined as the product of x1 and x2:</p><pre><code class=\"language-text\">x3 = x1*x2</code></pre><p>This newly introduced feature cross seamlessly integrates into the linear formula:</p><pre><code class=\"language-text\">Y = b + w1*x1 + w2*x2 + w3*x3</code></pre><p>Even though w3 encodes nonlinear information, the linear model adapts naturally during training, requiring no special adjustments.</p><p>Feature crosses prove highly effective in enhancing and scale linear models when dealing with massive datasets which often have non linearity. Integrating feature crosses into our modeling practices significantly improves architecture and results. They play a crucial role in capturing complex relationships between features that might be not related when considered in isolation. By enabling the model to learn both individual and combined effects of features, feature crosses prove essential in capturing the difficulties of the of real-world data.</p><p>For a matter of fact, sometimes in real-world applications, feature crosses are used not just for continuous features, but also for one-hot feature vectors. One-hot feature vector crosses act as logical conjunctions, revealing unknown patterns and interactions in the data which might not be known previously.</p><p>For an instance, think of online shopping, now there might be some one-hot encoding features related to user behavior, shopping type, and product type, now feature crosses allows the model to create features like “Product Segment A AND Shopping Type Mobile.” This logical conjunction helps in capturing difficult patterns, I mean it may give some insights regarding when and how Product Segment A is being choosed when the person is shopping for a Mobile set, potentially improving predictive performance overall. Sometimes when our linear model is not performing upto mark, creating the features crosses might be a good choice.</p><h3 id=\"classification-metrics\">Classification Metrics</h3><p>Accuracy is a commonly used metric to evaluate the performance of a classification model. It is defined as the ratio of correctly predicted instances to the total number of instances. While accuracy is a straightforward and intuitive metric, it can be misleading in certain situations. In cases when we have in-balanced dataset, a classifier can achieve a higher accuracy by just simply predicting the majority class. However, this doesn’t mean the model is performing well. It might be failing to correctly identify instances of the minority class, which could be the more critical class in certain applications.</p><p>Let’s consider an example of a medical diagnostic test for a rare disease. Suppose we have a dataset with 1000 instances, and only 10 of them belong to the positive class (people with the disease), while the remaining 990 instances belong to the negative class (people without the disease).</p><p>Now, let’s imagine we have a simple classifier that predicts everyone as negative. Here’s how the confusion matrix would look like:</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/assets/images/tptn.png?raw=true\" alt=\"accuracy\" /></p><p>Now, let’s calculate the accuracy:</p><p><em>Accuracy = Number of Correct Predictions​ / Total number of instances =&gt; 990 / 1000  =&gt;  99 %</em></p><p>The accuracy seems very high (99%), but the model is not performing well in identifying individuals with the disease. It predicts every dataset point as negative, so it misses the cases of the positive class. In a medical context, failing to identify individuals with the disease (false negatives) can have serious consequences.</p><p>In this example, accuracy is misleading because the dataset is imbalanced, and the classifier achieves high accuracy by simply predicting the majority class. In such situations, accuracy alone doesn’t provide a complete picture of the model’s performance, and it’s important to consider other metrics like sensitivity (recall) or the F1 score, which take into account the ability of the model to correctly identify positive instances.</p><ol>  <li>    <p>Precision and Recall are generally used to get the better understanding of the classification process we did. Precision means out of all the cases where your model marked something as positive, how many of them were actually true. So let’s say if marking an email correctly as spam if considered as Positive thing, then out of all the cases where your model marked some emails as Positive, how many of those emails were actually spam. On the other hand, Recall means out of all the positive cases from your dataset, what percentage did our model marked correctly as positive. So if you have 100 emails and 40 of them are spammed and your model predicted 30 of the 40 spammed emails as Spammed then the recall would be 30/40.</p>  </li>  <li>    <p>Remember that there is always some kind of trade-off between the Precision and Recall. As let’s say our threshold is 0.4 initially, that means all the cases where our classification model gives us the probability equal to or greater than 0.4, it’s considered to be Positive, so if we are more concerned for higher precision, or we want our model to be very sure that if it marks something as positive , it would be positive actually then we can increase the threshold to maybe a higher number of 0.6, so now only those instances when the probability from the classification model is higher than or equal to 0.6 will be considered positive which in turn will decrease the false positive and increase the Precision overall</p>  </li>  <li>    <p>But if we increase the threshold to 0.6 then it might happen that the instances when the probability for some dataset points are below the 0.6, let’s say 0.5 but our model will still marked them negative as it’s not above or equal to the threshold that we decided already, so now the model misses that, and it will only predict something as positive if it’s beyond that threshold and in that process it might miss the actual ones. So for those cases Recall will decrease</p>  </li>  <li>    <p>Tuning a threshold for logistic regression is different from tuning hyperparameters such as learning rate. Part of choosing a threshold is assessing how much you’ll suffer for making a mistake. For example, mistakenly labeling a non-spam message as spam is very bad. However, mistakenly labeling a spam message as non-spam is unpleasant, but hardly the end of your job.</p>  </li>  <li>    <p>In general, a model that outperforms another model on both precision and recall is likely the better model. Obviously, we’ll need to make sure that comparison is being done at a precision / recall point that is useful in practice for this to be meaningful. For example, suppose our spam detection model needs to have at least 90% precision to be useful and avoid unnecessary false alarms. In this case, comparing one model at {20% precision, 99% recall} to another at {15% precision, 98% recall} is not particularly instructive, as neither model meets the 90% precision requirement. But with that caveat in mind, this is a good way to think about comparing models when using precision and recall.</p>  </li>  <li>    <p>Precision and Recall solely depends on the threshold we choose, so it’s important to come up with an adequate set of threshold values.</p>  </li></ol><h3 id=\"roc-and-auc\">ROC and AUC</h3><p>ROC (Receiver Operating Characteristic) and AUC (Area Under the Curve) are closely related concepts used to evaluate the performance of classification models, particularly binary classifiers. Here are the key differences between ROC and AUC:</p><p><img src=\"https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/assets/images/ROC_Curve.png?raw=true\" alt=\"rocauccurve\" /></p><ol>  <li><strong>ROC (Receiver Operating Characteristic):</strong>    <ul>      <li>        <p><strong>Definition:</strong> The ROC curve is a graphical representation of the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) at various thresholds.</p>      </li>      <li>        <p><strong>Components:</strong> The ROC curve is typically a plot of true positive rate (y-axis) against the false positive rate (x-axis) for different threshold values.</p>      </li>      <li>        <p><strong>Interpretation:</strong> A model with a better performance will have an ROC curve that is closer to the top-left corner of the plot, indicating higher true positive rates and lower false positive rates across different threshold values.</p>      </li>    </ul>  </li>  <li><strong>AUC (Area Under the Curve):</strong>    <ul>      <li>        <p><strong>Definition:</strong> AUC is a scalar value that represents the area under the ROC curve. It provides a single numerical summary of the model’s ability to distinguish between the two classes.</p>      </li>      <li>        <p><strong>Range:</strong> AUC values range from 0 to 1, where a higher AUC indicates better discrimination performance. A model with an AUC of 0.5 is no better than random, while an AUC of 1 represents a perfect classifier.</p>      </li>      <li>        <p><strong>Interpretation:</strong> The AUC is a useful metric for comparing and ranking different models. A higher AUC suggests a better overall ability of the model to discriminate between positive and negative instances across all possible threshold values. That being said, a model with higher AUC suggests that our model ranks a random positive example more highly than a random negative example.</p>      </li>      <li>        <p>AUC is desirable for the following two reasons , first it is <strong>scale-invariant</strong> as it measures how well predictions are ranked, rather than their absolute values. Secondly it is <strong>classification-threshold-invariant</strong>. as it measures the quality of the model’s predictions irrespective of what classification threshold is chosen.</p>      </li>    </ul>    <p>However, both these reasons come with caveats, which may limit the usefulness of AUC in certain use cases:</p>  </li></ol><ul>  <li>    <p><strong>Scale invariance is not always desirable.</strong> For example, sometimes we really do need well calibrated probability outputs, and AUC won’t tell us about that.</p>  </li>  <li>    <p><strong>Classification-threshold invariance is not always desirable.</strong> In cases where there are wide disparities in the cost of false negatives vs. false positives, it may be critical to minimize one type of classification error. For example, when doing email spam detection, you likely want to prioritize minimizing false positives even if that results in a significant increase of false negatives. I mean if you are doing that job, You’d want to avoid marking important emails as spam, even if it means missing some spam emails. AUC isn’t a useful metric for this type of optimization.</p>  </li></ul><h3 id=\"key-differences\">Key Differences:</h3><ul>  <li>    <p><strong>Format:</strong> ROC is a curve (plot), while AUC is a single scalar value.</p>  </li>  <li>    <p><strong>Graphical Representation:</strong> ROC visually displays the trade-off between true positive rate and false positive rate, while AUC condenses this information into a single number.</p>  </li>  <li>    <p><strong>Interpretation:</strong> ROC is useful for understanding the model’s behavior across different thresholds, while AUC provides a summary measure of overall model performance.</p>  </li>  <li>    <p><em>Note : To understand what Threshold term means, refer to topic of Classification Metrics above this section.</em></p>  </li></ul>",
            "url": "http://localhost:4000/2024/01/14/ml-fine-print",
            
            
            
            "tags": ["Machine Learning","Deep Learning"],
            
            "date_published": "2024-01-14T00:00:00+05:30",
            "date_modified": "2024-01-14T00:00:00+05:30",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        }
    
    ]
}