<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Create LLM apps using RAG" /><meta property="og:locale" content="en_US" /><meta name="description" content="RAG and Langcahin for creating the personalized bots" /><meta property="og:description" content="RAG and Langcahin for creating the personalized bots" /><link rel="canonical" href="http://localhost:4000/2024/01/23/rag-application-with-langchain" /><meta property="og:url" content="http://localhost:4000/2024/01/23/rag-application-with-langchain" /><meta property="og:site_name" content="Blixxi Labs" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2024-01-23T00:00:00+05:30" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Create LLM apps using RAG" /><meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-01-23T00:00:00+05:30","datePublished":"2024-01-23T00:00:00+05:30","description":"RAG and Langcahin for creating the personalized bots","headline":"Create LLM apps using RAG","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2024/01/23/rag-application-with-langchain"},"url":"http://localhost:4000/2024/01/23/rag-application-with-langchain"}</script><title> Create LLM apps using RAG - Blixxi Labs</title><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Blixxi Labs" href="/atom.xml"><link rel="alternate" type="application/json" title="Blixxi Labs" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#f5f5f5}code{padding:.1rem;font-size:.85rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.post ol,.project ul,.post ol,.post ul{padding-left:1rem}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}</style></head><body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">Blixxi Labs</h1>--><nav role="navigation" aria-hidden="true"><ul><li><a href="/" >Home</a></li><li><a href="/about" >About</a></li></ul></nav></header><section class="post"><h2>Create LLM apps using RAG</h2><p>Last Updated: 30/01/2024 <br /> Status: Working in progress</p><p><em>If you’ve ever thought about creating a custom bot for your documents or website that interacts based on specific data, you’re in the right place. I’m here to assist you in developing a bot that leverages Langchain and RAG strategies for this purpose.</em></p><h3 id="understanding-the-limitations-of-chatgpt-and-llms">Understanding the Limitations of ChatGPT and LLMs</h3><p>ChatGPTs and other Large Language Models (LLMs) are extensively trained on text corpora to comprehend language semantics and coherence. Despite their impressive capabilities, these models have limitations that require careful consideration for particular use cases. One significant challenge is the potential for hallucinations, where the model might generate inaccurate or contextually irrelevant information.</p><p>Imagine requesting the model to enhance your company policies; in such scenarios, ChatGPTs and other Large Language Models might struggle to provide factual responses because they lack training on your company’s specific policies. Instead, they may generate nonsensical or irrelevant responses, which can be unhelpful. So, how can we ensure that an LLM comprehends our specific data and generates responses accordingly? This is where techniques like Retrieval Augmentation Generation (RAG) come to the rescue.</p><h3 id="what-is-rag">What is RAG?</h3><p>RAG or Retrieval Augmented Generation uses three main workflows to generate and give the better response</p><ul><li><p>Information Retrieval: When a user asks a question, the AI system retrieves the relevant data from a well-maintained knowledge library or external sources like databases, articles, APIs, or document repositories. This is achieved by converting the query into a numerical format or vector that can be understood by machines.</p></li><li><p>LLM: The retrieved data is then presented to the LLM or Large Language Model, along with the user’s query. The LLM uses this new knowledge and its training data to generate the response.</p></li><li><p>Response: Finally, the LLM generates a response that is more accurate and relevant since it has been augmented with the retrieved information. I mean we gave LLM some additional information from our Knowledge library which allows LLMs to provide more contextually relevant and factual responses, solving the problem of models when they are just hallucinating or providing irrelevant answer.</p></li></ul><p>Let’s take an example of company policies again. Suppose you have an HR bot that handles queries related to your Company policies. Now if someones asks anything specific to the policies, The bot can pull the most recent policy documents from the knowledge libray, pass the relevant context to a well crafted prompt which is then passed further to the LLM for generating the response.</p><p>To make it more easy, Imagine a LLM as your knowledgeable friend who seems to know everything, from Geography to Computer Science from Politics to Philosophy. Now, picture yourself asking this friend a few questions:</p><ul><li>“Who handles my laundry on weekends?”</li><li>“Who lives next door to me?”</li><li>“What brand of peanut butter do I prefer?”</li></ul><p>Chances are, your friend wouldn’t be able to answer these questions, right? Most of the time, no. But let’s say this distant friend becomes closer to you over time, he comes at your place regularly, know your parents very well, you both hangout pretty often, you go on outings, blah blah blah.. You got the point right? I mean he is gaining access to personal and insider information about you. Now, when you pose the same questions, they can somehow answer those question with more relevance now because because he is better suited with your personal insights.</p><p>Similarly, an LLM, when provided with additional information or access to your use case data, won’t guess or hallucinate. Instead, it will leverage that data to provide relevant and accurate answers.</p><h3 id="to-break-it-down-here-are-the-exact-steps-to-create-any-rag-application">To break it down, here are the exact steps to create any RAG application…</h3><ol><li>Extract the relevant information from your data sources.</li><li>Break the information into the small chunks.</li><li>Store the chunks as their embedddings into a vector database.</li><li>Create a prompt template which will be fed to the LLM with the query and the context.</li><li>Convert the query to it’s relevant embedding using same embedding model.</li><li>Fetch k number of relevant documents related to the query from the vector database.</li><li>Pass the relevant documents to the LLM and get the response.</li></ol><h3 id="faqs">FAQs</h3><ol><li><p>We will be using <u><a href="https://python.langchain.com/docs/get_started/introduction">Langchain</a></u> for this task, Basically it’s like a wrapper which lets you talk and manage to your LLM operations better.</p></li><li><p>Along with it we will be using <u><a href="https://huggingface.co/">Hugging Face</a></u>, it is like an open-source library for building, training, and deploying state-of-the-art machine learning models, especially about NLP. To use the HuggingFace we need the access token, Get your access token <u><a href="https://huggingface.co/docs/hub/security-tokens">here</a></u></p></li><li><p>For our models, we’ll need two key components: a LLM (Large Language Model) and an embedding model. While paid sources like OpenAI offer these, we’ll be utilizing open-source models to ensure accessibility for everyone.</p></li><li><p>To keep things simple, our data ingestion process will involve using a URL and some PDFs. While you can incorporate additional data sources if needed, we’ll concentrate solely on these two for now.</p></li></ol><p>With Langchain for the interface, Hugging Face for fetching the models, along with open-source components, we’re all set to go! This way, we save some bucks while still having everything we need. Let’s move to the next steps</p><h3 id="environment-setup">Environment Setup</h3><p>Open your favorite editor, create a python environment and install the relevant dependencies</p><pre><code class="language-python">python3 -m venv env
source env/bin/activate
pip3 install dotenv langchain langchain_community 
</code></pre><p>Now create a .env file in the same directory to place your Hugging face api credentials like this</p><pre><code class="language-python">HUGGINGFACEHUB_API_TOKEN = hf_KKNWfBqgwCUOHdHFrBwQ.....
</code></pre><p>Ensure the name “HUGGINGFACEHUB_API_TOKEN” remains unchanged, as it is crucial for authentication purposes.</p><p>Finally create a data folder in the project’s root directory, designated as the central repository for storing PDF documents. You can add some sample PDFs for testing purposes; for instance, I am using the <u><a href="https://arxiv.org/pdf/2207.02696.pdf">Yolo V7</a></u> and <u><a href="https://arxiv.org/abs/1706.03762">Transformers</a></u> paper for demonstration. It’s important to note that this designated folder will function as our primary source for data ingestion.</p><p>It seems like everything is in order, and we’re all set!</p><h3 id="step-1--extracting-the-relevant-information">Step 1 : Extracting the relevant information</h3><p>To get your RAG application running, the first thing we need to do is to extract the relevant information from the various data sources. It can be a website page, a PDF file, a notion link, a google doc whatever it is, it needs to be extracted from it’s original source first.</p><pre><code class="language-python">from langchain.document_loaders import PyPDFLoader
from langchain_community.document_loaders import WebBaseLoader

# Loading the environment variables
load_dotenv()
HF_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")

# Loading the web url and data 
url_loader = WebBaseLoader("https://docs.smith.langchain.com/overview")
documents_loader = DirectoryLoader('../data/', glob="./*.pdf", loader_cls=PyPDFLoader)

# Creating the instances
url_docs = url_loader.load()
data_docs = documents_loader.load()

# Combining all the data that we ingested
docs = url_docs + data_docs
</code></pre><p>This will ingest all the data from the URL link and the PDFs.</p><h3 id="step-2--breaking-the-information-into-smaller-chunks">Step 2 : Breaking the information into smaller chunks</h3><p>We’ve all the necessary data for developing our RAG application. Now, it’s time to break down this information into smaller chunks. Later, we’ll utilize an embedding model to convert these chunks into their respective embeddings. But why it’s important?</p><p>Think of it like this: If you’re tasked with digesting a 100-page book all at once and then asked a specific question about it, it would be challenging to retrieve the necessary information from the entire book to provide an answer. However, if you’re permitted to break the book into smaller, manageable chunks—let’s say 10 pages each—and each chunk is labeled with an index or numbering from 0 to 9, the process becomes much simpler. When the same question is posed after this breakdown, you can easily locate the relevant chunk based on its index and then extract the information needed to answer the question accurately.</p><p>Picture the book as your extracted information, with each 10-page segment representing a small chunk of data, and the index as the embedding. Essentially, we’ll apply an embedding model to these chunks to transform the information into their respective embeddings. While as humans, we may not directly comprehend or relate to these embeddings, they serve as numeric representations of the chunks to our application. This is how you can do this in Python</p><pre><code class="language-python">from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)
documents = text_splitter.split_documents(docs)
</code></pre><p>Now the chunk_size parameter specifies the maximum number of characters that a chunk can contain, while the chunk_overlap parameter specifies the number of characters that should overlap between two adjacent chunks. With the chunk_overlap set to 50, the last 50 characters of the adjacent chunks will be shared between each other. This approach helps to prevent important information from being split across two chunks, ensuring that each chunk contains sufficient contextual information for the subsequent processing or analysis. As the shared information at the boundary of neighboring chunks enables a more seamless transition and understanding of the text’s content. The best strategy for choosing the chunk_size and chunk_overlap parameters largely depends on the nature of the documents and the purpose of the application.</p><h3 id="step-3--creating-the-embeddings-and-store-them-into-a-vectordatabase">Step 3 : Creating the embeddings and store them into a vectordatabase</h3><p>We have two ways to get embeddings from these chunks. First, we can download a model, handle preprocessing, and do computations on our own. Or, we can use Hugging Face’s model hub. They’ve got lots of pre-trained models for different NLP tasks, including embeddings. With this approach, we’ll use one of their embedding models. We’ll just give our chunks to this model, and Hugging Face’s servers will do the hard work like preprocessing and computing. This saves us from doing it all on our own machines.</p><p>We have a bunch of options for embedding models, and you can check out the leaderboard <a href="https://huggingface.co/spaces/mteb/leaderboard">here</a>. But for now, we’ll go with “bge-base-en-v1.5”. It’s great for making embeddings for English text, plus it’s smaller, so it loads quickly and gets the job done fast.</p><pre><code class="language-python">from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings

# Creating the embeddings object
embeddings = HuggingFaceInferenceAPIEmbeddings(
    api_key=HF_TOKEN, model_name="BAAI/bge-base-en-v1.5"
)
</code></pre><p>Here’s a way to see the number of embeddings for each chunk</p><pre><code class="language-python">query = "Hello I want to see the length of the embeddings for this document."
embeddings.embed_documents([query])[0]

# 768
</code></pre><p>When it comes to vector databases, there are plenty of options out there. Some, like Pinecone, are paid but offer fast performance and extra features compared to open-source alternatives like FAISS or Chroma. However, if you don’t need to scale your database to handle hundreds of users fetching and storing data every minute, open-source options are more than sufficient. They work really well. So, what we’ll do is create an instance of FAISS vector database and store our embeddings in it. It’s straightforward and doesn’t require any rocket science.</p><pre><code class="language-python">from langchain_community.vectorstores import FAISS

# Creating a vectorstore object
vectorstore = FAISS.from_documents(documents, embeddings)
</code></pre><h3 id="step-4--create-a-prompt-template-which-will-be-fed-to-the-llm">Step 4 : Create a prompt template which will be fed to the LLM</h3><p>Ok now comes the prompt template. So when you write a question to the ChatGPT and it answers that question, you are basically providing a prompt to the model so that it can understand what’s the question is. When companies train the models, they decide what kind of prompt they are going to use for invoking the model and ask the question. So for example,if you are working with “Mistral 7B instruct” and you want the optimal results it’s recommended to use the following chat template:</p><pre><code class="language-python">&lt;s&gt;[INST] Instruction [/INST] Model answer&lt;/s&gt;[INST] Follow-up instruction [/INST]
</code></pre><p>Note that <s> and </s> are special tokens for beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings. It’s just that the Mistral 7B instruct is made in such a way that the model look for those special tokens to understand the question better. Different types of LLMs have different kinds of instructed prompts.</p><p>Now for our case we are going to use “huggingfaceh4/zephyr-7b-alpha” which is a text generation model. Just to make it clear, Zephyr-7B-α has not been aligned or formated to human preferences with techniques like RLHF (Reinforcement Learning with Human Feedback) or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so). Instead of writing a Prompt on our own,</p><h3 id="step-5--convert-the-query-to-its-relevant-embedding-using-same-embedding-model">Step 5 : Convert the query to it’s relevant embedding using same embedding model.</h3><p>Ok so far we have converted our textual information into chunks, then we embedded those chunks into there specific embeddings and further more we have stored our embedded chunks into the vector database. Now comes the query or the question that we want to ask to our RAG application. We just can’t pass the query to our model and ask for the information, instead the query will be passed to the same embedding model that is being used for chunks earlier and then only we can do the next set of tasks. Why it’s important? Wel by embedding queries, we enable models to efficiently compare them with previously processed chunks of text, as then only we can do tasks like finding similar documents or generating relevant responses. It’s like translating language into a language computers understand. Imagine you are English Speaker and your friend is a Hindi Speaker, no one understands each other language. Now you give a 1 page document written in Hindi to your English Speaker friend, now your friend will convert that document into the Hindi one first and then will understand the information.</p><p>Now you asks a question to your english speaking friend in Hindi which is related to that documet. Now what would happen is, your friend will convert that question into the english first and then only will be able to understand the question and find relevant response from that information which you have shared earlier.</p><p>Ok so your friend is an embedding model which converted your previous texts into the embeddings. Now when you asks a query or a question, it will be converted into the relevant embeddings from the same embedding model that is being used ealier for chunks and then some search operation will be performed to find the relevant response to your query.</p><p>I hope you get it why we did convert our query into the embeddings first.</p><h3 id="step-6--fetch-k-number-of-documents">Step 6 : Fetch K number of documents.</h3><p>Now comes the retriver. What it does is, it goes into the vector database, performs <em>somekind of search</em> to find the relevant documents (chunks) and return k number of relevant documents which are ranked accordingly based on which one is more contextually related to the query or the question that you asked. We can set k as a parameter which means if you want 2 relevant documents or 5 or 10. Genearlly, if you have less amount of data, it’s advisable to keep k to 2 and for longer documents, it’s recommended to keep k in bewteen 10 - 20</p><h3 id="step-7--pass-the-relevant-documents-to-the-llm-and-get-the-response">Step 7 : Pass the relevant documents to the LLM and get the response.</h3><p>Now comes the last step. So to make things more robust, we will ask our retriever to fetch the k number of relevant documents from the database. We will then pass those k number of documents to our LLM as the context and will ask the LLM to generate a relevant response from that context. That’s it.</p><h3 id="whats-next">What’s next?</h3><p>So there are so many things which can be tweaked here. We can change our embedding model for something more better and better in terms of indexing, searching technique for retriever can be changed too, as well as a better LLM to generate the relevant response.</p><span class="meta"><time datetime="2024-01-23T00:00:00+05:30">January 23, 2024</time></section><!-- --- layout: default ---<section class="post"><h2>Create LLM apps using RAG</h2><p>Last Updated: 30/01/2024 <br /> Status: Working in progress</p><p><em>If you’ve ever thought about creating a custom bot for your documents or website that interacts based on specific data, you’re in the right place. I’m here to assist you in developing a bot that leverages Langchain and RAG strategies for this purpose.</em></p><h3 id="understanding-the-limitations-of-chatgpt-and-llms">Understanding the Limitations of ChatGPT and LLMs</h3><p>ChatGPTs and other Large Language Models (LLMs) are extensively trained on text corpora to comprehend language semantics and coherence. Despite their impressive capabilities, these models have limitations that require careful consideration for particular use cases. One significant challenge is the potential for hallucinations, where the model might generate inaccurate or contextually irrelevant information.</p><p>Imagine requesting the model to enhance your company policies; in such scenarios, ChatGPTs and other Large Language Models might struggle to provide factual responses because they lack training on your company’s specific policies. Instead, they may generate nonsensical or irrelevant responses, which can be unhelpful. So, how can we ensure that an LLM comprehends our specific data and generates responses accordingly? This is where techniques like Retrieval Augmentation Generation (RAG) come to the rescue.</p><h3 id="what-is-rag">What is RAG?</h3><p>RAG or Retrieval Augmented Generation uses three main workflows to generate and give the better response</p><ul><li><p>Information Retrieval: When a user asks a question, the AI system retrieves the relevant data from a well-maintained knowledge library or external sources like databases, articles, APIs, or document repositories. This is achieved by converting the query into a numerical format or vector that can be understood by machines.</p></li><li><p>LLM: The retrieved data is then presented to the LLM or Large Language Model, along with the user’s query. The LLM uses this new knowledge and its training data to generate the response.</p></li><li><p>Response: Finally, the LLM generates a response that is more accurate and relevant since it has been augmented with the retrieved information. I mean we gave LLM some additional information from our Knowledge library which allows LLMs to provide more contextually relevant and factual responses, solving the problem of models when they are just hallucinating or providing irrelevant answer.</p></li></ul><p>Let’s take an example of company policies again. Suppose you have an HR bot that handles queries related to your Company policies. Now if someones asks anything specific to the policies, The bot can pull the most recent policy documents from the knowledge libray, pass the relevant context to a well crafted prompt which is then passed further to the LLM for generating the response.</p><p>To make it more easy, Imagine a LLM as your knowledgeable friend who seems to know everything, from Geography to Computer Science from Politics to Philosophy. Now, picture yourself asking this friend a few questions:</p><ul><li>“Who handles my laundry on weekends?”</li><li>“Who lives next door to me?”</li><li>“What brand of peanut butter do I prefer?”</li></ul><p>Chances are, your friend wouldn’t be able to answer these questions, right? Most of the time, no. But let’s say this distant friend becomes closer to you over time, he comes at your place regularly, know your parents very well, you both hangout pretty often, you go on outings, blah blah blah.. You got the point right? I mean he is gaining access to personal and insider information about you. Now, when you pose the same questions, they can somehow answer those question with more relevance now because because he is better suited with your personal insights.</p><p>Similarly, an LLM, when provided with additional information or access to your use case data, won’t guess or hallucinate. Instead, it will leverage that data to provide relevant and accurate answers.</p><h3 id="to-break-it-down-here-are-the-exact-steps-to-create-any-rag-application">To break it down, here are the exact steps to create any RAG application…</h3><ol><li>Extract the relevant information from your data sources.</li><li>Break the information into the small chunks.</li><li>Store the chunks as their embedddings into a vector database.</li><li>Create a prompt template which will be fed to the LLM with the query and the context.</li><li>Convert the query to it’s relevant embedding using same embedding model.</li><li>Fetch k number of relevant documents related to the query from the vector database.</li><li>Pass the relevant documents to the LLM and get the response.</li></ol><h3 id="faqs">FAQs</h3><ol><li><p>We will be using <u><a href="https://python.langchain.com/docs/get_started/introduction">Langchain</a></u> for this task, Basically it’s like a wrapper which lets you talk and manage to your LLM operations better.</p></li><li><p>Along with it we will be using <u><a href="https://huggingface.co/">Hugging Face</a></u>, it is like an open-source library for building, training, and deploying state-of-the-art machine learning models, especially about NLP. To use the HuggingFace we need the access token, Get your access token <u><a href="https://huggingface.co/docs/hub/security-tokens">here</a></u></p></li><li><p>For our models, we’ll need two key components: a LLM (Large Language Model) and an embedding model. While paid sources like OpenAI offer these, we’ll be utilizing open-source models to ensure accessibility for everyone.</p></li><li><p>To keep things simple, our data ingestion process will involve using a URL and some PDFs. While you can incorporate additional data sources if needed, we’ll concentrate solely on these two for now.</p></li></ol><p>With Langchain for the interface, Hugging Face for fetching the models, along with open-source components, we’re all set to go! This way, we save some bucks while still having everything we need. Let’s move to the next steps</p><h3 id="environment-setup">Environment Setup</h3><p>Open your favorite editor, create a python environment and install the relevant dependencies</p><pre><code class="language-python">python3 -m venv env
source env/bin/activate
pip3 install dotenv langchain langchain_community 
</code></pre><p>Now create a .env file in the same directory to place your Hugging face api credentials like this</p><pre><code class="language-python">HUGGINGFACEHUB_API_TOKEN = hf_KKNWfBqgwCUOHdHFrBwQ.....
</code></pre><p>Ensure the name “HUGGINGFACEHUB_API_TOKEN” remains unchanged, as it is crucial for authentication purposes.</p><p>Finally create a data folder in the project’s root directory, designated as the central repository for storing PDF documents. You can add some sample PDFs for testing purposes; for instance, I am using the <u><a href="https://arxiv.org/pdf/2207.02696.pdf">Yolo V7</a></u> and <u><a href="https://arxiv.org/abs/1706.03762">Transformers</a></u> paper for demonstration. It’s important to note that this designated folder will function as our primary source for data ingestion.</p><p>It seems like everything is in order, and we’re all set!</p><h3 id="step-1--extracting-the-relevant-information">Step 1 : Extracting the relevant information</h3><p>To get your RAG application running, the first thing we need to do is to extract the relevant information from the various data sources. It can be a website page, a PDF file, a notion link, a google doc whatever it is, it needs to be extracted from it’s original source first.</p><pre><code class="language-python">from langchain.document_loaders import PyPDFLoader
from langchain_community.document_loaders import WebBaseLoader

# Loading the environment variables
load_dotenv()
HF_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")

# Loading the web url and data 
url_loader = WebBaseLoader("https://docs.smith.langchain.com/overview")
documents_loader = DirectoryLoader('../data/', glob="./*.pdf", loader_cls=PyPDFLoader)

# Creating the instances
url_docs = url_loader.load()
data_docs = documents_loader.load()

# Combining all the data that we ingested
docs = url_docs + data_docs
</code></pre><p>This will ingest all the data from the URL link and the PDFs.</p><h3 id="step-2--breaking-the-information-into-smaller-chunks">Step 2 : Breaking the information into smaller chunks</h3><p>We’ve all the necessary data for developing our RAG application. Now, it’s time to break down this information into smaller chunks. Later, we’ll utilize an embedding model to convert these chunks into their respective embeddings. But why it’s important?</p><p>Think of it like this: If you’re tasked with digesting a 100-page book all at once and then asked a specific question about it, it would be challenging to retrieve the necessary information from the entire book to provide an answer. However, if you’re permitted to break the book into smaller, manageable chunks—let’s say 10 pages each—and each chunk is labeled with an index or numbering from 0 to 9, the process becomes much simpler. When the same question is posed after this breakdown, you can easily locate the relevant chunk based on its index and then extract the information needed to answer the question accurately.</p><p>Picture the book as your extracted information, with each 10-page segment representing a small chunk of data, and the index as the embedding. Essentially, we’ll apply an embedding model to these chunks to transform the information into their respective embeddings. While as humans, we may not directly comprehend or relate to these embeddings, they serve as numeric representations of the chunks to our application. This is how you can do this in Python</p><pre><code class="language-python">from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 50)
documents = text_splitter.split_documents(docs)
</code></pre><p>Now the chunk_size parameter specifies the maximum number of characters that a chunk can contain, while the chunk_overlap parameter specifies the number of characters that should overlap between two adjacent chunks. With the chunk_overlap set to 50, the last 50 characters of the adjacent chunks will be shared between each other. This approach helps to prevent important information from being split across two chunks, ensuring that each chunk contains sufficient contextual information for the subsequent processing or analysis. As the shared information at the boundary of neighboring chunks enables a more seamless transition and understanding of the text’s content. The best strategy for choosing the chunk_size and chunk_overlap parameters largely depends on the nature of the documents and the purpose of the application.</p><h3 id="step-3--creating-the-embeddings-and-store-them-into-a-vectordatabase">Step 3 : Creating the embeddings and store them into a vectordatabase</h3><p>We have two ways to get embeddings from these chunks. First, we can download a model, handle preprocessing, and do computations on our own. Or, we can use Hugging Face’s model hub. They’ve got lots of pre-trained models for different NLP tasks, including embeddings. With this approach, we’ll use one of their embedding models. We’ll just give our chunks to this model, and Hugging Face’s servers will do the hard work like preprocessing and computing. This saves us from doing it all on our own machines.</p><p>We have a bunch of options for embedding models, and you can check out the leaderboard <a href="https://huggingface.co/spaces/mteb/leaderboard">here</a>. But for now, we’ll go with “bge-base-en-v1.5”. It’s great for making embeddings for English text, plus it’s smaller, so it loads quickly and gets the job done fast.</p><pre><code class="language-python">from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings

# Creating the embeddings object
embeddings = HuggingFaceInferenceAPIEmbeddings(
    api_key=HF_TOKEN, model_name="BAAI/bge-base-en-v1.5"
)
</code></pre><p>Here’s a way to see the number of embeddings for each chunk</p><pre><code class="language-python">query = "Hello I want to see the length of the embeddings for this document."
embeddings.embed_documents([query])[0]

# 768
</code></pre><p>When it comes to vector databases, there are plenty of options out there. Some, like Pinecone, are paid but offer fast performance and extra features compared to open-source alternatives like FAISS or Chroma. However, if you don’t need to scale your database to handle hundreds of users fetching and storing data every minute, open-source options are more than sufficient. They work really well. So, what we’ll do is create an instance of FAISS vector database and store our embeddings in it. It’s straightforward and doesn’t require any rocket science.</p><pre><code class="language-python">from langchain_community.vectorstores import FAISS

# Creating a vectorstore object
vectorstore = FAISS.from_documents(documents, embeddings)
</code></pre><h3 id="step-4--create-a-prompt-template-which-will-be-fed-to-the-llm">Step 4 : Create a prompt template which will be fed to the LLM</h3><p>Ok now comes the prompt template. So when you write a question to the ChatGPT and it answers that question, you are basically providing a prompt to the model so that it can understand what’s the question is. When companies train the models, they decide what kind of prompt they are going to use for invoking the model and ask the question. So for example,if you are working with “Mistral 7B instruct” and you want the optimal results it’s recommended to use the following chat template:</p><pre><code class="language-python">&lt;s&gt;[INST] Instruction [/INST] Model answer&lt;/s&gt;[INST] Follow-up instruction [/INST]
</code></pre><p>Note that <s> and </s> are special tokens for beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings. It’s just that the Mistral 7B instruct is made in such a way that the model look for those special tokens to understand the question better. Different types of LLMs have different kinds of instructed prompts.</p><p>Now for our case we are going to use “huggingfaceh4/zephyr-7b-alpha” which is a text generation model. Just to make it clear, Zephyr-7B-α has not been aligned or formated to human preferences with techniques like RLHF (Reinforcement Learning with Human Feedback) or deployed with in-the-loop filtering of responses like ChatGPT, so the model can produce problematic outputs (especially when prompted to do so). Instead of writing a Prompt on our own,</p><h3 id="step-5--convert-the-query-to-its-relevant-embedding-using-same-embedding-model">Step 5 : Convert the query to it’s relevant embedding using same embedding model.</h3><p>Ok so far we have converted our textual information into chunks, then we embedded those chunks into there specific embeddings and further more we have stored our embedded chunks into the vector database. Now comes the query or the question that we want to ask to our RAG application. We just can’t pass the query to our model and ask for the information, instead the query will be passed to the same embedding model that is being used for chunks earlier and then only we can do the next set of tasks. Why it’s important? Wel by embedding queries, we enable models to efficiently compare them with previously processed chunks of text, as then only we can do tasks like finding similar documents or generating relevant responses. It’s like translating language into a language computers understand. Imagine you are English Speaker and your friend is a Hindi Speaker, no one understands each other language. Now you give a 1 page document written in Hindi to your English Speaker friend, now your friend will convert that document into the Hindi one first and then will understand the information.</p><p>Now you asks a question to your english speaking friend in Hindi which is related to that documet. Now what would happen is, your friend will convert that question into the english first and then only will be able to understand the question and find relevant response from that information which you have shared earlier.</p><p>Ok so your friend is an embedding model which converted your previous texts into the embeddings. Now when you asks a query or a question, it will be converted into the relevant embeddings from the same embedding model that is being used ealier for chunks and then some search operation will be performed to find the relevant response to your query.</p><p>I hope you get it why we did convert our query into the embeddings first.</p><h3 id="step-6--fetch-k-number-of-documents">Step 6 : Fetch K number of documents.</h3><p>Now comes the retriver. What it does is, it goes into the vector database, performs <em>somekind of search</em> to find the relevant documents (chunks) and return k number of relevant documents which are ranked accordingly based on which one is more contextually related to the query or the question that you asked. We can set k as a parameter which means if you want 2 relevant documents or 5 or 10. Genearlly, if you have less amount of data, it’s advisable to keep k to 2 and for longer documents, it’s recommended to keep k in bewteen 10 - 20</p><h3 id="step-7--pass-the-relevant-documents-to-the-llm-and-get-the-response">Step 7 : Pass the relevant documents to the LLM and get the response.</h3><p>Now comes the last step. So to make things more robust, we will ask our retriever to fetch the k number of relevant documents from the database. We will then pass those k number of documents to our LLM as the context and will ask the LLM to generate a relevant response from that context. That’s it.</p><h3 id="whats-next">What’s next?</h3><p>So there are so many things which can be tweaked here. We can change our embedding model for something more better and better in terms of indexing, searching technique for retriever can be changed too, as well as a better LLM to generate the relevant response.</p><span class="meta"><time datetime="2024-01-23T00:00:00+05:30">January 23, 2024</time> &middot; <a href="/tag/LLM">LLM</a>, <a href="/tag/Deep Learning">Deep Learning</a></span>--> <!--</section>--></main></body></html>
