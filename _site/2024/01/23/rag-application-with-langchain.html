<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Create LLM apps using RAG" /><meta property="og:locale" content="en_US" /><meta name="description" content="RAG and Langcahin for creating the personalized bots" /><meta property="og:description" content="RAG and Langcahin for creating the personalized bots" /><link rel="canonical" href="http://localhost:4000/2024/01/23/rag-application-with-langchain" /><meta property="og:url" content="http://localhost:4000/2024/01/23/rag-application-with-langchain" /><meta property="og:site_name" content="Blixxi Labs" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2024-01-23T00:00:00+05:30" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Create LLM apps using RAG" /><meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-01-23T00:00:00+05:30","datePublished":"2024-01-23T00:00:00+05:30","description":"RAG and Langcahin for creating the personalized bots","headline":"Create LLM apps using RAG","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2024/01/23/rag-application-with-langchain"},"url":"http://localhost:4000/2024/01/23/rag-application-with-langchain"}</script><title> Create LLM apps using RAG - Blixxi Labs</title><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Blixxi Labs" href="/atom.xml"><link rel="alternate" type="application/json" title="Blixxi Labs" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#fff}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.post ol,.project ul,.post ol,.post ul{padding-left:1rem}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}</style></head><body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">Blixxi Labs</h1>--><nav role="navigation" aria-hidden="true"><ul><li><a href="/" >Home</a></li><li><a href="/about" >About</a></li></ul></nav></header><section class="post"><h2>Create LLM apps using RAG</h2><p>Last updated : 25/01/2024 <br /> Status: In progress</p><p><em>If you’ve ever thought about creating a custom bot for your documents or website that interacts based on specific data, you’re in the right place. I’m here to assist you in developing a bot that leverages Langchain and RAG strategies for this purpose.</em></p><h3 id="limitation-of-chatgpt-and-llms">Limitation of ChatGPT and LLMs</h3><p>ChatGPTs and other Large Language Models (LLMs) undergo extensive training on corpora to understand language semantics and coherence. Despite their remarkable utility, these models have limitations that need consideration for specific use cases. One notable challenge is the potential for hallucinations, where the model may generate inaccurate or contextually irrelevant information. Imagine asking the model to improve your company policies; in such cases, ChatGPTs and other Large Language Models might struggle to provide a factual response because they aren’t trained on your company’s specific policies. Instead, they may produce nonsensical or irrelevant responses, which may not be helpful.</p><p>So, how can we make an LLM understand our specific data and generate responses accordingly? This is where techniques like Retrieval Augmentation Generation (RAG) come to the rescue.</p><h3 id="whats-rag-really">What’s RAG really?</h3><p>In a nutshell, RAG combines two key functionalities: retrieving information from a vast database and then generating responses using natural language processing. The process is akin to this: first, the relevant context is retrieved from a vector database and next, the retrieved information is used to create a specific prompt that is fed to the LLM (large language model) which then generates a response.</p><p>Let’s take an example of company policies again. Suppose you have an HR bot that handles queries related to your Company policies. Now if someones asks anything specific to the policies, The bot can pull the most recent policy documents and summarize the relevant points in a conversational manner. This not only improves the quality of responses but also ensures that the information provided is up-to-date and in line with current company policies.</p><h3 id="to-break-it-down-here-are-the-steps-to-create-any-rag-application">To break it down, here are the steps to create any RAG application:</h3><ol><li>Extract the relevant information from your data sources.</li><li>Break the information into the small chunks</li><li>Store the chunks as their embedddings into a vector database</li><li>Create a prompt template which will be fed to the LLM</li><li>Convert the query to it’s relevant embedding using same embedding model.</li><li>Fetch k number of relevant documents related to the query from the vector database</li><li>Pass the relevant documents to the LLM with the specific prompt</li><li>Get the response.</li></ol><p>We will do all this with LangChain, a framework for interfacing with LLMs to create chains of operations and autonomous agents. Basically it’s like a wrapper which lets you talk and manage your LLM responses better. Let’s get started then..</p><h3 id="extracting-the-relevant-information">Extracting the relevant information</h3><p>To get your RAG application running, the first thing we need to do is to extract the relevant information from the various data sources. It can be a website page, a PDF file, a notion link, whatever it is, it needs to be extracted from it’s original source which we will use later on as a reference for our LLM responses. For the sake of simplicity, we will start with a website link and a PDF file.</p><span class="meta"><time datetime="2024-01-23T00:00:00+05:30">January 23, 2024</time> &middot; <a href="/tag/LLM">LLM</a>, <a href="/tag/Deep Learning">Deep Learning</a></span></section></main></body></html>
