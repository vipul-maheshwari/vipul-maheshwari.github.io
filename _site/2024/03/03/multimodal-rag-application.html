<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Multimodal RAG applications" /><meta property="og:locale" content="en_US" /><meta name="description" content="Multimodal RAG applications using lanceDB" /><meta property="og:description" content="Multimodal RAG applications using lanceDB" /><link rel="canonical" href="http://localhost:4000/2024/03/03/multimodal-rag-application" /><meta property="og:url" content="http://localhost:4000/2024/03/03/multimodal-rag-application" /><meta property="og:site_name" content="Blixxi Labs" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2024-03-03T00:00:00+05:30" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Multimodal RAG applications" /><meta name="twitter:site" content="@PinakaX" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-03-03T00:00:00+05:30","datePublished":"2024-03-03T00:00:00+05:30","description":"Multimodal RAG applications using lanceDB","headline":"Multimodal RAG applications","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2024/03/03/multimodal-rag-application"},"url":"http://localhost:4000/2024/03/03/multimodal-rag-application"}</script><title> Multimodal RAG applications - Blixxi Labs</title><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Blixxi Labs" href="/atom.xml"><link rel="alternate" type="application/json" title="Blixxi Labs" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#f5f5f5;max-width:100%;overflow-x:auto}code{padding:.1rem;font-size:.85rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%;background:none;display:block;margin:auto}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.post ol,.project ul,.post ol,.post ul{padding-left:1rem}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}.back-to-top{position:fixed;bottom:20px;right:20px;background-color:#fff;color:#000;padding:10px;text-decoration:none;border-radius:5px;display:block;margin-right:20%}</style></head><body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">Blixxi Labs</h1>--><nav role="navigation" aria-hidden="true"><ul><li><a href="/" >Home</a></li><li><a href="/about" >About</a></li><li><a href="/contact" >Contact</a></li></ul></nav></header><section class="post"><h1>Multimodal RAG applications</h1><p><em>Artificial Intelligence (AI) has been actively working with text for quite some time, but the world isn‚Äôt solely centered around words. If you take a moment to look around, you‚Äôll find a mix of text, images, videos, audios, and their combinations.</em></p><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/Renevant%20Cheetah-66.jpg?raw=true" alt="boomer_ai" /></p><p>Today we are going to work on Multimodality which is basically a concept that essentially empowers AI models with the capacity to perceive, listen, and comprehend data in diverse formats together with the text. Pretty much like how we do!</p><p>In an ideal situation, we should be able to mix different types of data together and show them to a generative AI model at the same time and iterate on it. It could be as simple as telling the AI model, ‚ÄúHey, a few days ago, I sent you a picture of a brown, short dog. Can you find that picture for me?‚Äù and the model should then give us the details of that picture. Basically, we want the AI to understand the things more like how we humans do, becoming really good at handling and responding to all kinds of information.</p><p>But the challenge here is to make a computer understands one data format with it‚Äôs related reference, and that could be a mix of text, audio, thermal imagery, and videos. Now to make this happen, we use something called Embeddings. It‚Äôs really a numeric vector which contains a bunch of numbers written together that might not mean much to us but understood by machines very well.</p><h3 id="cat-is-equal-to-cat">Cat is equal to Cat</h3><p>Let‚Äôs think of the text components for now, so we are currently aiming that our model should learn that words like ‚ÄúDog‚Äù and ‚ÄúCat‚Äù are closely linked to the word ‚ÄúPet.‚Äù Now this understanding is easily achievable by using an embedding model which will convert these text words into their respective embeddings first and then the model is trained to follow a straightforward logic: if words are related, they are close together in the vector space, if not, they would be separated by the adequate distance.</p><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/embeddings.png?raw=true" alt="embeddings" /></p><p>But to help a model recognize that an image of a ‚ÄúCat‚Äù and the word ‚ÄúCat‚Äù are similar, we rely on Multimodal Embeddings. To simplify things a bit, imagine there is a magic box which is capable of handling various inputs ‚Äì images, audios, text, and more.</p><p>Now, when we feed the box with an image of a ‚ÄúCat‚Äù with the text ‚ÄúCat,‚Äù it performs its magic and produces two numeric vectors. When these two vectors were given to a machines, it made machines to think, ‚ÄúHmm, based on these numeric values, it seems like both are connected to ‚ÄúCat‚Äù. So that‚Äôs exactly what we were aiming for! Our goal was to help machines to recognize the close connection between an image of a ‚ÄúCat‚Äù and the text ‚ÄúCat‚Äù. However, to validate this concept, when we plot those two numeric vectors in a vector space, it turns out they are very close to each other. This outcome exactly mirrors what we observed earlier with the proximity of the two text words ‚ÄúCat‚Äù and ‚ÄúDog‚Äù in the vector space.</p><h2 id="ladies-and-gentlemen-thats-the-essence-of-multimodality-">Ladies and gentlemen, that‚Äôs the essence of Multimodality. üëè</h2><p>So we made our model to comprehend the association between ‚ÄúCat‚Äù images and the word ‚ÄúCat.‚Äù Well this is it, I mean if you are able to do this, you would have ingested the audio, images, videos as well as the word ‚ÄúCat‚Äù and the model will understand how the cat is being portrayed across all kinds of file format..</p><h3 id="rag-is-here">RAG is here..</h3><p>Well if you don‚Äôt know what RAG means, I would highly advise to read this article <a href="https://vipul-maheshwari.github.io/2024/02/14/rag-application-with-langchain">here</a> which I wrote some days back and loved by ton of people, not exageerating it but yeah, it‚Äôs good to get the balls rolling..</p><p>So there are impressive models like DALLE-2 that provide text-to-image functionality. Essentially, you input text, and the model generates relevant images for you. But can we create a system similar to Multimodal RAG, where the model produces output images based on our own data? Alright, so the goal for today is to create an AI model that when asked something like, ‚ÄúHow many girls were there in my party?‚Äù üíÄ not only provides textual information but also includes a relevant image related to it. Think of it as an extension of simple RAG system, but now incorporating images.</p><p>Before we dive in, remember that Multimodality isn‚Äôt limited to just text-to-image or image-to-text as it encompasses the freedom to input and output any type of data. However, for now, let‚Äôs concentrate on the interaction from image to text exclusively.</p><h3 id="contrasive-learning">Contrasive learning</h3><p>Now the question is, What exactly was that box doing? The magic it performs is known as Contrastive Learning. While the term might sound complex, it‚Äôs not that tricky. To simplify, consider a dataset with images, along with a caption describing what the image represents.</p><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/clipmodel.png?raw=true" alt="clipmodel" /></p><p>Alright, now what happens is: we give our text-image model with these Positive and Negative samples, where each sample consists of an image and a descriptive text. Positive samples are those where the image and text are correctly align ‚Äì for instance, a picture of a cat matched with the text ‚Äúthis is an image of a cat.‚Äù Conversely, negative samples involve a mismatch, like presenting an image of a dog alongside the text ‚Äúthis is an image of a cat.‚Äù</p><p>Now we train our text-image model to recognize that positive samples offer accurate interpretations, while negative samples are misleading and should be disregarded during training. In formal terms this technique is called ‚ÄúCLIP‚Äù (Contrastive Language-Image Pre-training) introduced by OpenAI where authors trained an image-text model on something around 400 million image caption pairs taken from the interent and everytime model makes a mistake, the contrasive loss function increases and penalize it to make sure the model trains well. The same kind of principles are applied to the other modality combinations as well, so voice of cat with the word cat is positive sample for speech-text model, a video of cat with the descriptive text ‚Äúthis is a cat‚Äù is a positive sample for video-text model.</p><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/easy.png?raw=true" alt="easy" /></p><h3 id="show-time">Show time</h3><p>Well you don‚Äôt have to build that box from scratch because folks have already done it for us. There‚Äôs a Multimodal embedding model, like the ‚ÄúViT-L/14‚Äù from OpenAI. This model can handle various data types, including text, images, videos, audios, and even thermal and gyroscope data. Now, onto the next question: how do we store those embeddings?</p><p>For that we‚Äôll need a vector database that can efficiently fetch, query, and retrieve relevant embeddings for us, ideally one that supports multimodal data and doesn‚Äôt burn a hole in our wallets. That‚Äôs where LanceDB comes into play.</p><h3 id="vector-database">Vector database</h3><p>When we talk about the vector database, there are ton of options available in the current market, but there is something about the LanceDB which makes it stands out as an optimal choice for a vector database, As far as I have used it, it address the limitations of traditional embedded databases in handling AI/ML workloads. When I say traditional, it typically means those databases management tools which are not aligned with the usage of heavy compuatation that comes with the ML infra.</p><p>TLDR; LanceDB operates on a serverless architecture, meaning storage and compute are separated into two distinct units. This design makes it exceptionally fast for RAG use cases, ensuring fast fetching and retrieval. Additionally, it has some notable advantages ‚Äì being open source, utilizing its Lance columnar data format built on top of Apache Arrow for high efficiency, persistent storage capabilities, and incorporating its own Disk Approximate Nearest Neighbor search. All these factors collectively make LanceDB an ideal solution for accessing and working with multimodal data. I love you LanceDB ‚ù§Ô∏è.</p><h3 id="data-time">Data time</h3><p>To add some excitement, I‚Äôve crafted a GTA-V Image Captioning dataset, featuring thousands of images, each paired with a descriptive text illustrating the image‚Äôs content. Now, when we train our magic box, the expectation is clear ‚Äì if I ask that box to provide me an image of ‚Äúroad with a stop sign,‚Äù it should deliver a GTA-V image of a road with a stop sign on it. Otherwise, what‚Äôs the point, right?</p><h3 id="faq">FAQ</h3><ol><li>We will be using ‚ÄúViT-L/14‚Äù to convert our multimodal data into it‚Äôs respective embeddings.</li><li>LanceDB as our vector database to store the relevant embeddings.</li><li>GTA-V Image Captioning dataset for our magic box.</li></ol><h3 id="environment-setup">Environment Setup</h3><p>I am using a MacBook Air M1, and it‚Äôs important to note that some kind of dependencies and configurations may vary depending on the type of system that you are running, so it‚Äôs important to take that into account.</p><p>Here are the steps to install the relevant dependencies</p><pre><code class="language-python"># Create a virtual environment
python3 -m venv env

# Activate the virtual environment
source env/bin/activate

# Upgrade pip in the virtual environment
pip install --upgrade pip

# Install required dependencies
pip3 install lancedb clip torch datasets pillow 
pip3 install git+https://github.com/openai/CLIP.git
</code></pre><p>And don‚Äôt forget to get your access token from the hugging face to download the data.</p><h3 id="downloading-the-data">Downloading the Data</h3><p>Dataset can easily be fetched using the datasets library.</p><pre><code class="language-python">import clip
import torch
import os
from datasets import load_dataset

ds = load_dataset("vipulmaheshwari/GTA-Image-Captioning-Dataset")
device = torch.device("mps")
model, preprocess = clip.load("ViT-L-14", device=device)
</code></pre><p>Downloading the dataset may require some time, so please take a moment to relax while this process completes. Once the download is finished, you can visualize some sample points like this:</p><pre><code class="language-python">from textwrap import wrap
import matplotlib.pyplot as plt
import numpy as np

def plot_images(images, captions):
    plt.figure(figsize=(15, 7))
    for i in range(len(images)):
        ax = plt.subplot(1, len(images), i + 1)
        caption = captions[i]
        caption = "\n".join(wrap(caption, 12))
        plt.title(caption)
        plt.imshow(images[i])
        plt.axis("off")

# Assuming ds is a dictionary with "train" key containing a list of samples
sample_dataset = ds["train"]
random_indices = np.random.choice(len(sample_dataset), size=2, replace=False)
random_indices = [index.item() for index in random_indices]

# Get the random images and their captions
random_images = [np.array(sample_dataset[index]["image"]) for index in random_indices]
random_captions = [sample_dataset[index]["text"] for index in random_indices]

# Plot the random images with their captions
plot_images(random_images, random_captions)

# Show the plot
plt.show()
</code></pre><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/output3.png?raw=true" alt="output3" /></p><h3 id="storing-the-embeddings">Storing the Embeddings</h3><p>The dataset consists of two key features: the image and its corresponding descriptive text. Initially, our task is to create a LanceDB table to store the embeddings. This process is straightforward ‚Äì all you need to do is define the relevant schema. In our case, the columns include ‚Äúvector‚Äù for storing the multimodal embeddings, a ‚Äútext‚Äù column for the descriptive text, and a ‚Äúlabel‚Äù column for the corresponding IDs.</p><pre><code class="language-python">import pyarrow as pa
import lancedb
import tqdm

db = lancedb.connect('./data/tables')
schema = pa.schema(
  [
      pa.field("vector", pa.list_(pa.float32(), 512)),
      pa.field("text", pa.string()),
      pa.field("id", pa.int32())
  ])
tbl = db.create_table("gta_data", schema=schema, mode="overwrite")
</code></pre><p>Executing this will generate a table with the specified schema, and it‚Äôs ready to store the embeddings along with the relevant columns. It‚Äôs as straightforward as that ‚Äì almost too easy!</p><h3 id="encode-the-images">Encode the Images</h3><p>Now, we‚Äôll simply take the images from the dataset, feed them into an encoding function that leverages our Multimodal Embedding model, and generate the corresponding embeddings. These embeddings will then be stored in the database.</p><pre><code class="language-python">def embed_image(img):
    processed_image = preprocess(img)
    unsqueezed_image = processed_image.unsqueeze(0).to(device)
    embeddings = model.encode_image(unsqueezed_image)
    
    # Detach, move to CPU, convert to numpy array, and extract the first element as a list
    result = embeddings.detach().cpu().numpy()[0].tolist()
    return result
</code></pre><p>So our <code>embed_image</code> function takes an input image, prepocesses it through our CLIP model preprocessor, encode the preprocessed image and returns a list representing the embeddings of that image. This returned embedding serves as a concise numerical representation, capturing all the key features and patterns within the image for downstream tasks or analysis. The next thing is to call this function for all the images and store the relevant embeddings in the database.</p><pre><code class="language-python">data = []
for i in range(len(ds["train"])):
    img = ds["train"][i]['image']
    text = ds["train"][i]['text']
    
    # Encode the image
    encoded_img = embed_image(img)
    data.append({"vector": encoded_img, "text": text, "id" : i})
</code></pre><p>Here, we‚Äôre just taking a list, adding the numeric embeddings, reference text and the current index id to it. All that‚Äôs left is to include this list in our LanceDB table. And voila, our datalake for the embeddings is set up and good to go!</p><pre><code class="language-python">tbl.add(data)
tbl.to_pandas()
</code></pre><p>Up until now, we‚Äôve efficiently converted the images into their respective multimodal embeddings and stored them in the LanceDB table. Now the LanceDB tables offer a convenient feature: if there‚Äôs a need to add or remove images, it‚Äôs remarkably straightforward. Just encode the new image and add it, following to the same steps we followed for the previous images.</p><h3 id="query-search">Query search</h3><p>Our next move is to embed our text query using the same multimodal embedding model we used for our images. Remember that ‚Äúbox‚Äù I mentioned earlier? Essentially, we want this box to create embeddings for both our images and our texts which ensures that the representation of different types of data happens in the same way. Following this, we just need to initiate a search to find the nearest image embeddings that matches our text query.</p><pre><code class="language-python">
def embed_txt(txt):
    tokenized_text = clip.tokenize([txt]).to(device)
    embeddings = model.encode_text(tokenized_text)
    
    # Detach, move to CPU, convert to numpy array, and extract the first element as a list
    result = embeddings.detach().cpu().numpy()[0].tolist()
    return result

res = tbl.search(embed_txt("a road with a stop")).limit(3).to_pandas()
res
</code></pre><pre><code class="language-txt">0 | [0.064575195, .. ] | there is a stop sign...| 569 |	131.995728
1 | [-0.07989502, .. ] | there is a bus that... | 423 | 135.047852
2 | [0.06756592, .. ]  | amazing view of a ...	| 30  | 135.309937
</code></pre><p>Let‚Äôs slow down a bit and understand what just happened. Putting simply, the code snippet executes a search algorithm in it‚Äôs core to pinpoint the most relevant image embedding that align with our text query. The resulting output, as showcased above, give us the embeddings which closely resembles to our text query. In the result, the second column presents the embedding vector, while the third column contains the description of the image that closely matches our text query. Essentially, we‚Äôve determined which image is closely corresponds to our text query by examining the embeddings of both our text query and the image.</p><h3 id="its-as-much-as-similiar-to-saying-if-these-numbers-represents-the-word-cat-i-spot-an-image-with-a-similar-set-of-numbers-so-most-likely-its-a-match-for-an-image-of-a-cat-">It‚Äôs as much as similiar to saying, If these numbers represents the word ‚ÄúCat‚Äù, I spot an image with a similar set of numbers, so most likely it‚Äôs a match for an image of a ‚ÄúCat‚Äù. üò∫</h3><p>If you are looking for the explanation of how the search happens, I will write a detailed explanation in the coming write ups because it‚Äôs so exciting to look under the hood and see how the searching happens. Essentially there is something called Approximate Nearest Neighbors (ANN) which is a technique used to efficiently find the closest points in high-dimensional spaces. ANN is extensively used in data mining, machine learning, computer vision and NLP use cases. So when we passed our embedded text query to the searching algorithm and asked it to give us the closest sample point in the vector space, it used a type of ANN algorithm to get it for us. Specifically LanceDB utilizes DANN (Deep Approximate Nearest Neighbor) for searching the relevant embeddings within its ecosystem..</p><p>So we have 5 columns for our result, first is just an index number, second is the embedding vector, third is the description of the image that matches our text query, fourth is the label of the image but I would like to have your attention on the last column. It‚Äôs called Distance. So when I talked about the ANN algorithm, it essentially draws a line between the current data point which is in our case the embedding of our text query and find out which image data point is closest to ours. So when you notice the other data points from the results have more distance as compared to the top one, that means they are a little bit further away than the top one. How the distance is calculated is the part of the algorithm itself.</p><p>In our results, we have five columns. The first is the index number, the second is the embedding vector, the third is the description of the image matching our text query, and the fourth is the label of the image. However, let‚Äôs focus on the last column ‚Äì Distance. When I mentioned the ANN algorithm, it simply draws a line between the current data point (in our case, the embedding of our text query) and identifies which data point (image) is closest to it. If you observe that the other data points in the results have a greater distance compared to the top one, it indicates they are a bit further away or more unrelated to our query. Just to make it clear, the calculation of distance is a part of the algorithm itself.</p><h2 id="d-day">D-DAY</h2><p>Now that we have all the necessary information, displaying the most relevant image for our query is straightforward. Simply take the relevant label of the top-matched embedding vector and showcase the corresponding image.</p><pre><code class="language-python">data_id = int(res['id'][0])
display(ds["train"][data_id]['image'])
print(ds["train"][data_id]['text'])
</code></pre><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/output_final.png?raw=true" alt="output_final" /></p><pre><code class="language-python">there is a truck driving down a street with a stop sign
</code></pre><h3 id="whats-next">What‚Äôs next?</h3><p>To make things more interesting, I‚Äôm currently working on creating an extensive GTA-V captioning dataset. This dataset will include a larger number of images paired with their respective reference text, providing us with a richer set of queries to explore and experiment with.. Nevertheless, there‚Äôs always room for refining the model. We can explore creating a customized CLIP model, adjusting various parameters. Increasing the number of training epochs may afford the model more time to grasp the relevance between embeddings. Additionally, there‚Äôs an impressive multimodal embedding model developed by the Meta known as <a href="https://imagebind.metademolab.com/">ImageBind</a>. We can consider trying ImageBind as an alternative to our current multimodal embedding model and compare the outcomes. With numerous options available, the fundamental concept behind the Multimodal RAG workflow remains largely consistent.</p><p>Here‚Äôs how everything comes together in one frame and this is the <a href="https://colab.research.google.com/drive/1LM-WrDSBXpiMZ94CtaMCaGHlkxqGR6WK?usp=sharing">Collab</a> for your reference</p><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/multimodalrag.png?raw=true" alt="multimodal_rag" /></p><span class="meta"><time datetime="2024-03-03T00:00:00+05:30">March 3, 2024</time></span></section><!-- --- layout: default ---<section class="post"><h2>Multimodal RAG applications</h2><p><em>Artificial Intelligence (AI) has been actively working with text for quite some time, but the world isn‚Äôt solely centered around words. If you take a moment to look around, you‚Äôll find a mix of text, images, videos, audios, and their combinations.</em></p><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/Renevant%20Cheetah-66.jpg?raw=true" alt="boomer_ai" /></p><p>Today we are going to work on Multimodality which is basically a concept that essentially empowers AI models with the capacity to perceive, listen, and comprehend data in diverse formats together with the text. Pretty much like how we do!</p><p>In an ideal situation, we should be able to mix different types of data together and show them to a generative AI model at the same time and iterate on it. It could be as simple as telling the AI model, ‚ÄúHey, a few days ago, I sent you a picture of a brown, short dog. Can you find that picture for me?‚Äù and the model should then give us the details of that picture. Basically, we want the AI to understand the things more like how we humans do, becoming really good at handling and responding to all kinds of information.</p><p>But the challenge here is to make a computer understands one data format with it‚Äôs related reference, and that could be a mix of text, audio, thermal imagery, and videos. Now to make this happen, we use something called Embeddings. It‚Äôs really a numeric vector which contains a bunch of numbers written together that might not mean much to us but understood by machines very well.</p><h3 id="cat-is-equal-to-cat">Cat is equal to Cat</h3><p>Let‚Äôs think of the text components for now, so we are currently aiming that our model should learn that words like ‚ÄúDog‚Äù and ‚ÄúCat‚Äù are closely linked to the word ‚ÄúPet.‚Äù Now this understanding is easily achievable by using an embedding model which will convert these text words into their respective embeddings first and then the model is trained to follow a straightforward logic: if words are related, they are close together in the vector space, if not, they would be separated by the adequate distance.</p><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/embeddings.png?raw=true" alt="embeddings" /></p><p>But to help a model recognize that an image of a ‚ÄúCat‚Äù and the word ‚ÄúCat‚Äù are similar, we rely on Multimodal Embeddings. To simplify things a bit, imagine there is a magic box which is capable of handling various inputs ‚Äì images, audios, text, and more.</p><p>Now, when we feed the box with an image of a ‚ÄúCat‚Äù with the text ‚ÄúCat,‚Äù it performs its magic and produces two numeric vectors. When these two vectors were given to a machines, it made machines to think, ‚ÄúHmm, based on these numeric values, it seems like both are connected to ‚ÄúCat‚Äù. So that‚Äôs exactly what we were aiming for! Our goal was to help machines to recognize the close connection between an image of a ‚ÄúCat‚Äù and the text ‚ÄúCat‚Äù. However, to validate this concept, when we plot those two numeric vectors in a vector space, it turns out they are very close to each other. This outcome exactly mirrors what we observed earlier with the proximity of the two text words ‚ÄúCat‚Äù and ‚ÄúDog‚Äù in the vector space.</p><h2 id="ladies-and-gentlemen-thats-the-essence-of-multimodality-">Ladies and gentlemen, that‚Äôs the essence of Multimodality. üëè</h2><p>So we made our model to comprehend the association between ‚ÄúCat‚Äù images and the word ‚ÄúCat.‚Äù Well this is it, I mean if you are able to do this, you would have ingested the audio, images, videos as well as the word ‚ÄúCat‚Äù and the model will understand how the cat is being portrayed across all kinds of file format..</p><h3 id="rag-is-here">RAG is here..</h3><p>Well if you don‚Äôt know what RAG means, I would highly advise to read this article <a href="https://vipul-maheshwari.github.io/2024/02/14/rag-application-with-langchain">here</a> which I wrote some days back and loved by ton of people, not exageerating it but yeah, it‚Äôs good to get the balls rolling..</p><p>So there are impressive models like DALLE-2 that provide text-to-image functionality. Essentially, you input text, and the model generates relevant images for you. But can we create a system similar to Multimodal RAG, where the model produces output images based on our own data? Alright, so the goal for today is to create an AI model that when asked something like, ‚ÄúHow many girls were there in my party?‚Äù üíÄ not only provides textual information but also includes a relevant image related to it. Think of it as an extension of simple RAG system, but now incorporating images.</p><p>Before we dive in, remember that Multimodality isn‚Äôt limited to just text-to-image or image-to-text as it encompasses the freedom to input and output any type of data. However, for now, let‚Äôs concentrate on the interaction from image to text exclusively.</p><h3 id="contrasive-learning">Contrasive learning</h3><p>Now the question is, What exactly was that box doing? The magic it performs is known as Contrastive Learning. While the term might sound complex, it‚Äôs not that tricky. To simplify, consider a dataset with images, along with a caption describing what the image represents.</p><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/clipmodel.png?raw=true" alt="clipmodel" /></p><p>Alright, now what happens is: we give our text-image model with these Positive and Negative samples, where each sample consists of an image and a descriptive text. Positive samples are those where the image and text are correctly align ‚Äì for instance, a picture of a cat matched with the text ‚Äúthis is an image of a cat.‚Äù Conversely, negative samples involve a mismatch, like presenting an image of a dog alongside the text ‚Äúthis is an image of a cat.‚Äù</p><p>Now we train our text-image model to recognize that positive samples offer accurate interpretations, while negative samples are misleading and should be disregarded during training. In formal terms this technique is called ‚ÄúCLIP‚Äù (Contrastive Language-Image Pre-training) introduced by OpenAI where authors trained an image-text model on something around 400 million image caption pairs taken from the interent and everytime model makes a mistake, the contrasive loss function increases and penalize it to make sure the model trains well. The same kind of principles are applied to the other modality combinations as well, so voice of cat with the word cat is positive sample for speech-text model, a video of cat with the descriptive text ‚Äúthis is a cat‚Äù is a positive sample for video-text model.</p><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/easy.png?raw=true" alt="easy" /></p><h3 id="show-time">Show time</h3><p>Well you don‚Äôt have to build that box from scratch because folks have already done it for us. There‚Äôs a Multimodal embedding model, like the ‚ÄúViT-L/14‚Äù from OpenAI. This model can handle various data types, including text, images, videos, audios, and even thermal and gyroscope data. Now, onto the next question: how do we store those embeddings?</p><p>For that we‚Äôll need a vector database that can efficiently fetch, query, and retrieve relevant embeddings for us, ideally one that supports multimodal data and doesn‚Äôt burn a hole in our wallets. That‚Äôs where LanceDB comes into play.</p><h3 id="vector-database">Vector database</h3><p>When we talk about the vector database, there are ton of options available in the current market, but there is something about the LanceDB which makes it stands out as an optimal choice for a vector database, As far as I have used it, it address the limitations of traditional embedded databases in handling AI/ML workloads. When I say traditional, it typically means those databases management tools which are not aligned with the usage of heavy compuatation that comes with the ML infra.</p><p>TLDR; LanceDB operates on a serverless architecture, meaning storage and compute are separated into two distinct units. This design makes it exceptionally fast for RAG use cases, ensuring fast fetching and retrieval. Additionally, it has some notable advantages ‚Äì being open source, utilizing its Lance columnar data format built on top of Apache Arrow for high efficiency, persistent storage capabilities, and incorporating its own Disk Approximate Nearest Neighbor search. All these factors collectively make LanceDB an ideal solution for accessing and working with multimodal data. I love you LanceDB ‚ù§Ô∏è.</p><h3 id="data-time">Data time</h3><p>To add some excitement, I‚Äôve crafted a GTA-V Image Captioning dataset, featuring thousands of images, each paired with a descriptive text illustrating the image‚Äôs content. Now, when we train our magic box, the expectation is clear ‚Äì if I ask that box to provide me an image of ‚Äúroad with a stop sign,‚Äù it should deliver a GTA-V image of a road with a stop sign on it. Otherwise, what‚Äôs the point, right?</p><h3 id="faq">FAQ</h3><ol><li>We will be using ‚ÄúViT-L/14‚Äù to convert our multimodal data into it‚Äôs respective embeddings.</li><li>LanceDB as our vector database to store the relevant embeddings.</li><li>GTA-V Image Captioning dataset for our magic box.</li></ol><h3 id="environment-setup">Environment Setup</h3><p>I am using a MacBook Air M1, and it‚Äôs important to note that some kind of dependencies and configurations may vary depending on the type of system that you are running, so it‚Äôs important to take that into account.</p><p>Here are the steps to install the relevant dependencies</p><pre><code class="language-python"># Create a virtual environment
python3 -m venv env

# Activate the virtual environment
source env/bin/activate

# Upgrade pip in the virtual environment
pip install --upgrade pip

# Install required dependencies
pip3 install lancedb clip torch datasets pillow 
pip3 install git+https://github.com/openai/CLIP.git
</code></pre><p>And don‚Äôt forget to get your access token from the hugging face to download the data.</p><h3 id="downloading-the-data">Downloading the Data</h3><p>Dataset can easily be fetched using the datasets library.</p><pre><code class="language-python">import clip
import torch
import os
from datasets import load_dataset

ds = load_dataset("vipulmaheshwari/GTA-Image-Captioning-Dataset")
device = torch.device("mps")
model, preprocess = clip.load("ViT-L-14", device=device)
</code></pre><p>Downloading the dataset may require some time, so please take a moment to relax while this process completes. Once the download is finished, you can visualize some sample points like this:</p><pre><code class="language-python">from textwrap import wrap
import matplotlib.pyplot as plt
import numpy as np

def plot_images(images, captions):
    plt.figure(figsize=(15, 7))
    for i in range(len(images)):
        ax = plt.subplot(1, len(images), i + 1)
        caption = captions[i]
        caption = "\n".join(wrap(caption, 12))
        plt.title(caption)
        plt.imshow(images[i])
        plt.axis("off")

# Assuming ds is a dictionary with "train" key containing a list of samples
sample_dataset = ds["train"]
random_indices = np.random.choice(len(sample_dataset), size=2, replace=False)
random_indices = [index.item() for index in random_indices]

# Get the random images and their captions
random_images = [np.array(sample_dataset[index]["image"]) for index in random_indices]
random_captions = [sample_dataset[index]["text"] for index in random_indices]

# Plot the random images with their captions
plot_images(random_images, random_captions)

# Show the plot
plt.show()
</code></pre><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/output3.png?raw=true" alt="output3" /></p><h3 id="storing-the-embeddings">Storing the Embeddings</h3><p>The dataset consists of two key features: the image and its corresponding descriptive text. Initially, our task is to create a LanceDB table to store the embeddings. This process is straightforward ‚Äì all you need to do is define the relevant schema. In our case, the columns include ‚Äúvector‚Äù for storing the multimodal embeddings, a ‚Äútext‚Äù column for the descriptive text, and a ‚Äúlabel‚Äù column for the corresponding IDs.</p><pre><code class="language-python">import pyarrow as pa
import lancedb
import tqdm

db = lancedb.connect('./data/tables')
schema = pa.schema(
  [
      pa.field("vector", pa.list_(pa.float32(), 512)),
      pa.field("text", pa.string()),
      pa.field("id", pa.int32())
  ])
tbl = db.create_table("gta_data", schema=schema, mode="overwrite")
</code></pre><p>Executing this will generate a table with the specified schema, and it‚Äôs ready to store the embeddings along with the relevant columns. It‚Äôs as straightforward as that ‚Äì almost too easy!</p><h3 id="encode-the-images">Encode the Images</h3><p>Now, we‚Äôll simply take the images from the dataset, feed them into an encoding function that leverages our Multimodal Embedding model, and generate the corresponding embeddings. These embeddings will then be stored in the database.</p><pre><code class="language-python">def embed_image(img):
    processed_image = preprocess(img)
    unsqueezed_image = processed_image.unsqueeze(0).to(device)
    embeddings = model.encode_image(unsqueezed_image)
    
    # Detach, move to CPU, convert to numpy array, and extract the first element as a list
    result = embeddings.detach().cpu().numpy()[0].tolist()
    return result
</code></pre><p>So our <code>embed_image</code> function takes an input image, prepocesses it through our CLIP model preprocessor, encode the preprocessed image and returns a list representing the embeddings of that image. This returned embedding serves as a concise numerical representation, capturing all the key features and patterns within the image for downstream tasks or analysis. The next thing is to call this function for all the images and store the relevant embeddings in the database.</p><pre><code class="language-python">data = []
for i in range(len(ds["train"])):
    img = ds["train"][i]['image']
    text = ds["train"][i]['text']
    
    # Encode the image
    encoded_img = embed_image(img)
    data.append({"vector": encoded_img, "text": text, "id" : i})
</code></pre><p>Here, we‚Äôre just taking a list, adding the numeric embeddings, reference text and the current index id to it. All that‚Äôs left is to include this list in our LanceDB table. And voila, our datalake for the embeddings is set up and good to go!</p><pre><code class="language-python">tbl.add(data)
tbl.to_pandas()
</code></pre><p>Up until now, we‚Äôve efficiently converted the images into their respective multimodal embeddings and stored them in the LanceDB table. Now the LanceDB tables offer a convenient feature: if there‚Äôs a need to add or remove images, it‚Äôs remarkably straightforward. Just encode the new image and add it, following to the same steps we followed for the previous images.</p><h3 id="query-search">Query search</h3><p>Our next move is to embed our text query using the same multimodal embedding model we used for our images. Remember that ‚Äúbox‚Äù I mentioned earlier? Essentially, we want this box to create embeddings for both our images and our texts which ensures that the representation of different types of data happens in the same way. Following this, we just need to initiate a search to find the nearest image embeddings that matches our text query.</p><pre><code class="language-python">
def embed_txt(txt):
    tokenized_text = clip.tokenize([txt]).to(device)
    embeddings = model.encode_text(tokenized_text)
    
    # Detach, move to CPU, convert to numpy array, and extract the first element as a list
    result = embeddings.detach().cpu().numpy()[0].tolist()
    return result

res = tbl.search(embed_txt("a road with a stop")).limit(3).to_pandas()
res
</code></pre><pre><code class="language-txt">0 | [0.064575195, .. ] | there is a stop sign...| 569 |	131.995728
1 | [-0.07989502, .. ] | there is a bus that... | 423 | 135.047852
2 | [0.06756592, .. ]  | amazing view of a ...	| 30  | 135.309937
</code></pre><p>Let‚Äôs slow down a bit and understand what just happened. Putting simply, the code snippet executes a search algorithm in it‚Äôs core to pinpoint the most relevant image embedding that align with our text query. The resulting output, as showcased above, give us the embeddings which closely resembles to our text query. In the result, the second column presents the embedding vector, while the third column contains the description of the image that closely matches our text query. Essentially, we‚Äôve determined which image is closely corresponds to our text query by examining the embeddings of both our text query and the image.</p><h3 id="its-as-much-as-similiar-to-saying-if-these-numbers-represents-the-word-cat-i-spot-an-image-with-a-similar-set-of-numbers-so-most-likely-its-a-match-for-an-image-of-a-cat-">It‚Äôs as much as similiar to saying, If these numbers represents the word ‚ÄúCat‚Äù, I spot an image with a similar set of numbers, so most likely it‚Äôs a match for an image of a ‚ÄúCat‚Äù. üò∫</h3><p>If you are looking for the explanation of how the search happens, I will write a detailed explanation in the coming write ups because it‚Äôs so exciting to look under the hood and see how the searching happens. Essentially there is something called Approximate Nearest Neighbors (ANN) which is a technique used to efficiently find the closest points in high-dimensional spaces. ANN is extensively used in data mining, machine learning, computer vision and NLP use cases. So when we passed our embedded text query to the searching algorithm and asked it to give us the closest sample point in the vector space, it used a type of ANN algorithm to get it for us. Specifically LanceDB utilizes DANN (Deep Approximate Nearest Neighbor) for searching the relevant embeddings within its ecosystem..</p><p>So we have 5 columns for our result, first is just an index number, second is the embedding vector, third is the description of the image that matches our text query, fourth is the label of the image but I would like to have your attention on the last column. It‚Äôs called Distance. So when I talked about the ANN algorithm, it essentially draws a line between the current data point which is in our case the embedding of our text query and find out which image data point is closest to ours. So when you notice the other data points from the results have more distance as compared to the top one, that means they are a little bit further away than the top one. How the distance is calculated is the part of the algorithm itself.</p><p>In our results, we have five columns. The first is the index number, the second is the embedding vector, the third is the description of the image matching our text query, and the fourth is the label of the image. However, let‚Äôs focus on the last column ‚Äì Distance. When I mentioned the ANN algorithm, it simply draws a line between the current data point (in our case, the embedding of our text query) and identifies which data point (image) is closest to it. If you observe that the other data points in the results have a greater distance compared to the top one, it indicates they are a bit further away or more unrelated to our query. Just to make it clear, the calculation of distance is a part of the algorithm itself.</p><h2 id="d-day">D-DAY</h2><p>Now that we have all the necessary information, displaying the most relevant image for our query is straightforward. Simply take the relevant label of the top-matched embedding vector and showcase the corresponding image.</p><pre><code class="language-python">data_id = int(res['id'][0])
display(ds["train"][data_id]['image'])
print(ds["train"][data_id]['text'])
</code></pre><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/output_final.png?raw=true" alt="output_final" /></p><pre><code class="language-python">there is a truck driving down a street with a stop sign
</code></pre><h3 id="whats-next">What‚Äôs next?</h3><p>To make things more interesting, I‚Äôm currently working on creating an extensive GTA-V captioning dataset. This dataset will include a larger number of images paired with their respective reference text, providing us with a richer set of queries to explore and experiment with.. Nevertheless, there‚Äôs always room for refining the model. We can explore creating a customized CLIP model, adjusting various parameters. Increasing the number of training epochs may afford the model more time to grasp the relevance between embeddings. Additionally, there‚Äôs an impressive multimodal embedding model developed by the Meta known as <a href="https://imagebind.metademolab.com/">ImageBind</a>. We can consider trying ImageBind as an alternative to our current multimodal embedding model and compare the outcomes. With numerous options available, the fundamental concept behind the Multimodal RAG workflow remains largely consistent.</p><p>Here‚Äôs how everything comes together in one frame and this is the <a href="https://colab.research.google.com/drive/1LM-WrDSBXpiMZ94CtaMCaGHlkxqGR6WK?usp=sharing">Collab</a> for your reference</p><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/multimodal_rag/multimodalrag.png?raw=true" alt="multimodal_rag" /></p><span class="meta"><time datetime="2024-03-03T00:00:00+05:30">March 3, 2024</time> &middot; <a href="/tag/LLM">LLM</a>, <a href="/tag/Deep Learning">Deep Learning</a></span>--> <!--</section>--></main><script async src="https://www.googletagmanager.com/gtag/js?id=G-JBZRCCYMBP"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-JBZRCCYMBP'); </script></body></html>
