<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Create a custom retriever" /><meta property="og:locale" content="en_US" /><meta name="description" content="This post is all about creating a custom retriever." /><meta property="og:description" content="This post is all about creating a custom retriever." /><link rel="canonical" href="http://localhost:4000/2025/07/02/custom-retriever" /><meta property="og:url" content="http://localhost:4000/2025/07/02/custom-retriever" /><meta property="og:site_name" content="Deox Labs" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-07-02T00:00:00+05:30" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Create a custom retriever" /><meta name="twitter:site" content="@fuxssss" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-07-02T00:00:00+05:30","datePublished":"2025-07-02T00:00:00+05:30","description":"This post is all about creating a custom retriever.","headline":"Create a custom retriever","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2025/07/02/custom-retriever"},"url":"http://localhost:4000/2025/07/02/custom-retriever"}</script><title> Create a custom retriever - Deox Labs</title><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Deox Labs" href="/atom.xml"><link rel="alternate" type="application/json" title="Deox Labs" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#f5f5f5;max-width:100%;overflow-x:auto}code{padding:.1rem;font-size:.85rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%;background:none;display:block;margin:auto}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}.post ol,.project ul,.post ol,.post ul{padding-left:1rem}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}@media print{.no-print,.no-print *{display:none !important}}.back-to-top{position:fixed;bottom:20px;right:20px;background-color:#fff;color:#000;padding:10px;text-decoration:none;border-radius:5px;display:block;margin-right:20%}</style></head><body><main><header aria-hidden="true" class="no-print"> <!--<h1 class="logo">Deox Labs</h1>--><nav role="navigation" aria-hidden="true"><ul><li><a href="/" >Home</a></li><li><a href="/about" >About</a></li><li><a href="/contact" >Contact</a></li><li><a href="/projects" >Projects</a></li><li><a href="/search" >Search</a></li></ul></nav></header><section class="post"><h1>Create a custom retriever</h1><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/custom-retriever/custom_retriever.png?raw=true" alt="title_image" /></p><p>As retrieval-augmented generation (RAG) systems continue to evolve, the need for <strong>custom, domain-specific retrievers</strong> is becoming more and more obvious. Sure, traditional vector databases are great for basic similarity search—but the moment you throw in more complex, context-heavy queries, they start to fall short. Especially when you’re working with real-world data that needs richer filtering or semantic understanding.</p><p>In this post, we’ll walk through how to build a <strong>custom retriever</strong> by combining <strong>Superlinked</strong>—a high-performance vector compute engine—with <strong>LlamaIndex</strong>’s flexible retrieval framework. Think of this as a hands-on guide: we’ll show you how to wire things up end-to-end using a <strong>Steam game recommendation system</strong> as the example. We’ll tap into semantic similarity, multi-field indexing, and advanced query logic to make sure recommendations aren’t just relevant—they actually make sense.</p><h2 id="why-custom-retrievers-matter">Why Custom Retrievers Matter</h2><p>Before we dive into the implementation, let’s talk about why building your own retriever is often the better choice in real-world RAG setups.</p><ol><li><strong>Tuned for Your Domain</strong> – Generic retrievers are fine for general use, but they tend to miss the subtle stuff. Think about jargon, shorthand, or domain-specific phrasing—those don’t usually get picked up unless your retriever knows what to look for. That’s where custom ones shine: you can hardwire in that context.</li><li><strong>Works Beyond Just Text</strong> – Most real-world data isn’t just plain text. You’ll often have metadata, tags, maybe even embeddings from images or audio. For example, in a game recommendation system, we don’t just care about the game description—we also want to factor in genres, tags, user ratings, and more. Think about this logic: someone searching for a “strategy co-op game with sci-fi elements” won’t get far with text-only matching.</li><li><strong>Custom Filtering and Ranking Logic</strong> – Sometimes you want to apply your own rules to how things are scored or filtered. Maybe you want to prioritize newer content, or penalize results that don’t meet certain quality thresholds. I mean, having that kind of control is like giving your retriever an actual brain—it can reason through relevance instead of just relying on vector distances.</li><li><strong>Performance Gains</strong> – Let’s be real: general-purpose solutions are built to work “okay” for everyone, not great for you. If you know your data and your access patterns, you can fine-tune your retriever to run faster, rank better, and reduce unnecessary noise in the results.</li></ol><h2 id="implementation-breakdown">Implementation Breakdown</h2><h3 id="part-1-core-dependencies-and-imports">Part 1: Core Dependencies and Imports</h3><pre><code class="language-python">import pandas as pd
from typing import List
from llama_index.core.retrievers import BaseRetriever
from llama_index.core.schema import NodeWithScore, QueryBundle, TextNode
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.response_synthesizers import get_response_synthesizer
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
import superlinked.framework as sl

</code></pre><p>The import structure reveals our hybrid approach:</p><ul><li><strong>LlamaIndex Core</strong>: Provides the retrieval abstraction layer</li><li><strong>Superlinked Framework</strong>: Handles vector computation and semantic search</li><li><strong>Pandas</strong>: Manages data preprocessing and manipulation</li></ul><h3 id="part-2-understanding-llamaindex-custom-retrievers">Part 2: Understanding LlamaIndex Custom Retrievers</h3><p>Before diving into our Superlinked implementation, it’s crucial to understand how LlamaIndex’s custom retriever architecture works and why it’s so powerful for building domain-specific RAG applications.</p><h3 id="baseretriever-abstraction">BaseRetriever Abstraction</h3><p>LlamaIndex provides an abstract <code>BaseRetriever</code> class that serves as the foundation for all retrieval operations. The beauty of this design lies in its simplicity—any custom retriever only needs to implement one core method:</p><pre><code class="language-python">from abc import abstractmethod
from llama_index.core.retrievers import BaseRetriever
from llama_index.core.schema import NodeWithScore, QueryBundle

class BaseRetriever:
    @abstractmethod
    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        """Retrieve nodes given query."""
        pass
</code></pre><p>The moat here is the presence of the Retrieval Protocol from the LlamaIndex. As this “retrieval protocol” makes it easy to plug in different backends or strategies without having to touch the rest of your system. Let’s break it down on what’s exactly is going on :</p><ol><li><p><strong>Input: <code>QueryBundle</code></strong></p><p>This is the query object passed into your retriever. At minimum, it contains the user’s raw query string (e.g., “sci-fi strategy games”). But it can also include extra metadata—like filters, embeddings, or user preferences. Basically, anything that might help shape a more relevant response.</p></li><li><p><strong>Output: <code>List[NodeWithScore]</code></strong></p><p>The retriever returns a list of nodes—these are your chunks of content, documents, or data entries—each paired with a relevance score. The higher the score, the more relevant the node is to the query. This list is what gets passed downstream to the LLM or other post-processing steps. As in our case, we are plugging on the</p></li><li><p><strong>Processing: Backend-Agnostic</strong></p><p>Here’s the cool part: how you get from query to result is totally up to you. You can use a vector database, a traditional search engine, a REST API, or even something handcrafted for your specific use case. This decouples logic and gives you full control over the retrieval stack.</p></li></ol><h3 id="but-why-this-matters">But Why This Matters?</h3><p>This abstraction isn’t just clean—it’s <em>powerful</em>. It means you can:</p><ul><li><strong>Combine multiple strategies</strong> – Use dense vector search <em>and</em> keyword filtering together if needed.</li><li><strong>Run A/B tests easily</strong> – Compare different retrievers to see what gives better results for your users.</li><li><strong>Plug into any agent or tool</strong> – Whether you’re building a chatbot, a search UI, or a full-blown agent system, this retriever interface slots in easily.</li></ul><p>Think of the retrieval protocol as the API contract between your “retrieval brain” and everything else. Once you follow it, you’re free to innovate however you want behind the scenes.</p><h3 id="plugging-superlinked-into-llamaindex">Plugging Superlinked into LlamaIndex</h3><p>Now let’s see how we bridge Superlinked’s advanced vector computation capabilities with LlamaIndex’s retrieval abstraction:</p><pre><code class="language-python">class SuperlinkedSteamGamesRetriever(BaseRetriever):
    """A custom LlamaIndex retriever using Superlinked for Steam games data."""
    
    def __init__(self, csv_file: str, top_k: int = 10):
        self.top_k = top_k
        self.df = pd.read_csv(csv_file)
        
        # Critical: Text field combination for semantic richness
        self.df['combined_text'] = (
            self.df['name'].astype(str) + " " +
            self.df['desc_snippet'].astype(str) + " " +
            self.df['genre'].astype(str) + " " +
            self.df['game_details'].astype(str) + " " +
            self.df['game_description'].astype(str)
        )
        
        self._setup_superlinked()
</code></pre><p><strong>Integration Architecture Deep Dive:</strong></p><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/custom-retriever/architecture.png?raw=true" alt="architecture.png" /></p><h3 id="part-3-superlinked-schema-definition-and-setup">Part 3: Superlinked Schema Definition and Setup</h3><pre><code class="language-python">def _setup_superlinked(self):
    """Set up Superlinked schema, space, index, and executor."""

    # Schema Definition - The Foundation of Type Safety
    class GameSchema(sl.Schema):
        game_number: sl.IdField
        name: sl.String
        desc_snippet: sl.String
        game_details: sl.String
        languages: sl.String
        genre: sl.String
        game_description: sl.String
        original_price: sl.Float
        discount_price: sl.Float
        combined_text: sl.String  # Our semantic search target

</code></pre><p>In Superlinked, the schema isn’t just about defining data types— it’s more like a formal definition between our data and the underlying vector compute engine. This schema determines how our data gets parsed, indexed, and queried—so getting it right is crucial.</p><p>In our <code>SuperlinkedSteamGamesRetriever</code>, the schema is defined like this:</p><pre><code class="language-python">class GameSchema(sl.Schema):
    game_number: sl.IdField
    name: sl.String
    desc_snippet: sl.String
    game_details: sl.String
    languages: sl.String
    genre: sl.String
    game_description: sl.String
    original_price: sl.Float
    discount_price: sl.Float
    combined_text: sl.String  # Key field for semantic search

</code></pre><p>Let’s break down what each of these elements actually <em>does</em>:</p><ul><li><p><strong><code>sl.IdField</code> (→ <code>game_number</code>)</strong></p><p>Think of this as our primary key. It gives each game a unique identity and allows Superlinked to index and retrieve items efficiently, I mean basically it’s about how we are telling the Superlinked to segregate the unique identify of the games, and btw it’s especially important when you’re dealing with thousands of records.</p></li><li><p><strong><code>sl.String</code> and <code>sl.Float</code></strong></p><p>Now these aren’t just type hints—they enable Superlinked to optimize operations differently depending on the field. For instance, <code>sl.String</code> fields can be embedded and compared semantically, while <code>sl.Float</code> fields can support numeric filtering or sorting.</p></li><li><p><strong><code>combined_text</code></strong></p><p>This is the <strong>semantic anchor</strong> of our retriever. It’s a synthetic field where we concatenate the game name, description, genre, and other relevant attributes into a single block of text. This lets us build a single <strong>text similarity space</strong> using sentence-transformer embeddings:</p><pre><code class="language-python">  self.text_space = sl.TextSimilaritySpace(
      text=self.game.combined_text,
      model="sentence-transformers/all-mpnet-base-v2"
  )
    
</code></pre><p>Why do this? Because users don’t just search by genre or name—they describe what they’re <em>looking for</em>. By embedding all the important signals into <code>combined_text</code>, we can better match fuzzy, natural-language queries with the most relevant games.</p></li></ul><p>This schema definition is what allows Superlinked to <strong>parse our dataset, build the index, and execute semantic queries</strong>—all while retaining full control over how data is structured and ranked.</p><h3 id="part-4-vector-space-configuration">Part 4: Vector Space Configuration</h3><pre><code class="language-python">    # Create text similarity space using sentence transformers
    self.text_space = sl.TextSimilaritySpace(
        text=self.game.combined_text,
        model="sentence-transformers/all-mpnet-base-v2"
    )

    # Create index
    self.index = sl.Index([self.text_space])

</code></pre><p>To power the semantic search over our Steam games dataset, we made two intentional design choices that balance performance, simplicity, and flexibility.</p><p>First, for the embedding model, we selected <code>all-mpnet-base-v2</code> from the Sentence Transformers library. This model produces 768-dimensional embeddings that strike a solid middle ground: they’re expressive enough to capture rich semantic meaning, yet lightweight enough to be fast in production. I mean it’s a reliable general-purpose model, known to perform well across diverse text types — which matters a lot when your data ranges from short genre tags to long-form game descriptions. In our case, we needed a model that wouldn’t choke on either end of that spectrum, and <code>all-mpnet-base-v2</code> handled it cleanly.</p><p>Next, although Superlinked supports multi-space indexing — where you can combine multiple fields or even modalities (like text + images) — we deliberately kept things simple with a single <code>TextSimilaritySpace</code>. I would have included the <code>RecencySpace</code> in here too but we don’t have the information on the release date for the games. But just to put this out here, if we have the release date information, we could plug in the RecencySpace here, and we can even sort the games with the TextSimilaritySpace along with the Recency of the games. Cool..</p><p>Now for the current <code>TextSimilaritySpace</code> details, this was built on a custom field we created by concatenating the name, genre, description, and other textual fields for each game. The idea was straightforward: keep the query logic minimal, and let the model’s semantic power do the heavy lifting.</p><h3 id="part-5-data-pipeline-and-executor-setup">Part 5: Data Pipeline and Executor Setup</h3><pre><code class="language-python">    # Map DataFrame columns to schema - Critical for data integrity
    parser = sl.DataFrameParser(
        self.game,
        mapping={
            self.game.game_number: "game_number",
            self.game.name: "name",
            self.game.desc_snippet: "desc_snippet",
            self.game.game_details: "game_details",
            self.game.languages: "languages",
            self.game.genre: "genre",
            self.game.game_description: "game_description",
            self.game.original_price: "original_price",
            self.game.discount_price: "discount_price",
            self.game.combined_text: "combined_text"
        }
    )

    # Set up in-memory execution environment
    source = sl.InMemorySource(self.game, parser=parser)
    self.executor = sl.InMemoryExecutor(sources=[source], indices=[self.index])
    self.app = self.executor.run()

    # Load and process data
    source.put([self.df])

</code></pre><p>At the heart of our retrieval system is a streamlined pipeline built for both clarity and speed. We start with the <code>DataFrameParser</code>, which serves as our ETL layer. It ensures that each field in the dataset is correctly typed and consistently mapped to our schema — essentially acting as the contract between our raw CSV data and the Superlinked indexing layer.</p><p>Once the data is structured, we feed it into an <code>InMemorySource</code>, which is ideal for datasets that comfortably fit in memory . This approach keeps everything lightning-fast without introducing storage overhead or network latency. Finally, the queries are handled by an <code>InMemoryExecutor</code>, which is optimised for sub-millisecond latency. This is what makes Superlinked suitable for real-time applications like interactive recommendation systems, where speed directly impacts user experience.</p><h3 id="part-6-the-retrieval-engine">Part 6: The Retrieval Engine</h3><pre><code class="language-python">def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
    """Core retrieval logic with semantic similarity scoring."""

    query_text = query_bundle.query_str

    # Construct Superlinked query with explicit field selection
    query = (
        sl.Query(self.index)
        .find(self.game)
        .similar(self.text_space, query_text)
        .select([
            self.game.game_number,
            self.game.name,
            self.game.desc_snippet,
            self.game.game_details,
            self.game.languages,
            self.game.genre,
            self.game.game_description,
            self.game.original_price,
            self.game.discount_price
        ])
        .limit(self.top_k)
    )

    # Execute and convert to pandas for processing
    result = self.app.query(query)
    df_result = sl.PandasConverter.to_pandas(result)

</code></pre><p>One of the things that makes Superlinked genuinely enjoyable to work with is its fluent-style query builder. If you’ve used libraries like SQLAlchemy or Django ORM, the pattern will feel familiar — each method in the chain adds clarity instead of clutter. In our case, the query starts by selecting the relevant index and defining the similarity search using the <code>.similar()</code> method, which computes cosine similarity in the embedding space. This is what allows us to retrieve semantically close games based on the user’s natural language query.</p><p>Another thoughtful design decision we made was to <strong>explicitly select the fields</strong> we care about in the result set — rather than doing something like <code>SELECT *</code>. This might sound minor, but it keeps the data lean, reduces processing overhead, and ensures we’re not passing around unnecessary payload during post-processing. Think of it as precision over bulk — especially important when you’re moving data between components in a latency-sensitive pipeline.</p><h3 id="part-7-result-processing-and-node-creation">Part 7: Result Processing and Node Creation</h3><pre><code class="language-python">    # Convert to LlamaIndex NodeWithScore format
    nodes_with_scores = []
    for i, row in df_result.iterrows():
        text = f"{row['name']}: {row['desc_snippet']}"
        metadata = {
            "game_number": row["id"],
            "name": row["name"],
            "desc_snippet": row["desc_snippet"],
            "game_details": row["game_details"],
            "languages": row["languages"],
            "genre": row["genre"],
            "game_description": row["game_description"],
            "original_price": row["original_price"],
            "discount_price": row["discount_price"]
        }

        # Simple ranking score based on result position
        score = 1.0 - (i / self.top_k)
        node = TextNode(text=text, metadata=metadata)
        nodes_with_scores.append(NodeWithScore(node=node, score=score))

    return nodes_with_scores

</code></pre><p>Now once we receive the results from Superlinked, we transform them into a format that plays well with LlamaIndex. First, we construct a <strong>human-readable text</strong> string by combining the game’s name with its short description — this becomes the content of each node, making it easier for the language model to reason about. It’s a small touch, but it really improves how relevant and understandable the retrieved data is when passed to the LLM.</p><p>Next, we make sure that <strong>all original fields</strong> from the dataset — including things like genre, pricing, and game details — are retained in the metadata. This is crucial because downstream processes might want to filter, display, or rank results based on this information. We don’t want to lose any useful context once we start working with the retrieved nodes.</p><p>Finally, we apply a lightweight <strong>score normalisation</strong> strategy. Instead of relying on raw similarity scores, we assign scores based on the position of the result in the ranked list. This keeps things simple and consistent — the top result always has the highest score, and the rest follow in descending order. It’s not fancy, but it gives us a stable and interpretable scoring system that works well across different queries.</p><h2 id="show-time-executing-the-pipeline">Show Time: Executing the pipeline</h2><p>Now that all components are in place, it’s time to bring our Retrieval-Augmented Generation (RAG) system to life. Below is the end-to-end integration of Superlinked and LlamaIndex in action.</p><pre><code class="language-python"># Initialize the RAG pipeline
Settings.llm = OpenAI(api_key="your-api-key-here")
retriever = SuperlinkedSteamGamesRetriever("sampled_games.csv", top_k=5)
response_synthesizer = get_response_synthesizer()

query_engine = RetrieverQueryEngine(
    retriever=retriever,
    response_synthesizer=response_synthesizer
)

# Execute a query
query = "I want to find a magic game with spells?"
response = query_engine.query(query)
print(response)
</code></pre><p>This setup combines our custom semantic retriever with a response synthesizer backed by an LLM. The query is seamlessly passed through the retrieval pipeline and returned with a meaningful response.</p><span class="meta"><time datetime="2025-07-02T00:00:00+05:30">July 2, 2025</time></span></section><!-- --- layout: default ---<section class="post"><h2>Create a custom retriever</h2><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/custom-retriever/custom_retriever.png?raw=true" alt="title_image" /></p><p>As retrieval-augmented generation (RAG) systems continue to evolve, the need for <strong>custom, domain-specific retrievers</strong> is becoming more and more obvious. Sure, traditional vector databases are great for basic similarity search—but the moment you throw in more complex, context-heavy queries, they start to fall short. Especially when you’re working with real-world data that needs richer filtering or semantic understanding.</p><p>In this post, we’ll walk through how to build a <strong>custom retriever</strong> by combining <strong>Superlinked</strong>—a high-performance vector compute engine—with <strong>LlamaIndex</strong>’s flexible retrieval framework. Think of this as a hands-on guide: we’ll show you how to wire things up end-to-end using a <strong>Steam game recommendation system</strong> as the example. We’ll tap into semantic similarity, multi-field indexing, and advanced query logic to make sure recommendations aren’t just relevant—they actually make sense.</p><h2 id="why-custom-retrievers-matter">Why Custom Retrievers Matter</h2><p>Before we dive into the implementation, let’s talk about why building your own retriever is often the better choice in real-world RAG setups.</p><ol><li><strong>Tuned for Your Domain</strong> – Generic retrievers are fine for general use, but they tend to miss the subtle stuff. Think about jargon, shorthand, or domain-specific phrasing—those don’t usually get picked up unless your retriever knows what to look for. That’s where custom ones shine: you can hardwire in that context.</li><li><strong>Works Beyond Just Text</strong> – Most real-world data isn’t just plain text. You’ll often have metadata, tags, maybe even embeddings from images or audio. For example, in a game recommendation system, we don’t just care about the game description—we also want to factor in genres, tags, user ratings, and more. Think about this logic: someone searching for a “strategy co-op game with sci-fi elements” won’t get far with text-only matching.</li><li><strong>Custom Filtering and Ranking Logic</strong> – Sometimes you want to apply your own rules to how things are scored or filtered. Maybe you want to prioritize newer content, or penalize results that don’t meet certain quality thresholds. I mean, having that kind of control is like giving your retriever an actual brain—it can reason through relevance instead of just relying on vector distances.</li><li><strong>Performance Gains</strong> – Let’s be real: general-purpose solutions are built to work “okay” for everyone, not great for you. If you know your data and your access patterns, you can fine-tune your retriever to run faster, rank better, and reduce unnecessary noise in the results.</li></ol><h2 id="implementation-breakdown">Implementation Breakdown</h2><h3 id="part-1-core-dependencies-and-imports">Part 1: Core Dependencies and Imports</h3><pre><code class="language-python">import pandas as pd
from typing import List
from llama_index.core.retrievers import BaseRetriever
from llama_index.core.schema import NodeWithScore, QueryBundle, TextNode
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.response_synthesizers import get_response_synthesizer
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
import superlinked.framework as sl

</code></pre><p>The import structure reveals our hybrid approach:</p><ul><li><strong>LlamaIndex Core</strong>: Provides the retrieval abstraction layer</li><li><strong>Superlinked Framework</strong>: Handles vector computation and semantic search</li><li><strong>Pandas</strong>: Manages data preprocessing and manipulation</li></ul><h3 id="part-2-understanding-llamaindex-custom-retrievers">Part 2: Understanding LlamaIndex Custom Retrievers</h3><p>Before diving into our Superlinked implementation, it’s crucial to understand how LlamaIndex’s custom retriever architecture works and why it’s so powerful for building domain-specific RAG applications.</p><h3 id="baseretriever-abstraction">BaseRetriever Abstraction</h3><p>LlamaIndex provides an abstract <code>BaseRetriever</code> class that serves as the foundation for all retrieval operations. The beauty of this design lies in its simplicity—any custom retriever only needs to implement one core method:</p><pre><code class="language-python">from abc import abstractmethod
from llama_index.core.retrievers import BaseRetriever
from llama_index.core.schema import NodeWithScore, QueryBundle

class BaseRetriever:
    @abstractmethod
    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
        """Retrieve nodes given query."""
        pass
</code></pre><p>The moat here is the presence of the Retrieval Protocol from the LlamaIndex. As this “retrieval protocol” makes it easy to plug in different backends or strategies without having to touch the rest of your system. Let’s break it down on what’s exactly is going on :</p><ol><li><p><strong>Input: <code>QueryBundle</code></strong></p><p>This is the query object passed into your retriever. At minimum, it contains the user’s raw query string (e.g., “sci-fi strategy games”). But it can also include extra metadata—like filters, embeddings, or user preferences. Basically, anything that might help shape a more relevant response.</p></li><li><p><strong>Output: <code>List[NodeWithScore]</code></strong></p><p>The retriever returns a list of nodes—these are your chunks of content, documents, or data entries—each paired with a relevance score. The higher the score, the more relevant the node is to the query. This list is what gets passed downstream to the LLM or other post-processing steps. As in our case, we are plugging on the</p></li><li><p><strong>Processing: Backend-Agnostic</strong></p><p>Here’s the cool part: how you get from query to result is totally up to you. You can use a vector database, a traditional search engine, a REST API, or even something handcrafted for your specific use case. This decouples logic and gives you full control over the retrieval stack.</p></li></ol><h3 id="but-why-this-matters">But Why This Matters?</h3><p>This abstraction isn’t just clean—it’s <em>powerful</em>. It means you can:</p><ul><li><strong>Combine multiple strategies</strong> – Use dense vector search <em>and</em> keyword filtering together if needed.</li><li><strong>Run A/B tests easily</strong> – Compare different retrievers to see what gives better results for your users.</li><li><strong>Plug into any agent or tool</strong> – Whether you’re building a chatbot, a search UI, or a full-blown agent system, this retriever interface slots in easily.</li></ul><p>Think of the retrieval protocol as the API contract between your “retrieval brain” and everything else. Once you follow it, you’re free to innovate however you want behind the scenes.</p><h3 id="plugging-superlinked-into-llamaindex">Plugging Superlinked into LlamaIndex</h3><p>Now let’s see how we bridge Superlinked’s advanced vector computation capabilities with LlamaIndex’s retrieval abstraction:</p><pre><code class="language-python">class SuperlinkedSteamGamesRetriever(BaseRetriever):
    """A custom LlamaIndex retriever using Superlinked for Steam games data."""
    
    def __init__(self, csv_file: str, top_k: int = 10):
        self.top_k = top_k
        self.df = pd.read_csv(csv_file)
        
        # Critical: Text field combination for semantic richness
        self.df['combined_text'] = (
            self.df['name'].astype(str) + " " +
            self.df['desc_snippet'].astype(str) + " " +
            self.df['genre'].astype(str) + " " +
            self.df['game_details'].astype(str) + " " +
            self.df['game_description'].astype(str)
        )
        
        self._setup_superlinked()
</code></pre><p><strong>Integration Architecture Deep Dive:</strong></p><p><img src="https://github.com/vipul-maheshwari/vipul-maheshwari.github.io/blob/main/images/custom-retriever/architecture.png?raw=true" alt="architecture.png" /></p><h3 id="part-3-superlinked-schema-definition-and-setup">Part 3: Superlinked Schema Definition and Setup</h3><pre><code class="language-python">def _setup_superlinked(self):
    """Set up Superlinked schema, space, index, and executor."""

    # Schema Definition - The Foundation of Type Safety
    class GameSchema(sl.Schema):
        game_number: sl.IdField
        name: sl.String
        desc_snippet: sl.String
        game_details: sl.String
        languages: sl.String
        genre: sl.String
        game_description: sl.String
        original_price: sl.Float
        discount_price: sl.Float
        combined_text: sl.String  # Our semantic search target

</code></pre><p>In Superlinked, the schema isn’t just about defining data types— it’s more like a formal definition between our data and the underlying vector compute engine. This schema determines how our data gets parsed, indexed, and queried—so getting it right is crucial.</p><p>In our <code>SuperlinkedSteamGamesRetriever</code>, the schema is defined like this:</p><pre><code class="language-python">class GameSchema(sl.Schema):
    game_number: sl.IdField
    name: sl.String
    desc_snippet: sl.String
    game_details: sl.String
    languages: sl.String
    genre: sl.String
    game_description: sl.String
    original_price: sl.Float
    discount_price: sl.Float
    combined_text: sl.String  # Key field for semantic search

</code></pre><p>Let’s break down what each of these elements actually <em>does</em>:</p><ul><li><p><strong><code>sl.IdField</code> (→ <code>game_number</code>)</strong></p><p>Think of this as our primary key. It gives each game a unique identity and allows Superlinked to index and retrieve items efficiently, I mean basically it’s about how we are telling the Superlinked to segregate the unique identify of the games, and btw it’s especially important when you’re dealing with thousands of records.</p></li><li><p><strong><code>sl.String</code> and <code>sl.Float</code></strong></p><p>Now these aren’t just type hints—they enable Superlinked to optimize operations differently depending on the field. For instance, <code>sl.String</code> fields can be embedded and compared semantically, while <code>sl.Float</code> fields can support numeric filtering or sorting.</p></li><li><p><strong><code>combined_text</code></strong></p><p>This is the <strong>semantic anchor</strong> of our retriever. It’s a synthetic field where we concatenate the game name, description, genre, and other relevant attributes into a single block of text. This lets us build a single <strong>text similarity space</strong> using sentence-transformer embeddings:</p><pre><code class="language-python">  self.text_space = sl.TextSimilaritySpace(
      text=self.game.combined_text,
      model="sentence-transformers/all-mpnet-base-v2"
  )
    
</code></pre><p>Why do this? Because users don’t just search by genre or name—they describe what they’re <em>looking for</em>. By embedding all the important signals into <code>combined_text</code>, we can better match fuzzy, natural-language queries with the most relevant games.</p></li></ul><p>This schema definition is what allows Superlinked to <strong>parse our dataset, build the index, and execute semantic queries</strong>—all while retaining full control over how data is structured and ranked.</p><h3 id="part-4-vector-space-configuration">Part 4: Vector Space Configuration</h3><pre><code class="language-python">    # Create text similarity space using sentence transformers
    self.text_space = sl.TextSimilaritySpace(
        text=self.game.combined_text,
        model="sentence-transformers/all-mpnet-base-v2"
    )

    # Create index
    self.index = sl.Index([self.text_space])

</code></pre><p>To power the semantic search over our Steam games dataset, we made two intentional design choices that balance performance, simplicity, and flexibility.</p><p>First, for the embedding model, we selected <code>all-mpnet-base-v2</code> from the Sentence Transformers library. This model produces 768-dimensional embeddings that strike a solid middle ground: they’re expressive enough to capture rich semantic meaning, yet lightweight enough to be fast in production. I mean it’s a reliable general-purpose model, known to perform well across diverse text types — which matters a lot when your data ranges from short genre tags to long-form game descriptions. In our case, we needed a model that wouldn’t choke on either end of that spectrum, and <code>all-mpnet-base-v2</code> handled it cleanly.</p><p>Next, although Superlinked supports multi-space indexing — where you can combine multiple fields or even modalities (like text + images) — we deliberately kept things simple with a single <code>TextSimilaritySpace</code>. I would have included the <code>RecencySpace</code> in here too but we don’t have the information on the release date for the games. But just to put this out here, if we have the release date information, we could plug in the RecencySpace here, and we can even sort the games with the TextSimilaritySpace along with the Recency of the games. Cool..</p><p>Now for the current <code>TextSimilaritySpace</code> details, this was built on a custom field we created by concatenating the name, genre, description, and other textual fields for each game. The idea was straightforward: keep the query logic minimal, and let the model’s semantic power do the heavy lifting.</p><h3 id="part-5-data-pipeline-and-executor-setup">Part 5: Data Pipeline and Executor Setup</h3><pre><code class="language-python">    # Map DataFrame columns to schema - Critical for data integrity
    parser = sl.DataFrameParser(
        self.game,
        mapping={
            self.game.game_number: "game_number",
            self.game.name: "name",
            self.game.desc_snippet: "desc_snippet",
            self.game.game_details: "game_details",
            self.game.languages: "languages",
            self.game.genre: "genre",
            self.game.game_description: "game_description",
            self.game.original_price: "original_price",
            self.game.discount_price: "discount_price",
            self.game.combined_text: "combined_text"
        }
    )

    # Set up in-memory execution environment
    source = sl.InMemorySource(self.game, parser=parser)
    self.executor = sl.InMemoryExecutor(sources=[source], indices=[self.index])
    self.app = self.executor.run()

    # Load and process data
    source.put([self.df])

</code></pre><p>At the heart of our retrieval system is a streamlined pipeline built for both clarity and speed. We start with the <code>DataFrameParser</code>, which serves as our ETL layer. It ensures that each field in the dataset is correctly typed and consistently mapped to our schema — essentially acting as the contract between our raw CSV data and the Superlinked indexing layer.</p><p>Once the data is structured, we feed it into an <code>InMemorySource</code>, which is ideal for datasets that comfortably fit in memory . This approach keeps everything lightning-fast without introducing storage overhead or network latency. Finally, the queries are handled by an <code>InMemoryExecutor</code>, which is optimised for sub-millisecond latency. This is what makes Superlinked suitable for real-time applications like interactive recommendation systems, where speed directly impacts user experience.</p><h3 id="part-6-the-retrieval-engine">Part 6: The Retrieval Engine</h3><pre><code class="language-python">def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:
    """Core retrieval logic with semantic similarity scoring."""

    query_text = query_bundle.query_str

    # Construct Superlinked query with explicit field selection
    query = (
        sl.Query(self.index)
        .find(self.game)
        .similar(self.text_space, query_text)
        .select([
            self.game.game_number,
            self.game.name,
            self.game.desc_snippet,
            self.game.game_details,
            self.game.languages,
            self.game.genre,
            self.game.game_description,
            self.game.original_price,
            self.game.discount_price
        ])
        .limit(self.top_k)
    )

    # Execute and convert to pandas for processing
    result = self.app.query(query)
    df_result = sl.PandasConverter.to_pandas(result)

</code></pre><p>One of the things that makes Superlinked genuinely enjoyable to work with is its fluent-style query builder. If you’ve used libraries like SQLAlchemy or Django ORM, the pattern will feel familiar — each method in the chain adds clarity instead of clutter. In our case, the query starts by selecting the relevant index and defining the similarity search using the <code>.similar()</code> method, which computes cosine similarity in the embedding space. This is what allows us to retrieve semantically close games based on the user’s natural language query.</p><p>Another thoughtful design decision we made was to <strong>explicitly select the fields</strong> we care about in the result set — rather than doing something like <code>SELECT *</code>. This might sound minor, but it keeps the data lean, reduces processing overhead, and ensures we’re not passing around unnecessary payload during post-processing. Think of it as precision over bulk — especially important when you’re moving data between components in a latency-sensitive pipeline.</p><h3 id="part-7-result-processing-and-node-creation">Part 7: Result Processing and Node Creation</h3><pre><code class="language-python">    # Convert to LlamaIndex NodeWithScore format
    nodes_with_scores = []
    for i, row in df_result.iterrows():
        text = f"{row['name']}: {row['desc_snippet']}"
        metadata = {
            "game_number": row["id"],
            "name": row["name"],
            "desc_snippet": row["desc_snippet"],
            "game_details": row["game_details"],
            "languages": row["languages"],
            "genre": row["genre"],
            "game_description": row["game_description"],
            "original_price": row["original_price"],
            "discount_price": row["discount_price"]
        }

        # Simple ranking score based on result position
        score = 1.0 - (i / self.top_k)
        node = TextNode(text=text, metadata=metadata)
        nodes_with_scores.append(NodeWithScore(node=node, score=score))

    return nodes_with_scores

</code></pre><p>Now once we receive the results from Superlinked, we transform them into a format that plays well with LlamaIndex. First, we construct a <strong>human-readable text</strong> string by combining the game’s name with its short description — this becomes the content of each node, making it easier for the language model to reason about. It’s a small touch, but it really improves how relevant and understandable the retrieved data is when passed to the LLM.</p><p>Next, we make sure that <strong>all original fields</strong> from the dataset — including things like genre, pricing, and game details — are retained in the metadata. This is crucial because downstream processes might want to filter, display, or rank results based on this information. We don’t want to lose any useful context once we start working with the retrieved nodes.</p><p>Finally, we apply a lightweight <strong>score normalisation</strong> strategy. Instead of relying on raw similarity scores, we assign scores based on the position of the result in the ranked list. This keeps things simple and consistent — the top result always has the highest score, and the rest follow in descending order. It’s not fancy, but it gives us a stable and interpretable scoring system that works well across different queries.</p><h2 id="show-time-executing-the-pipeline">Show Time: Executing the pipeline</h2><p>Now that all components are in place, it’s time to bring our Retrieval-Augmented Generation (RAG) system to life. Below is the end-to-end integration of Superlinked and LlamaIndex in action.</p><pre><code class="language-python"># Initialize the RAG pipeline
Settings.llm = OpenAI(api_key="your-api-key-here")
retriever = SuperlinkedSteamGamesRetriever("sampled_games.csv", top_k=5)
response_synthesizer = get_response_synthesizer()

query_engine = RetrieverQueryEngine(
    retriever=retriever,
    response_synthesizer=response_synthesizer
)

# Execute a query
query = "I want to find a magic game with spells?"
response = query_engine.query(query)
print(response)
</code></pre><p>This setup combines our custom semantic retriever with a response synthesizer backed by an LLM. The query is seamlessly passed through the retrieval pipeline and returned with a meaningful response.</p><span class="meta"><time datetime="2025-07-02T00:00:00+05:30">July 2, 2025</time> &middot; <a href="/tag/Retriever">Retriever</a>, <a href="/tag/Superlinked">Superlinked</a></span>--> <!--</section>--></main><script async src="https://www.googletagmanager.com/gtag/js?id=G-JBZRCCYMBP"></script> <script> window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-JBZRCCYMBP'); </script></body></html>
